{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tZ1_CCkjZ5BZ"
      },
      "outputs": [],
      "source": [
        "import keras\n",
        "import os\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import KFold"
      ],
      "id": "tZ1_CCkjZ5BZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDV9KRNIt7gO"
      },
      "outputs": [],
      "source": [
        "final_test = True #@param {type: \"boolean\"}"
      ],
      "id": "XDV9KRNIt7gO"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YGQaCks3IQCr"
      },
      "source": [
        "## Cora"
      ],
      "id": "YGQaCks3IQCr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2v75ZzEoINmD"
      },
      "outputs": [],
      "source": [
        "zip_file = keras.utils.get_file(\n",
        "    fname=\"cora.tgz\",\n",
        "    origin=\"https://linqs-data.soe.ucsc.edu/public/lbc/cora.tgz\",\n",
        "    extract=True,\n",
        ")\n",
        "data_dir = os.path.join(os.path.dirname(zip_file), \"cora\")\n",
        "data_name = \"cora.cites\"\n",
        "node_data_name = 'cora.content'"
      ],
      "id": "2v75ZzEoINmD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "zBLlg7LjlZhi",
        "outputId": "f646b854-0252-4fdc-da0b-80ece4f2ddd4"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/root/.keras/datasets'"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.path.dirname(zip_file)"
      ],
      "id": "zBLlg7LjlZhi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oVMQTJToiFXA"
      },
      "source": [
        "## Pubmed"
      ],
      "id": "oVMQTJToiFXA"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ECpIFeASiLf1",
        "outputId": "59609f84-0678-41f9-e7df-1a4673f2fe56"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://linqs-data.soe.ucsc.edu/public/Pubmed-Diabetes.tgz\n",
            "14710584/14710584 [==============================] - 2s 0us/step\n"
          ]
        }
      ],
      "source": [
        "zip_file = keras.utils.get_file(\n",
        "    fname=\"pubmed.tgz\",\n",
        "    origin=\"https://linqs-data.soe.ucsc.edu/public/Pubmed-Diabetes.tgz\",\n",
        "    extract=True,\n",
        ")\n",
        "data_name = \"data/Pubmed-Diabetes.DIRECTED.cites.tab\"\n",
        "node_data_name = \"data/Pubmed-Diabetes.NODE.paper.tab\"\n",
        "data_dir = os.path.join(os.path.dirname(zip_file), \"Pubmed-Diabetes\")\n"
      ],
      "id": "ECpIFeASiLf1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OTfJqT91jQlk",
        "outputId": "a4eee093-6f18-4740-a43b-63571282ea68"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['README', 'citeseer.cites', 'citeseer.content']"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "os.listdir(data_dir)"
      ],
      "id": "OTfJqT91jQlk"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UCT40J9FLx7P"
      },
      "source": [
        "# CiteSeer"
      ],
      "id": "UCT40J9FLx7P"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBVNFybOLxUd",
        "outputId": "053347bc-8b28-4251-fe71-e10e3fa5433d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://linqs-data.soe.ucsc.edu/public/datasets/citeseer-doc-classification/citeseer-doc-classification.zip\n",
            "356438/356438 [==============================] - 0s 1us/step\n"
          ]
        }
      ],
      "source": [
        "zip_file = keras.utils.get_file(\n",
        "    fname=\"citeseer.tgz\",\n",
        "    origin=\"https://linqs-data.soe.ucsc.edu/public/datasets/citeseer-doc-classification/citeseer-doc-classification.zip\",\n",
        "    extract=True,\n",
        ")\n",
        "data_name = \"citeseer.cites\"\n",
        "node_data_name = \"citeseer.content\"\n",
        "data_dir = os.path.join(os.path.dirname(zip_file), \"citeseer-doc-classification\")"
      ],
      "id": "MBVNFybOLxUd"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ixuJ7rmlR_s1"
      },
      "source": [
        "# OGB Mag"
      ],
      "id": "ixuJ7rmlR_s1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g6Hbkb-zSHWa",
        "outputId": "d9d6ff2f-3760-45cb-ead5-bad1e329fc51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting ogb\n",
            "  Downloading ogb-1.3.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.8/78.8 kB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torch_geometric\n",
            "  Downloading torch_geometric-2.3.1.tar.gz (661 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m661.6/661.6 kB\u001b[0m \u001b[31m55.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (2.0.0+cu118)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.29.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (4.65.0)\n",
            "Requirement already satisfied: scikit-learn>=0.20.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.2.2)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.5.3)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.16.0)\n",
            "Requirement already satisfied: urllib3>=1.24.0 in /usr/local/lib/python3.10/dist-packages (from ogb) (1.26.15)\n",
            "Collecting outdated>=0.2.0 (from ogb)\n",
            "  Downloading outdated-0.2.2-py2.py3-none-any.whl (7.5 kB)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (1.10.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (2.27.1)\n",
            "Requirement already satisfied: pyparsing in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (3.0.9)\n",
            "Requirement already satisfied: psutil>=5.8.0 in /usr/local/lib/python3.10/dist-packages (from torch_geometric) (5.9.5)\n",
            "Requirement already satisfied: setuptools>=44 in /usr/local/lib/python3.10/dist-packages (from outdated>=0.2.0->ogb) (67.7.2)\n",
            "Collecting littleutils (from outdated>=0.2.0->ogb)\n",
            "  Downloading littleutils-0.2.2.tar.gz (6.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.24.0->ogb) (2022.7.1)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (1.2.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.20.0->ogb) (3.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.12.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->ogb) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->ogb) (16.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch_geometric) (2.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torch_geometric) (3.4)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->ogb) (1.3.0)\n",
            "Building wheels for collected packages: torch_geometric, littleutils\n",
            "  Building wheel for torch_geometric (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for torch_geometric: filename=torch_geometric-2.3.1-py3-none-any.whl size=910459 sha256=9f00d9e02523ee410695378b78f816720e56e4fc363806f02ba03fce5fa57366\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/dc/30/e2874821ff308ee67dcd7a66dbde912411e19e35a1addda028\n",
            "  Building wheel for littleutils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for littleutils: filename=littleutils-0.2.2-py3-none-any.whl size=7029 sha256=83ff91959dec33d5529b9c57549b0b102484d8ac4f7a669cc5c9ae86ee6f5192\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/fe/b0/27a9892da57472e538c7452a721a9cf463cc03cf7379889266\n",
            "Successfully built torch_geometric littleutils\n",
            "Installing collected packages: littleutils, outdated, torch_geometric, ogb\n",
            "Successfully installed littleutils-0.2.2 ogb-1.3.6 outdated-0.2.2 torch_geometric-2.3.1\n"
          ]
        }
      ],
      "source": [
        "%pip install ogb torch_geometric"
      ],
      "id": "g6Hbkb-zSHWa"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tASd3M0JSCNS",
        "outputId": "f6a86504-1d99-4068-841f-a83dd7c59008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://snap.stanford.edu/ogb/data/nodeproppred/mag.zip\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloaded 0.40 GB: 100%|██████████| 413/413 [00:29<00:00, 13.88it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting /Users/joshem/PhD Research/Data/mag.zip\n",
            "Loading necessary files...\n",
            "This might take a while.\n",
            "Processing graphs...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1/1 [00:00<00:00, 8439.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import ogb\n",
        "from ogb.nodeproppred import NodePropPredDataset \n",
        "from torch_geometric.data import DataLoader\n",
        "#H:\\\\PhD Research\\Data\\PROTEINS\\PROTEINS_node_attributes.txt\n",
        "# Download and process data at './dataset/ogbg_molhiv/'\n",
        "dataset = NodePropPredDataset(name = \"ogbn-mag\", root = '/Users/joshem/PhD Research/Data/')\n",
        "split_idx = dataset.get_idx_split()\n",
        "train_idx, valid_idx, test_idx = split_idx[\"train\"], split_idx[\"valid\"], split_idx[\"test\"]\n",
        "graph = dataset[0] # pyg graph object"
      ],
      "id": "tASd3M0JSCNS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eHsJhdrhD_1O"
      },
      "outputs": [],
      "source": [
        "import requests, zipfile, io\n",
        "r = requests.get('https://utdallas.box.com/shared/static/19yvbjlxzkwhjft5k9r9sfu23pkvm4aa.zip', stream=True)\n",
        "z = zipfile.ZipFile(io.BytesIO(r.content))\n",
        "z.extractall('/content')"
      ],
      "id": "eHsJhdrhD_1O"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "muYHE7IDDPjx"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/Feature_nbd_mag.csv', index_col=0)"
      ],
      "id": "muYHE7IDDPjx"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tq5X_6BrFjvv",
        "outputId": "d26d4438-aa97-4b70-aa64-6ea0477a3170"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "736389"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(data)"
      ],
      "id": "tq5X_6BrFjvv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8e5fdd8f"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "p_data = open(os.path.join(data_dir, f'{data_name}'))\n",
        "\n",
        "\n",
        "# Create a list from the data\n",
        "myls=list(p_data)\n",
        "\n",
        "# Remove the new line symbole from the list\n",
        "mylist = [line.rstrip('\\n') for line in myls]\n",
        "if data_name == 'data/Pubmed-Diabetes.DIRECTED.cites.tab':\n",
        "  mylist = [line.split('\\t')[1][6:] + '\\t' + line.split('\\t')[3][6:] \n",
        "              for line in mylist[2:]]\n",
        "#print(mylist)\n",
        "#print(graph_ind)\n",
        "#print(graph_level)\n",
        "\n",
        "p_data.close()"
      ],
      "id": "8e5fdd8f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nMehJszGk7fG",
        "outputId": "226dbbac-5d82-418a-b459-6222495a2dec"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['19127292\\t17363749',\n",
              " '19668377\\t17293876',\n",
              " '1313726\\t3002783',\n",
              " '19110882\\t14578298',\n",
              " '18606979\\t10333910',\n",
              " '19735543\\t8420806',\n",
              " '19228402\\t17017752',\n",
              " '19220880\\t12734781',\n",
              " '16595006\\t11790819',\n",
              " '19143817\\t11206408',\n",
              " '11943852\\t7722468',\n",
              " '18644886\\t3138126',\n",
              " '8325993\\t2186054',\n",
              " '18509207\\t15043684',\n",
              " '17894830\\t2044434',\n",
              " '9326333\\t7487991',\n",
              " '19228405\\t11561483',\n",
              " '17144912\\t1324940',\n",
              " '1541672\\t1991568',\n",
              " '18349042\\t12486503',\n",
              " '9737664\\t7594559',\n",
              " '17174749\\t1361076',\n",
              " '8100835\\t2180755',\n",
              " '11791216\\t11484155',\n",
              " '15192149\\t11869680',\n",
              " '16127462\\t9734395',\n",
              " '17327460\\t9758619',\n",
              " '8529129\\t1936620',\n",
              " '3280182\\t6884606',\n",
              " '14585101\\t9732339',\n",
              " '18477407\\t15968442',\n",
              " '16846517\\t12171451',\n",
              " '12560454\\t7024027',\n",
              " '12975475\\t11907129',\n",
              " '16075046\\t1284550',\n",
              " '18436707\\t15384883',\n",
              " '8145050\\t1972779',\n",
              " '11522610\\t2444321',\n",
              " '11454665\\t2933287',\n",
              " '18406405\\t9032105',\n",
              " '7833731\\t2951066',\n",
              " '11091269\\t9317167',\n",
              " '19360314\\t12829651',\n",
              " '7567975\\t3240835',\n",
              " '3891786\\t6311651',\n",
              " '16259490\\t10509871',\n",
              " '10190896\\t2794065',\n",
              " '18561508\\t16873794',\n",
              " '1504709\\t2148392',\n",
              " '15520861\\t14702111',\n",
              " '11522593\\t10963732',\n",
              " '3113569\\t4054448',\n",
              " '15647337\\t8452322',\n",
              " '2961842\\t6216134',\n",
              " '6594040\\t7233414',\n",
              " '16241867\\t11487743',\n",
              " '19073651\\t17536074',\n",
              " '14585101\\t11185325',\n",
              " '12456547\\t9802755',\n",
              " '15611284\\t11533710',\n",
              " '11598829\\t8349046',\n",
              " '19488997\\t16675714',\n",
              " '15687377\\t11979019',\n",
              " '19488997\\t2258796',\n",
              " '11507694\\t7573053',\n",
              " '8326004\\t1675318',\n",
              " '19005023\\t17130532',\n",
              " '18509500\\t17327327',\n",
              " '7769090\\t2148510',\n",
              " '15013978\\t11756343',\n",
              " '19436679\\t11565518',\n",
              " '7893584\\t1734985',\n",
              " '15467836\\t12606512',\n",
              " '19187735\\t14517715',\n",
              " '11919048\\t10191799',\n",
              " '18436707\\t11120765',\n",
              " '15508373\\t12110552',\n",
              " '18562637\\t15563560',\n",
              " '16873751\\t1516497',\n",
              " '15013978\\t11812768',\n",
              " '19151107\\t15333470',\n",
              " '159313\\t723641',\n",
              " '18498634\\t16865358',\n",
              " '17469043\\t14698999',\n",
              " '18513455\\t8012143',\n",
              " '3057885\\t2842756',\n",
              " '11943762\\t8090784',\n",
              " '17570749\\t12401727',\n",
              " '7913714\\t3287175',\n",
              " '1990836\\t3003909',\n",
              " '17428252\\t15855328',\n",
              " '18006654\\t17686944',\n",
              " '3496416\\t3896899',\n",
              " '10937502\\t1314967',\n",
              " '3708906\\t6791599',\n",
              " '16517804\\t15132888',\n",
              " '18924636\\t6380287',\n",
              " '11160138\\t1829330',\n",
              " '17597523\\t17451420',\n",
              " '19469001\\t15471153',\n",
              " '18841263\\t1587398',\n",
              " '17762000\\t12027929',\n",
              " '2189891\\t3290006',\n",
              " '18971435\\t17192330',\n",
              " '7568143\\t1697648',\n",
              " '17349010\\t11734230',\n",
              " '8636416\\t7989590',\n",
              " '18981116\\t11035773',\n",
              " '17662714\\t10333910',\n",
              " '18311190\\t12522068',\n",
              " '19508712\\t12823234',\n",
              " '16109069\\t1469084',\n",
              " '1469084\\t2424992',\n",
              " '15358643\\t9626161',\n",
              " '3632094\\t6369971',\n",
              " '11238556\\t9054944',\n",
              " '18996116\\t14618237',\n",
              " '18591388\\t17206141',\n",
              " '2961842\\t6373460',\n",
              " '15985177\\t3111583',\n",
              " '19587831\\t10480608',\n",
              " '9314549\\t2480383',\n",
              " '19007436\\t12773120',\n",
              " '12361980\\t1613467',\n",
              " '19228405\\t15975110',\n",
              " '650240\\t588254',\n",
              " '18060022\\t8278351',\n",
              " '19489075\\t1494889',\n",
              " '17570255\\t12765949',\n",
              " '14730479\\t11673414',\n",
              " '2703526\\t3905460',\n",
              " '17942684\\t17293876',\n",
              " '17969365\\t15889234',\n",
              " '17416797\\t16855264',\n",
              " '1402656\\t2119056',\n",
              " '16823476\\t15127202',\n",
              " '18430197\\t17389701',\n",
              " '18676351\\t8659491',\n",
              " '17302896\\t11476858',\n",
              " '3276208\\t6360766',\n",
              " '7888929\\t3240834',\n",
              " '18670622\\t12845430',\n",
              " '8326015\\t2185106',\n",
              " '9683605\\t8405760',\n",
              " '19060127\\t11135624',\n",
              " '17436030\\t15671192',\n",
              " '18330949\\t9203944',\n",
              " '15610327\\t10666428',\n",
              " '18678618\\t17846124',\n",
              " '12813916\\t11602628',\n",
              " '12819312\\t9732337',\n",
              " '19050058\\t17671651',\n",
              " '2969728\\t7002677',\n",
              " '19180473\\t15047612',\n",
              " '2040701\\t2668332',\n",
              " '3003160\\t6306391',\n",
              " '18753673\\t17601992',\n",
              " '17509149\\t16186404',\n",
              " '15670442\\t1914802',\n",
              " '19436648\\t16912128',\n",
              " '8612203\\t8375580',\n",
              " '2629640\\t3522325',\n",
              " '18776938\\t10580429',\n",
              " '18372903\\t17846126',\n",
              " '19448363\\t17999777',\n",
              " '9221759\\t8315397',\n",
              " '18682514\\t17327312',\n",
              " '8173400\\t8247074',\n",
              " '16766631\\t8194668',\n",
              " '2547480\\t3737674',\n",
              " '18346973\\t12211926',\n",
              " '11050183\\t10415012',\n",
              " '8024313\\t2187469',\n",
              " '3061933\\t6235285',\n",
              " '18523143\\t9028699',\n",
              " '18489578\\t2240915',\n",
              " '7860757\\t3894418',\n",
              " '18664617\\t14724435',\n",
              " '1826646\\t3311778',\n",
              " '19128359\\t17346189',\n",
              " '15738451\\t12882852',\n",
              " '17201925\\t17005949',\n",
              " '11790818\\t443443',\n",
              " '10807676\\t9723973',\n",
              " '19011169\\t11673498',\n",
              " '11286636\\t10724097',\n",
              " '3772281\\t7246127',\n",
              " '10516670\\t8156408',\n",
              " '19140227\\t17904449',\n",
              " '19323962\\t18591388',\n",
              " '19666551\\t17475933',\n",
              " '18667076\\t11452700',\n",
              " '17969381\\t16823476',\n",
              " '9427742\\t7528672',\n",
              " '10378067\\t1729850',\n",
              " '8567980\\t8326004',\n",
              " '6607315\\t6216134',\n",
              " '1469084\\t2881947',\n",
              " '15537844\\t8641115',\n",
              " '17727695\\t9549452',\n",
              " '14694146\\t8137498',\n",
              " '18241357\\t12441406',\n",
              " '18443201\\t17053028',\n",
              " '18414392\\t17495463',\n",
              " '18628530\\t8725855',\n",
              " '15383437\\t1643761',\n",
              " '9579150\\t8826962',\n",
              " '17932595\\t16728521',\n",
              " '9625758\\t3309126',\n",
              " '1469084\\t3260586',\n",
              " '8123622\\t6873204',\n",
              " '18716002\\t9794112',\n",
              " '19140227\\t10857962',\n",
              " '15932520\\t11978629',\n",
              " '8735622\\t3596063',\n",
              " '3578273\\t6334624',\n",
              " '16628253\\t10549628',\n",
              " '19114173\\t16868300',\n",
              " '14687274\\t9842286',\n",
              " '10337011\\t7537670',\n",
              " '17472435\\t15561965',\n",
              " '19436665\\t11491207',\n",
              " '19590589\\t8741811',\n",
              " '18241357\\t12591159',\n",
              " '18067546\\t2839472',\n",
              " '17503332\\t14647894',\n",
              " '8787686\\t2105341',\n",
              " '8878437\\t7761837',\n",
              " '9549452\\t8039603',\n",
              " '14694146\\t12777446',\n",
              " '16818907\\t10895843',\n",
              " '19289805\\t11832527',\n",
              " '10683375\\t9094710',\n",
              " '18802485\\t17876100',\n",
              " '9758619\\t8072542',\n",
              " '18468463\\t11533711',\n",
              " '18285525\\t17174134',\n",
              " '8450059\\t1929037',\n",
              " '16842487\\t11311100',\n",
              " '19587243\\t15734833',\n",
              " '2477102\\t6311651',\n",
              " '19291814\\t15907769',\n",
              " '18570678\\t18042079',\n",
              " '18687759\\t7867189',\n",
              " '9064326\\t7761837',\n",
              " '19171735\\t10599761',\n",
              " '9294105\\t8035658',\n",
              " '19509199\\t7623488',\n",
              " '19120273\\t17327428',\n",
              " '10491414\\t6384267',\n",
              " '18078023\\t15587767',\n",
              " '1675200\\t2253835',\n",
              " '15238494\\t10857938',\n",
              " '17434869\\t16025115',\n",
              " '18439548\\t17463246',\n",
              " '12750472\\t8637860',\n",
              " '1469103\\t3134969',\n",
              " '16075062\\t8690906',\n",
              " '17327457\\t15561963',\n",
              " '18782870\\t17460697',\n",
              " '3888193\\t6362429',\n",
              " '3045812\\t6233198',\n",
              " '8990188\\t2205920',\n",
              " '18710591\\t9159148',\n",
              " '18348689\\t9215219',\n",
              " '18664617\\t7621990',\n",
              " '11320276\\t1485947',\n",
              " '18162513\\t10882147',\n",
              " '17922167\\t3899825',\n",
              " '15492571\\t10580413',\n",
              " '14623908\\t9933106',\n",
              " '18426840\\t9075814',\n",
              " '16186411\\t15561969',\n",
              " '8182126\\t3993659',\n",
              " '8775937\\t3792666',\n",
              " '10630905\\t8941466',\n",
              " '15172958\\t8067445',\n",
              " '7533791\\t1301992',\n",
              " '8095192\\t3003909',\n",
              " '18932203\\t12928770',\n",
              " '16628253\\t15855569',\n",
              " '19091959\\t16960101',\n",
              " '11303130\\t10532529',\n",
              " '18818254\\t16738155',\n",
              " '17764005\\t16443764',\n",
              " '1884094\\t7001256',\n",
              " '9811884\\t1899142',\n",
              " '18606979\\t9135946',\n",
              " '7615815\\t8174841',\n",
              " '7888928\\t2389754',\n",
              " '7619068\\t1722397',\n",
              " '19125180\\t17445546',\n",
              " '16823472\\t15685168',\n",
              " '10714635\\t8057515',\n",
              " '1884094\\t2946385',\n",
              " '2128196\\t2447233',\n",
              " '9579150\\t2702906',\n",
              " '17100770\\t11600549',\n",
              " '1991854\\t1975377',\n",
              " '16847277\\t15530631',\n",
              " '18309686\\t7589820',\n",
              " '1463468\\t2668332',\n",
              " '11106373\\t8946834',\n",
              " '8200983\\t2406597',\n",
              " '14578206\\t12539039',\n",
              " '18535196\\t16873786',\n",
              " '18199798\\t16572114',\n",
              " '2180311\\t6839196',\n",
              " '2719080\\t6361268',\n",
              " '19337548\\t17596103',\n",
              " '11050183\\t9427742',\n",
              " '17443369\\t16327095',\n",
              " '18363276\\t11547217',\n",
              " '2307932\\t3496416',\n",
              " '12507898\\t10068486',\n",
              " '2794065\\t3906651',\n",
              " '3813637\\t7050326',\n",
              " '17129376\\t12488964',\n",
              " '15776395\\t15004560',\n",
              " '18204830\\t15013454',\n",
              " '18060660\\t17463246',\n",
              " '9328252\\t3437300',\n",
              " '17969365\\t12876091',\n",
              " '18633108\\t17463248',\n",
              " '15857516\\t2686492',\n",
              " '19055834\\t17093941',\n",
              " '19789636\\t16936215',\n",
              " '18489577\\t11289463',\n",
              " '18776148\\t15620463',\n",
              " '17942684\\t8326004',\n",
              " '16371630\\t10666428',\n",
              " '14980984\\t10599761',\n",
              " '18486765\\t15277401',\n",
              " '12149437\\t7589509',\n",
              " '7611284\\t7895953',\n",
              " '18199798\\t11967819',\n",
              " '19526210\\t14578243',\n",
              " '19133114\\t18191056',\n",
              " '1385478\\t3159965',\n",
              " '15381770\\t7888039',\n",
              " '19956100\\t14988278',\n",
              " '11744505\\t11017889',\n",
              " '7912625\\t6354812',\n",
              " '7532678\\t1406974',\n",
              " '8078905\\t2670595',\n",
              " '3156147\\t6448594',\n",
              " '19956103\\t17959935',\n",
              " '18678617\\t16118394',\n",
              " '17392166\\t12830456',\n",
              " '18420489\\t11781358',\n",
              " '19706162\\t3359580',\n",
              " '9410915\\t2015976',\n",
              " '18843550\\t17645549',\n",
              " '19389826\\t18952314',\n",
              " '19576321\\t12453968',\n",
              " '17988185\\t10097918',\n",
              " '9294791\\t8892716',\n",
              " '18291022\\t16847701',\n",
              " '1763069\\t2035962',\n",
              " '9892615\\t9103469',\n",
              " '16075053\\t16075062',\n",
              " '7488448\\t8472505',\n",
              " '15985157\\t192616',\n",
              " '19898642\\t17638715',\n",
              " '8363561\\t3044894',\n",
              " '15837817\\t7694152',\n",
              " '1370298\\t2653745',\n",
              " '8878437\\t1899431',\n",
              " '18227068\\t17670746',\n",
              " '19046221\\t9017359',\n",
              " '17237299\\t15051749',\n",
              " '11032784\\t2227105',\n",
              " '8920864\\t2205920',\n",
              " '1370298\\t3056757',\n",
              " '19194565\\t16731850',\n",
              " '18364392\\t15616004',\n",
              " '18245568\\t9826206',\n",
              " '16896942\\t11053301',\n",
              " '17090674\\t7914259',\n",
              " '18644069\\t9486993',\n",
              " '11511743\\t8519350',\n",
              " '17237940\\t17001469',\n",
              " '19056835\\t10630154',\n",
              " '18366806\\t17415550',\n",
              " '9435304\\t3546382',\n",
              " '16973667\\t8703884',\n",
              " '10692053\\t1675200',\n",
              " '15963175\\t12649049',\n",
              " '8170965\\t1736105',\n",
              " '10971508\\t8088010',\n",
              " '19475778\\t7794577',\n",
              " '8855307\\t2200729',\n",
              " '18801706\\t1473616',\n",
              " '19956098\\t12401728',\n",
              " '19743752\\t17924142',\n",
              " '14702111\\t2949622',\n",
              " '10848492\\t8533167',\n",
              " '19930636\\t14592535',\n",
              " '19065993\\t7589820',\n",
              " '16566827\\t15593124',\n",
              " '17307546\\t7809339',\n",
              " '19245711\\t11874934',\n",
              " '19065993\\t17156104',\n",
              " '16204372\\t1639955',\n",
              " '18854049\\t9684995',\n",
              " '16367807\\t10859350',\n",
              " '16847277\\t8270130',\n",
              " '1671006\\t2885918',\n",
              " '3944264\\t7002345',\n",
              " '3403714\\t2865274',\n",
              " '18854016\\t15983228',\n",
              " '19488997\\t9028691',\n",
              " '8941652\\t8247074',\n",
              " '18602983\\t17463249',\n",
              " '8198884\\t6471668',\n",
              " '18031995\\t11809620',\n",
              " '16309862\\t11133712',\n",
              " '18818245\\t16006542',\n",
              " '19055828\\t16855197',\n",
              " '12748907\\t10593567',\n",
              " '19254971\\t8366922',\n",
              " '19696902\\t1773700',\n",
              " '10666428\\t8455366',\n",
              " '19073938\\t11668341',\n",
              " '7860750\\t1727062',\n",
              " '11815493\\t10531848',\n",
              " '11435467\\t2403659',\n",
              " '18178847\\t10933399',\n",
              " '16542446\\t12453967',\n",
              " '17672906\\t11874930',\n",
              " '18006584\\t16306347',\n",
              " '18633113\\t17666460',\n",
              " '11091270\\t2406597',\n",
              " '3519679\\t6352727',\n",
              " '18583384\\t16489976',\n",
              " '18561511\\t9075803',\n",
              " '18670622\\t12453886',\n",
              " '18669807\\t8366922',\n",
              " '16716235\\t12810241',\n",
              " '17267595\\t8598488',\n",
              " '14585101\\t11028137',\n",
              " '19956093\\t19956106',\n",
              " '15889095\\t9122227',\n",
              " '19789630\\t17293876',\n",
              " '15890071\\t12086092',\n",
              " '6320806\\t160223',\n",
              " '19147816\\t1402665',\n",
              " '1849772\\t3025043',\n",
              " '18283633\\t16556846',\n",
              " '16369227\\t7039363',\n",
              " '18596953\\t15533587',\n",
              " '17210729\\t15790362',\n",
              " '18633107\\t3113569',\n",
              " '9435304\\t47533',\n",
              " '19088850\\t18231124',\n",
              " '8200974\\t6753469',\n",
              " '18596543\\t2941683',\n",
              " '12727928\\t10470079',\n",
              " '8514849\\t1756915',\n",
              " '18924636\\t12762952',\n",
              " '18443202\\t17463249',\n",
              " '18835956\\t10866057',\n",
              " '19096518\\t9742976',\n",
              " '9042932\\t2210067',\n",
              " '18611252\\t12882858',\n",
              " '10209507\\t9075797',\n",
              " '17259486\\t11832527',\n",
              " '12414951\\t8920894',\n",
              " '11790818\\t6458419',\n",
              " '18684786\\t16168670',\n",
              " '18776938\\t16823478',\n",
              " '18443202\\t17463246',\n",
              " '8636416\\t3510224',\n",
              " '8601111\\t8173400',\n",
              " '16236123\\t11590120',\n",
              " '10938050\\t7924774',\n",
              " '2498395\\t6278587',\n",
              " '8278351\\t2644534',\n",
              " '2384600\\t2662408',\n",
              " '19602480\\t19247628',\n",
              " '17594390\\t16555581',\n",
              " '19105914\\t7589943',\n",
              " '10545530\\t8945471',\n",
              " '19104970\\t12610784',\n",
              " '1763069\\t885298',\n",
              " '19479186\\t17765963',\n",
              " '16489972\\t10977022',\n",
              " '18448419\\t10580425',\n",
              " '18799632\\t3156147',\n",
              " '1972180\\t3264405',\n",
              " '15969768\\t2673695',\n",
              " '15932520\\t6362005',\n",
              " '17623014\\t15516696',\n",
              " '18262522\\t11819026',\n",
              " '18492786\\t8543793',\n",
              " '18776148\\t15372359',\n",
              " '19246639\\t10859350',\n",
              " '17571924\\t16775236',\n",
              " '18776141\\t11067779',\n",
              " '18654634\\t10331426',\n",
              " '18207200\\t7660530',\n",
              " '12967942\\t11473079',\n",
              " '19056762\\t11246881',\n",
              " '12832613\\t1357346',\n",
              " '8350054\\t2258003',\n",
              " '15606614\\t6966615',\n",
              " '18586907\\t15771578',\n",
              " '1467844\\t6347785',\n",
              " '2270941\\t2649324',\n",
              " '8381473\\t3032493',\n",
              " '15383791\\t15047609',\n",
              " '11711458\\t8933009',\n",
              " '17043101\\t16887982',\n",
              " '10482607\\t1661690',\n",
              " '10677521\\t10330293',\n",
              " '15546994\\t9844629',\n",
              " '8842069\\t1467683',\n",
              " '18091993\\t16723646',\n",
              " '19128359\\t17429581',\n",
              " '18492787\\t12618479',\n",
              " '10490523\\t2004172',\n",
              " '7573053\\t8072542',\n",
              " '8636427\\t7473199',\n",
              " '19273282\\t18080107',\n",
              " '18053255\\t2515052',\n",
              " '7722468\\t1409662',\n",
              " '18561513\\t17300593',\n",
              " '16049319\\t8941575',\n",
              " '19366943\\t3124685',\n",
              " '10550321\\t9296561',\n",
              " '1586783\\t7439541',\n",
              " '18478125\\t7716139',\n",
              " '19436665\\t17595355',\n",
              " '18337172\\t9878085',\n",
              " '3290380\\t3003909',\n",
              " '7769136\\t2824266',\n",
              " '16236379\\t11024581',\n",
              " '16400026\\t11527957',\n",
              " '12819007\\t10541297',\n",
              " '17969381\\t15505122',\n",
              " '19480671\\t15846663',\n",
              " '19046200\\t14988310',\n",
              " '8408615\\t1946445',\n",
              " '18535187\\t16105859',\n",
              " '18591387\\t11473032',\n",
              " '9053453\\t7860729',\n",
              " '17476357\\t14962949',\n",
              " '19956093\\t19956104',\n",
              " '17392158\\t8608603',\n",
              " '10739754\\t9491819',\n",
              " '16186282\\t8087927',\n",
              " '8692984\\t2189768',\n",
              " '17425627\\t15504997',\n",
              " '18292467\\t3543673',\n",
              " '9449718\\t8757636',\n",
              " '17319471\\t11594300',\n",
              " '17257275\\t9732337',\n",
              " '10191799\\t1299075',\n",
              " '18297260\\t16059790',\n",
              " '16672053\\t12660865',\n",
              " '3086921\\t6499635',\n",
              " '12133154\\t7967857',\n",
              " '16075062\\t11907129',\n",
              " '1522229\\t2523954',\n",
              " '7427231\\t1174829',\n",
              " '16109069\\t7744614',\n",
              " '19364331\\t16129698',\n",
              " '11507694\\t11334427',\n",
              " '3316278\\t2857024',\n",
              " '15965169\\t11168343',\n",
              " '19143814\\t9519723',\n",
              " '11927616\\t10330297',\n",
              " '17349010\\t15972866',\n",
              " '18198228\\t12502506',\n",
              " '3170749\\t3540010',\n",
              " '18590522\\t17018838',\n",
              " '18171473\\t8732717',\n",
              " '7944528\\t8330401',\n",
              " '18984671\\t11869302',\n",
              " '16249451\\t9445267',\n",
              " '19364331\\t10944172',\n",
              " '17703632\\t10359389',\n",
              " '1361492\\t2847947',\n",
              " '14983031\\t12925730',\n",
              " '18178393\\t16464953',\n",
              " '15988804\\t10333896',\n",
              " '1390526\\t6363177',\n",
              " '18973206\\t8879346',\n",
              " '17334651\\t12351431',\n",
              " '2510155\\t3968427',\n",
              " '18493734\\t15855341',\n",
              " '18755353\\t15983299',\n",
              " '8415742\\t2163026',\n",
              " '8675702\\t7720392',\n",
              " '8916747\\t1612225',\n",
              " '10944172\\t6134470',\n",
              " '18583417\\t15582161',\n",
              " '18400966\\t11978658',\n",
              " '9435304\\t9028724',\n",
              " '9239416\\t8287660',\n",
              " '9449718\\t8072542',\n",
              " '17532267\\t1719570',\n",
              " '1644920\\t2263645',\n",
              " '15492571\\t2897315',\n",
              " '17963344\\t15642082',\n",
              " '19479186\\t17363746',\n",
              " '17412305\\t10415018',\n",
              " '14623908\\t12145161',\n",
              " '17343760\\t16488505',\n",
              " '19721859\\t17805239',\n",
              " '10580857\\t2941683',\n",
              " '10336573\\t3090107',\n",
              " '19007436\\t15955116',\n",
              " '18825272\\t3073901',\n",
              " '19046244\\t9388398',\n",
              " '18769687\\t17160407',\n",
              " '15640422\\t7621998',\n",
              " '18269730\\t9075604',\n",
              " '18567926\\t12610013',\n",
              " '19114173\\t8858216',\n",
              " '17627978\\t9253967',\n",
              " '18215172\\t12681024',\n",
              " '18513544\\t2078922',\n",
              " '19111066\\t11063279',\n",
              " '19340286\\t15955116',\n",
              " '9529321\\t8522063',\n",
              " '8471028\\t2180755',\n",
              " '19179216\\t16371630',\n",
              " '17444424\\t15677776',\n",
              " '19220880\\t17315242',\n",
              " '19368707\\t17293876',\n",
              " '18047254\\t2403066',\n",
              " '19114173\\t8026285',\n",
              " '15082736\\t2964983',\n",
              " '17969365\\t9742976',\n",
              " '17290035\\t16801574',\n",
              " '18082004\\t12034408',\n",
              " '9708813\\t2226113',\n",
              " '8775937\\t8436836',\n",
              " '8127894\\t3540010',\n",
              " '12815107\\t9362527',\n",
              " '19040615\\t10562307',\n",
              " '19956098\\t11118029',\n",
              " '8012723\\t2923230',\n",
              " '15931309\\t11475269',\n",
              " '19956093\\t19956100',\n",
              " '18337172\\t1522229',\n",
              " '8200993\\t2189891',\n",
              " '10490523\\t7711537',\n",
              " '2257445\\t2850927',\n",
              " '17908332\\t8013760',\n",
              " '17969365\\t15140339',\n",
              " '18654634\\t12540634',\n",
              " '17932595\\t239528',\n",
              " '17697317\\t15526003',\n",
              " '19568428\\t18775581',\n",
              " '8564218\\t6698366',\n",
              " '17334651\\t11289047',\n",
              " '12456547\\t10776949',\n",
              " '11303130\\t2180661',\n",
              " '19449020\\t9817917',\n",
              " '18562629\\t12477967',\n",
              " '1592439\\t1975377',\n",
              " '18984738\\t15096540',\n",
              " '17983440\\t2205920',\n",
              " '12153522\\t8349046',\n",
              " '17627978\\t8017201',\n",
              " '17349009\\t12727928',\n",
              " '20061360\\t12136392',\n",
              " '1349195\\t2494458',\n",
              " '18782577\\t17327428',\n",
              " '3313390\\t6315512',\n",
              " '8352278\\t2885918',\n",
              " '18973206\\t15637426',\n",
              " '12569167\\t1532369',\n",
              " '3308958\\t2863188',\n",
              " '18567820\\t17434869',\n",
              " '18277383\\t16443880',\n",
              " '18492945\\t15711018',\n",
              " '1602013\\t3403714',\n",
              " '2173562\\t6389544',\n",
              " '18257922\\t10342344',\n",
              " '8675688\\t1446798',\n",
              " '17483299\\t16431079',\n",
              " '17151331\\t14514648',\n",
              " '19439071\\t18644071',\n",
              " '18346991\\t2677038',\n",
              " '1634622\\t2243134',\n",
              " '6373827\\t447832',\n",
              " '19688338\\t14578243',\n",
              " '18330949\\t2753246',\n",
              " '18316361\\t15919798',\n",
              " '6088584\\t6455904',\n",
              " '17188609\\t7926304',\n",
              " '18078023\\t16784180',\n",
              " '18561505\\t17517853',\n",
              " '18725615\\t18332288',\n",
              " '19165345\\t8549009',\n",
              " '18497687\\t15912092',\n",
              " '18779574\\t11812740',\n",
              " '17548847\\t12032107',\n",
              " '8145050\\t2882518',\n",
              " '19340895\\t14988838',\n",
              " '17727695\\t15616233',\n",
              " '14983031\\t12037147',\n",
              " '18078023\\t12074206',\n",
              " '17461531\\t8462766',\n",
              " '9727063\\t8551245',\n",
              " '7769140\\t2205920',\n",
              " '1469083\\t2890501',\n",
              " '8564237\\t2633909',\n",
              " '18445251\\t16773565',\n",
              " '18314421\\t2189768',\n",
              " '8371347\\t3956881',\n",
              " '18753291\\t16971655',\n",
              " '8733583\\t3816968',\n",
              " '12082183\\t8216686',\n",
              " '8012717\\t3816968',\n",
              " '14673094\\t3309126',\n",
              " '15997237\\t10857969',\n",
              " '18450419\\t15784469',\n",
              " '15837923\\t11781759',\n",
              " '12876160\\t1808637',\n",
              " '16412042\\t12235110',\n",
              " '16823472\\t9495343',\n",
              " '19403464\\t8366922',\n",
              " '19074809\\t17898301',\n",
              " '9664075\\t1202096',\n",
              " '17679132\\t15353531',\n",
              " '18561508\\t8720610',\n",
              " '19930636\\t8582537',\n",
              " '7615833\\t2210052',\n",
              " '7860730\\t1901765',\n",
              " '17953760\\t11194213',\n",
              " '1707531\\t2190098',\n",
              " '17151331\\t15983218',\n",
              " '8900244\\t1469084',\n",
              " '18190608\\t8100367',\n",
              " '19641379\\t10911004',\n",
              " '16423130\\t11955025',\n",
              " '7706455\\t8504767',\n",
              " '19046214\\t16478821',\n",
              " '15030523\\t10892762',\n",
              " '10848492\\t9257847',\n",
              " '7702375\\t3993659',\n",
              " '8555801\\t6738599',\n",
              " '18039812\\t16775037',\n",
              " '19156219\\t18716002',\n",
              " '18753291\\t16091403',\n",
              " '8601111\\t1932936',\n",
              " '3937338\\t3922361',\n",
              " '17983444\\t17082580',\n",
              " '9068300\\t1586783',\n",
              " '9794859\\t3203570',\n",
              " '18487476\\t14988310',\n",
              " '18477407\\t9773740',\n",
              " '18796620\\t15867184',\n",
              " '19956106\\t19956104',\n",
              " '14708927\\t12401412',\n",
              " '16127462\\t10588944',\n",
              " '16254324\\t12173683',\n",
              " '3891786\\t6124126',\n",
              " '8376591\\t3546382',\n",
              " '2185278\\t2537578',\n",
              " '15932520\\t9893161',\n",
              " '11719900\\t3057885',\n",
              " '1534652\\t3106832',\n",
              " '16451732\\t12919705',\n",
              " '15466998\\t1902595',\n",
              " '3493204\\t6220066',\n",
              " '19291814\\t9719467',\n",
              " '18205952\\t8733139',\n",
              " '19435491\\t12556541',\n",
              " '2263632\\t6870806',\n",
              " '19727402\\t12766110',\n",
              " '15155395\\t11742409',\n",
              " '10207172\\t7868915',\n",
              " '8108432\\t3033021',\n",
              " '1632685\\t3632094',\n",
              " '16796743\\t12140742',\n",
              " '18686043\\t11742409',\n",
              " '10430939\\t8871671',\n",
              " '9435304\\t1612231',\n",
              " '8637860\\t8288043',\n",
              " '18469202\\t16585050',\n",
              " '18435852\\t11565517',\n",
              " '8855307\\t1565635',\n",
              " '18028036\\t11565518',\n",
              " '16192452\\t1563334',\n",
              " '17889075\\t15784705',\n",
              " '18720028\\t16006679',\n",
              " '18678618\\t17603484',\n",
              " '9246003\\t6608876',\n",
              " '18776136\\t12516557',\n",
              " '17319100\\t12136399',\n",
              " '8401939\\t6297454',\n",
              " '19479186\\t10526729',\n",
              " '12896940\\t9732337',\n",
              " '17598085\\t17131142',\n",
              " '17213274\\t11832527',\n",
              " '19073651\\t16608888',\n",
              " '2571382\\t2444321',\n",
              " '18957531\\t17686944',\n",
              " '17415316\\t2653929',\n",
              " '17583177\\t15106814',\n",
              " '17034640\\t12808888',\n",
              " '17372793\\t15047092',\n",
              " '18657196\\t12940868',\n",
              " '16966600\\t15250035',\n",
              " '9707599\\t8641276',\n",
              " '7615815\\t8214048',\n",
              " '18544707\\t18443202',\n",
              " '8567980\\t7769140',\n",
              " '18258851\\t16177002',\n",
              " '18060660\\t17703236',\n",
              " '18599530\\t16595597',\n",
              " '16873750\\t12074262',\n",
              " '7558135\\t1971173',\n",
              " '17130504\\t7732997',\n",
              " '16494646\\t15539803',\n",
              " '10209433\\t3557875',\n",
              " '15776395\\t11799396',\n",
              " '19228808\\t10868967',\n",
              " '9732338\\t3034532',\n",
              " '3531381\\t7041259',\n",
              " '7931087\\t1896073',\n",
              " '12896940\\t9742977',\n",
              " '18483403\\t11815468',\n",
              " '18567926\\t3698784',\n",
              " '17476010\\t8554246',\n",
              " '15616247\\t11955025',\n",
              " '12456547\\t8846677',\n",
              " '2217193\\t2654288',\n",
              " '17876371\\t8117360',\n",
              " '18437223\\t1478367',\n",
              " '17244154\\t1285852',\n",
              " '16443789\\t15793255',\n",
              " '19060127\\t1936671',\n",
              " '18633108\\t17293876',\n",
              " '19030205\\t14593459',\n",
              " '19143814\\t10053014',\n",
              " '18422727\\t15616015',\n",
              " '19419582\\t17967711',\n",
              " '3944264\\t7144445',\n",
              " '19114173\\t8659491',\n",
              " '17043101\\t1909135',\n",
              " '3033025\\t6197336',\n",
              " '16488949\\t11118026',\n",
              " '17672906\\t12150599',\n",
              " '18561511\\t17003291',\n",
              " '18562629\\t7540575',\n",
              " '17461531\\t15031673',\n",
              " '9042932\\t2784133',\n",
              " '17603484\\t15056606',\n",
              " '19366942\\t16918588',\n",
              " '16886063\\t9166663',\n",
              " '8182126\\t2253832',\n",
              " '18314421\\t17293876',\n",
              " '18200815\\t12765945',\n",
              " '12361980\\t1733800',\n",
              " '17135629\\t7859931',\n",
              " '18513544\\t16567531',\n",
              " '16842480\\t15375790',\n",
              " '18248673\\t18070262',\n",
              " '18296490\\t12746279',\n",
              " '6385729\\t6895508',\n",
              " '18823555\\t15679073',\n",
              " '18178393\\t15983206',\n",
              " '3017115\\t6245984',\n",
              " '2892397\\t6363172',\n",
              " '12699416\\t11079741',\n",
              " '9238035\\t7612220',\n",
              " '19397795\\t7888692',\n",
              " '19436665\\t12087010',\n",
              " '18753291\\t15563560',\n",
              " '15738708\\t14709195',\n",
              " '19568428\\t7589820',\n",
              " '19095325\\t10954015',\n",
              " '1786627\\t3863753',\n",
              " '14707030\\t3525850',\n",
              " '19228405\\t9096971',\n",
              " '18698428\\t9169130',\n",
              " '16306359\\t12941777',\n",
              " '2173763\\t1690262',\n",
              " '11891618\\t11704924',\n",
              " '18809813\\t9669786',\n",
              " '18511765\\t16895879',\n",
              " '12409659\\t1733132',\n",
              " '9824500\\t1516628',\n",
              " '8423231\\t6698317',\n",
              " '17956579\\t14678267',\n",
              " '17148684\\t16381994',\n",
              " '18458138\\t9827853',\n",
              " '16597817\\t2970917',\n",
              " '11091269\\t9435304',\n",
              " '18060022\\t10677521',\n",
              " '18561513\\t11790216',\n",
              " '15383437\\t4053961',\n",
              " '19179216\\t16823477',\n",
              " '10944172\\t2956889',\n",
              " '17668382\\t12540637',\n",
              " '19668377\\t1541672',\n",
              " '15781936\\t8013746',\n",
              " '8012717\\t8012723',\n",
              " '15837923\\t10037770',\n",
              " '18477407\\t11815512',\n",
              " '3546382\\t6378696',\n",
              " '18417746\\t12164879',\n",
              " '18985156\\t17437080',\n",
              " '17261860\\t9786807',\n",
              " '19093033\\t18054738',\n",
              " '9246003\\t1139259',\n",
              " '17969380\\t2157941',\n",
              " '16205724\\t10604280',\n",
              " '18498660\\t16855264',\n",
              " '7545875\\t8402882',\n",
              " '8317480\\t2384193',\n",
              " '17908332\\t11553191',\n",
              " '19553558\\t11812768',\n",
              " '16628253\\t15919798',\n",
              " '18509204\\t14529698',\n",
              " '17989340\\t12110133',\n",
              " '16443884\\t2897940',\n",
              " '19450032\\t14529698',\n",
              " '10952644\\t2227133',\n",
              " '19120494\\t219345',\n",
              " '18650372\\t15531498',\n",
              " '18423879\\t11484070',\n",
              " '3280182\\t6368290',\n",
              " '11687636\\t7859934',\n",
              " '9062360\\t7703388',\n",
              " '18583417\\t17337254',\n",
              " '15467826\\t12540604',\n",
              " '15467824\\t15467829',\n",
              " '18262522\\t10520229',\n",
              " '19436648\\t18786605',\n",
              " '9202063\\t8614835',\n",
              " '17257277\\t10414932',\n",
              " '12163378\\t9222639',\n",
              " '11919048\\t3523246',\n",
              " '18591387\\t16306330',\n",
              " '17956579\\t14578287',\n",
              " '18435852\\t17517853',\n",
              " '8775937\\t2644534',\n",
              " '19065993\\t17223217',\n",
              " '11255900\\t1516497',\n",
              " '17302896\\t11978633',\n",
              " '7545875\\t2394030',\n",
              " '17349010\\t11476858',\n",
              " '11285306\\t9625758',\n",
              " '2223305\\t3188985',\n",
              " '15841215\\t12055342',\n",
              " '15466998\\t11498469',\n",
              " '17559889\\t14527633',\n",
              " '18235050\\t17538940',\n",
              " '2409808\\t508288',\n",
              " '19956106\\t18556337',\n",
              " '6391388\\t6351759',\n",
              " '18996116\\t3925127',\n",
              " '18776938\\t16931765',\n",
              " '19151107\\t11874924',\n",
              " '19105914\\t9272592',\n",
              " '7615815\\t2138534',\n",
              " '17967198\\t10777831',\n",
              " '17671651\\t6384269',\n",
              " '14514648\\t8040759',\n",
              " '18957534\\t17098085',\n",
              " '17584843\\t17023529',\n",
              " '17668636\\t8072544',\n",
              " '17594390\\t14969650',\n",
              " '17483299\\t10803701',\n",
              " '19065992\\t17476355',\n",
              " '18366646\\t14747303',\n",
              " '18178393\\t14755342',\n",
              " '18640486\\t15112169',\n",
              " '3052943\\t3891476',\n",
              " '16178863\\t10931424',\n",
              " '1864963\\t2806767',\n",
              " '15537844\\t11422762',\n",
              " '1592439\\t208156',\n",
              " '18776141\\t16028335',\n",
              " '1569197\\t2307296',\n",
              " '12153522\\t8908376',\n",
              " '19038792\\t16494646',\n",
              " '18493227\\t16046308',\n",
              " '10403912\\t8314020',\n",
              " '2442195\\t2861128',\n",
              " '17521324\\t10841006',\n",
              " '18984738\\t8314024',\n",
              " '19666551\\t3317417',\n",
              " '18682012\\t16249558',\n",
              " '17503332\\t16936217',\n",
              " '18284670\\t15721974',\n",
              " '10516670\\t1451951',\n",
              " '17185336\\t2335846',\n",
              " '17620445\\t10891831',\n",
              " '19065992\\t9742977',\n",
              " '18660851\\t9771706',\n",
              " '18556343\\t3293339',\n",
              " '19357773\\t17395743',\n",
              " ...]"
            ]
          },
          "execution_count": 104,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "mylist"
      ],
      "id": "nMehJszGk7fG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "b3d5d2f1",
        "outputId": "42ecc56a-cbeb-4439-ebce-f2cec75cccc8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        ID  w_0  w_1  w_2  w_3  w_4  w_5  w_6  w_7  w_8  ...  w_1424  w_1425  \\\n",
              "0    31336    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "1  1061127    0    0    0    0    0    0    0    0    0  ...       0       1   \n",
              "2  1106406    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "3    13195    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "4    37879    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "5  1126012    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "6  1107140    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "7  1102850    0    0    0    1    0    0    0    0    0  ...       0       0   \n",
              "8    31349    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "9  1106418    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "\n",
              "   w_1426  w_1427  w_1428  w_1429  w_1430  w_1431  w_1432  \\\n",
              "0       1       0       0       0       0       0       0   \n",
              "1       0       0       0       0       0       0       0   \n",
              "2       0       0       0       0       0       0       0   \n",
              "3       0       0       0       0       0       0       0   \n",
              "4       0       0       0       0       0       0       0   \n",
              "5       1       0       0       0       0       0       0   \n",
              "6       0       0       0       0       0       0       0   \n",
              "7       0       0       0       0       0       0       0   \n",
              "8       0       0       0       0       0       0       0   \n",
              "9       0       0       0       0       0       0       0   \n",
              "\n",
              "                    class  \n",
              "0         Neural_Networks  \n",
              "1           Rule_Learning  \n",
              "2  Reinforcement_Learning  \n",
              "3  Reinforcement_Learning  \n",
              "4   Probabilistic_Methods  \n",
              "5   Probabilistic_Methods  \n",
              "6                  Theory  \n",
              "7         Neural_Networks  \n",
              "8         Neural_Networks  \n",
              "9                  Theory  \n",
              "\n",
              "[10 rows x 1435 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f35e13b1-3eea-4094-814a-a8d6f3bfaa1b\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>w_0</th>\n",
              "      <th>w_1</th>\n",
              "      <th>w_2</th>\n",
              "      <th>w_3</th>\n",
              "      <th>w_4</th>\n",
              "      <th>w_5</th>\n",
              "      <th>w_6</th>\n",
              "      <th>w_7</th>\n",
              "      <th>w_8</th>\n",
              "      <th>...</th>\n",
              "      <th>w_1424</th>\n",
              "      <th>w_1425</th>\n",
              "      <th>w_1426</th>\n",
              "      <th>w_1427</th>\n",
              "      <th>w_1428</th>\n",
              "      <th>w_1429</th>\n",
              "      <th>w_1430</th>\n",
              "      <th>w_1431</th>\n",
              "      <th>w_1432</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>31336</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Neural_Networks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1061127</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Rule_Learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1106406</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Reinforcement_Learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>13195</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Reinforcement_Learning</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>37879</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Probabilistic_Methods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>1126012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Probabilistic_Methods</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>1107140</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Theory</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>1102850</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Neural_Networks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>31349</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Neural_Networks</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>1106418</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>Theory</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 1435 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f35e13b1-3eea-4094-814a-a8d6f3bfaa1b')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f35e13b1-3eea-4094-814a-a8d6f3bfaa1b button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f35e13b1-3eea-4094-814a-a8d6f3bfaa1b');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "if node_data_name == \"data/Pubmed-Diabetes.NODE.paper.tab\":\n",
        "  node_data = []\n",
        "  with open(os.path.join(data_dir, node_data_name)) as f:\n",
        "    for line in f.readlines()[2:]:\n",
        "      node_info = {d.split('=')[0]:d.split('=')[1] for d in line.split('\\t')[1:]}\n",
        "      node_info['ID'] = line.split('\\t')[0]\n",
        "      node_info['class'] = int(node_info.pop('label'))-1\n",
        "      node_data.append(node_info)\n",
        "    # print(node_data[1])\n",
        "  node_data = pd.DataFrame(node_data).fillna(0.0).drop('summary', axis=1)\n",
        "else:\n",
        "  num_features = 0\n",
        "  with open(os.path.join(data_dir, node_data_name)) as f:\n",
        "    num_features = len(f.readline().split())\n",
        "  feature_names = [\"w_{}\".format(ii) for ii in range(num_features-2)]\n",
        "  column_names = [\"ID\"]+ feature_names + [\"class\"]\n",
        "  node_data = pd.read_csv(os.path.join(data_dir, node_data_name), sep='\\t', header=None, names=column_names,\n",
        "                          dtype={'ID': str})\n",
        "\n",
        "  \n",
        "node_data.head(10)\n"
      ],
      "id": "b3d5d2f1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "XsCUTzr2S2P_",
        "outputId": "09cff3c7-d894-41bb-84d4-4f53d4ef1741"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'data/Pubmed-Diabetes.NODE.paper.tab'"
            ]
          },
          "execution_count": 97,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "node_data_name"
      ],
      "id": "XsCUTzr2S2P_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93a37c24"
      },
      "outputs": [],
      "source": [
        "if node_data_name == 'cora.content':\n",
        "  Data=node_data.replace({'class': {'Case_Based':0 , 'Genetic_Algorithms': 1,'Neural_Networks':2,'Probabilistic_Methods':3 , 'Reinforcement_Learning': 4,'Rule_Learning':5,'Theory':6}})\n",
        "  # Case_Based==0\n",
        "  #\t\tGenetic_Algorithms==1\n",
        "  #\t\tNeural_Networks==2\n",
        "  #\t\tProbabilistic_Methods==3\n",
        "  #\t\tReinforcement_Learning==4\n",
        "  #\t\tRule_Learning==5\n",
        "  #\t\tTheory==6\n",
        "  Data.head(10)\n",
        "\n",
        "elif node_data_name == 'citeseer.content':\n",
        "  Data=node_data.replace({'class': {'Agents':0 , 'AI': 1,'DB':2,'IR':3 , 'ML': 4,'HCI':5}})\n",
        "  Data.head(10)\n",
        "else:\n",
        "  Data = node_data"
      ],
      "id": "93a37c24"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 392
        },
        "id": "EJeFZKF4S8Fd",
        "outputId": "c3ef8976-1594-425b-8215-f97961fa549e"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-56c5d67b-74c0-4f72-a7b1-369538dc440e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>w_0</th>\n",
              "      <th>w_1</th>\n",
              "      <th>w_2</th>\n",
              "      <th>w_3</th>\n",
              "      <th>w_4</th>\n",
              "      <th>w_5</th>\n",
              "      <th>w_6</th>\n",
              "      <th>w_7</th>\n",
              "      <th>w_8</th>\n",
              "      <th>...</th>\n",
              "      <th>w_3694</th>\n",
              "      <th>w_3695</th>\n",
              "      <th>w_3696</th>\n",
              "      <th>w_3697</th>\n",
              "      <th>w_3698</th>\n",
              "      <th>w_3699</th>\n",
              "      <th>w_3700</th>\n",
              "      <th>w_3701</th>\n",
              "      <th>w_3702</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100157</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100598</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>105684</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>11099</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>114091</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>11510</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>115971</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>117999</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>120432</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>126894</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>10 rows × 3705 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-56c5d67b-74c0-4f72-a7b1-369538dc440e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-56c5d67b-74c0-4f72-a7b1-369538dc440e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-56c5d67b-74c0-4f72-a7b1-369538dc440e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       ID  w_0  w_1  w_2  w_3  w_4  w_5  w_6  w_7  w_8  ...  w_3694  w_3695  \\\n",
              "0  100157    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "1  100598    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "2  105684    0    1    0    0    0    0    0    0    0  ...       0       0   \n",
              "3   11099    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "4  114091    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "5   11510    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "6  115971    1    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "7  117999    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "8  120432    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "9  126894    0    0    0    0    0    0    0    0    0  ...       0       0   \n",
              "\n",
              "   w_3696  w_3697  w_3698  w_3699  w_3700  w_3701  w_3702  class  \n",
              "0       0       0       0       0       0       0       0      0  \n",
              "1       0       0       0       0       0       0       0      3  \n",
              "2       0       0       0       0       0       0       0      0  \n",
              "3       0       0       0       0       0       0       0      2  \n",
              "4       0       0       0       0       0       0       0      1  \n",
              "5       0       0       0       0       0       0       0      1  \n",
              "6       0       0       0       0       0       0       0      0  \n",
              "7       0       0       0       0       0       0       0      3  \n",
              "8       0       0       0       0       0       0       0      1  \n",
              "9       0       0       0       0       0       0       0      5  \n",
              "\n",
              "[10 rows x 3705 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Data.head(10)"
      ],
      "id": "EJeFZKF4S8Fd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "ff9a8793",
        "outputId": "8fd2f5aa-c95f-4539-a89e-12fdf16d36d6"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           ID  w_0  w_1  w_2  w_3  w_4  w_5  w_6  w_7  w_8  ...  w_1424  \\\n",
              "1622  1000012    0    0    0    0    0    0    0    0    0  ...       0   \n",
              "2044   100197    0    0    0    0    0    0    0    0    0  ...       0   \n",
              "32     100701    0    0    0    0    0    0    0    0    0  ...       0   \n",
              "243    100935    0    0    0    0    0    0    0    0    0  ...       0   \n",
              "1119   100961    0    0    0    0    0    0    0    0    0  ...       0   \n",
              "\n",
              "      w_1425  w_1426  w_1427  w_1428  w_1429  w_1430  w_1431  w_1432  class  \n",
              "1622       0       0       0       0       0       0       0       0      5  \n",
              "2044       0       0       0       0       0       0       0       0      2  \n",
              "32         0       0       0       0       0       0       0       0      0  \n",
              "243        0       0       0       0       0       0       0       0      1  \n",
              "1119       0       0       0       0       0       0       0       0      2  \n",
              "\n",
              "[5 rows x 1435 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5a86046-80b4-4fa9-9339-3130b4c79188\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>w_0</th>\n",
              "      <th>w_1</th>\n",
              "      <th>w_2</th>\n",
              "      <th>w_3</th>\n",
              "      <th>w_4</th>\n",
              "      <th>w_5</th>\n",
              "      <th>w_6</th>\n",
              "      <th>w_7</th>\n",
              "      <th>w_8</th>\n",
              "      <th>...</th>\n",
              "      <th>w_1424</th>\n",
              "      <th>w_1425</th>\n",
              "      <th>w_1426</th>\n",
              "      <th>w_1427</th>\n",
              "      <th>w_1428</th>\n",
              "      <th>w_1429</th>\n",
              "      <th>w_1430</th>\n",
              "      <th>w_1431</th>\n",
              "      <th>w_1432</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1622</th>\n",
              "      <td>1000012</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2044</th>\n",
              "      <td>100197</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>100701</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>243</th>\n",
              "      <td>100935</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1119</th>\n",
              "      <td>100961</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 1435 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5a86046-80b4-4fa9-9339-3130b4c79188')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f5a86046-80b4-4fa9-9339-3130b4c79188 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f5a86046-80b4-4fa9-9339-3130b4c79188');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ],
      "source": [
        "data_s=Data.sort_values(by=['ID'],ascending=True)\n",
        "data_s.head()"
      ],
      "id": "ff9a8793"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x8i2Ng-hzIys",
        "outputId": "ca3efcb3-edd3-4a81-f344-63ccdc3bd340"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['w-rat', 'w-common', 'w-use', 'w-examin', 'w-pathogenesi', 'w-retinopathi', 'w-mous', 'w-studi', 'w-anim', 'w-model', 'w-metabol', 'w-abnorm', 'w-contribut', 'w-develop', 'w-investig', 'w-mice', 'w-2', 'w-month', 'w-compar', 'w-obtain', 'w-method', 'w-induc', 'w-6', 'w-inject', 'w-experiment', 'w-normal', 'w-diet', 'w-30', 'w-hyperglycemia', 'w-level', 'w-lipid', 'w-oxid', 'w-activ', 'w-protein', 'w-kinas', 'w-c', 'w-measur', 'w-result', 'w-increas', 'w-retin', 'w-stress', 'w-3', 'w-similar', 'w-observ', 'w-conclus', 'w-play', 'w-import', 'w-role', 'w-present', 'summary', 'ID', 'class', 'w-p', 'w-m', 'w-r', 'w-muscl', 'w-control', 'w-chang', 'w-dure', 'w-lower', 'w-higher', 'w-mass', 'w-correl', 'w-decreas', 'w-determin', 'w-concentr', 'w-stimul', 'w-period', 'w-caus', 'w-mark', 'w-group', 'w-evid', 'w-fast', 'w-type', 'w-signific', 'w-differ', 'w-ratio', 'w-suggest', 'w-degre', 'w-occur', 'w-vivo', 'w-respect', 'w-dysfunct', 'w-region', 'w-high', 'w-appear', 'w-sever', 'w-affect', 'w-cardiovascular', 'w-complic', 'w-primari', 'w-death', 'w-patient', 'w-clinic', 'w-suscept', 'w-cardiac', 'w-tissu', 'w-specif', 'w-function', 'w-defect', 'w-possibl', 'w-indic', 'w-state', 'w-onli', 'w-bodi', 'w-weight', 'w-loss', 'w-valu', 'w-howev', 'w-4', 'w-condit', 'w-durat', 'w-8', 'w-week', 'w-onset', 'w-data', 'w-direct', 'w-report', 'w-provid', 'w-addit', 'w-evalu', 'w-sensit', 'w-heart', 'w-object', 'w-mean', 'w-blood', 'w-glucos', 'w-strong', 'w-hba', 'w-1c', 'w-a1c', 'w-variabl', 'w-independ', 'w-assess', 'w-relat', 'w-trial', 'w-research', 'w-design', 'w-profil', 'w-sampl', 'w-particip', 'w-n', 'w-1', 'w-consist', 'w-befor', 'w-min', 'w-predict', 'w-adjust', 'w-sex', 'w-treatment', 'w-7', 'w-gt', 'w-0', 'w-larg', 'w-influenc', 'w-base', 'w-standard', 'w-14', 'w-10', 'w-wherea', 'w-enhanc', 'w-manag', 'w-day', 'w-secret', 'w-cholesterol', 'w-insulin', 'w-24', 'w-h', 'w-low', 'w-rate', 'w-fatti', 'w-acid', 'w-effect', 'w-hormon', 'w-hepat', 'w-contrast', 'w-product', 'w-major', 'w-plasma', 'w-current', 'w-flow', 'w-chronic', 'w-mechan', 'w-test', 'w-therefor', 'w-analys', 'w-mrna', 'w-streptozotocin', 'w-did', 'w-15', 'w-g', 'w-25', 'w-mmol', 'w-l', 'w-5', 'w-reduc', 'w-number', 'w-densiti', 'w-posit', 'w-cell', 'w-17', 'w-mm', 'w-18', 'w-induct', 'w-associ', 'w-express', 'w-glycem', 'w-respons', 'w-therapi', 'w-random', 'w-initi', 'w-ani', 'w-singl', 'w-new', 'w-agent', 'w-metformin', 'w-medic', 'w-glycosyl', 'w-hemoglobin', 'w-analysi', 'w-baselin', 'w-health', 'w-factor', 'w-process', 'w-care', 'w-9', 'w-01', 'w-95', 'w-interv', 'w-ci', 'w-12', 'w-reduct', 'w-achiev', 'w-target', 'w-lt', 'w-diseas', 'w-class', 'w-age', 'w-obes', 'w-renal', 'w-improv', 'w-progress', 'w-noninsulindepend', 'w-mellitus', 'w-becaus', 'w-s', 'w-index', 'w-hypertens', 'w-need', 'w-followup', 'w-year', 'w-mg', 'w-dl', 'w-remain', 'w-subject', 'w-treat', 'w-oral', 'w-requir', 'w-0001', 'w-mortal', 'w-includ', 'w-vs', 'w-background', 'w-poor', 'w-drug', 'w-13', 'w-rang', 'w-combin', 'w-intervent', 'w-daili', 'w-dose', 'w-100', 'w-toler', 'w-receiv', 'w-11', 'w-postprandi', 'w-kg', 'w-hypoglycemia', 'w-frequent', 'w-event', 'w-versus', 'w-symptom', 'w-incid', 'w-parent', 'w-complex', 'w-longterm', 'w-inhibitor', 'w-peripher', 'w-nerv', 'w-stz', 'w-conduct', 'w-demonstr', 'w-frequenc', 'w-inhibit', 'w-neuropathi', 'w-pathway', 'w-shown', 'w-time', 'w-ii', 'w-individu', 'w-adult', 'w-50', 'w-60', 'w-diagnosi', 'w-healthi', 'w-follow', 'w-young', 'w-seen', 'w-alter', 'w-gene', 'w-e', 'w-identifi', 'w-previous', 'w-mediat', 'w-vascular', 'w-lipoprotein', 'w-involv', 'w-phenotyp', 'w-confirm', 'w-variant', 'w-endotheli', 'w-potenti', 'w-disord', 'w-popul', 'w-nonobes', 'w-aim', 'w-serum', 'w-hba1c', 'w-hypoglycaemia', 'w-continu', 'w-case', 'w-impair', 'w-risk', 'w-known', 'w-men', 'w-women', 'w-40', 'w-complet', 'w-estim', 'w-like', 'w-particular', 'w-human', 'w-character', 'w-elev', 'w-synthesi', 'w-greater', 'w-small', 'w-reveal', 'w-liver', 'w-niddm', 'w-genet', 'w-receptor', 'w-growth', 'w-pancreat', 'w-betacel', 'w-molecul', 'w-enzym', 'w-regul', 'w-polymorph', 'w-total', 'w-allel', 'w-02', 'w-resist', 'w-cpeptid', 'w-hypothesi', 'w-perform', 'w-score', 'w-001', 'w-05', 'w-histori', 'w-action', 'w-approxim', 'w-suppress', 'w-glucagon', 'w-ml', 'w-x', 'w-free', 'w-peopl', 'w-uptak', 'w-intens', 'w-relationship', 'w-prevent', 'w-autoimmun', 'w-recent', 'w-preval', 'w-nondiabet', 'w-genotyp', 'w-conclud', 'w-linkag', 'w-islet', 'w-peptid', 'w-form', 'w-membran', 'w-transgen', 'w-failur', 'w-isol', 'w-negat', 'w-earli', 'w-famili', 'w-chromosom', 'w-immun', 'w-support', 'w-16', 'w-cohort', 'w-insulindepend', 'w-outcom', 'w-screen', 'w-approach', 'w-infus', 'w-multipl', 'w-depend', 'w-physic', 'w-transport', 'w-acut', 'w-releas', 'w-presenc', 'w-glycaem', 'w-male', 'w-antibodi', 'w-femal', 'w-pattern', 'w-t2dm', 'w-promot', 'w-fat', 'w-d', 'w-bmi', 'w-haplotyp', 'w-triglycerid', 'w-interact', 'w-marker', 'w-describ', 'w-area', 'w-20', 'w-cytokin', 'w-bind', 'w-bb', 'w-alpha', 'w-beta', 'w-cd4', 'w-spontan', 'w-vitro', 'w-given', 'w-basal', 'w-protect', 'w-pressur', 'w-detect', 'w-exercis', 'w-children', 'w-adolesc', 'w-life', 'w-b', 'w-antigen', 'w-iddm', 'w-american', 'w-hla', 'w-arteri', 'w-nephropathi', 'w-review', 'w-destruct', 'w-content', 'w-autoantibodi', 'w-dm', 'w-select', 'w-infect', 'w-recipi', 'w-intak', 'w-placebo', 'w-db', 'w-pancrea', 'w-diagnos', 'w-glomerular', 'w-albumin', 'w-excret', 'w-syndrom', 'w-t', 'w-lymphocyt', 'w-produc', 'w-coronari', 'w-status', 'w-microalbuminuria', 'w-nod', 'w-mhc', 'w-insul', 'w-administr', 'w-revers', 'w-transplant', 'w-graft', 'w-t1d', 'w-lead', 'w-v', 'w-dietari', 'w-general', 'w-macrophag', 'w-kidney', 'w-urinari', 'w-myocardi', 'w-meal', 'w-ica', 'w-locus', 'w-tcell', 'w-depress', 'w-bone', 'w-mutat']\n"
          ]
        }
      ],
      "source": [
        "print(list(data_s.columns))"
      ],
      "id": "x8i2Ng-hzIys"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66c45ee1",
        "outputId": "e7ff163e-6826-4ccc-fe1e-ae9fdae2bdf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'1000012': 0, '100197': 1, '100701': 2, '100935': 3, '100961': 4, '101143': 5, '101145': 6, '101261': 7, '101263': 8, '101660': 9, '101662': 10, '10169': 11, '10174': 12, '10177': 13, '101811': 14, '10183': 15, '10186': 16, '102061': 17, '1022969': 18, '102406': 19, '1026': 20, '102879': 21, '102884': 22, '102938': 23, '102939': 24, '1031453': 25, '1033': 26, '1034': 27, '103430': 28, '103482': 29, '1035': 30, '103515': 31, '103528': 32, '103529': 33, '103531': 34, '103537': 35, '103543': 36, '10430': 37, '10435': 38, '104840': 39, '105057': 40, '1050679': 41, '10531': 42, '105856': 43, '105865': 44, '105899': 45, '1059953': 46, '1061127': 47, '1063773': 48, '106590': 49, '107177': 50, '1071981': 51, '107251': 52, '107252': 53, '107569': 54, '10793': 55, '10796': 56, '10798': 57, '108047': 58, '108962': 59, '108963': 60, '108974': 61, '108983': 62, '109323': 63, '1095507': 64, '10981': 65, '110041': 66, '110162': 67, '110163': 68, '110164': 69, '1102364': 70, '1102400': 71, '1102407': 72, '1102442': 73, '1102548': 74, '1102550': 75, '1102567': 76, '1102625': 77, '1102646': 78, '1102751': 79, '1102761': 80, '1102794': 81, '1102850': 82, '1102873': 83, '1103016': 84, '1103031': 85, '1103038': 86, '1103162': 87, '1103315': 88, '1103383': 89, '1103394': 90, '1103499': 91, '1103610': 92, '1103676': 93, '1103737': 94, '1103960': 95, '1103969': 96, '1103979': 97, '1103985': 98, '1104007': 99, '1104031': 100, '1104055': 101, '1104182': 102, '1104191': 103, '1104258': 104, '1104261': 105, '1104300': 106, '1104379': 107, '1104435': 108, '1104449': 109, '1104495': 110, '1104647': 111, '1104749': 112, '1104769': 113, '1104787': 114, '1104809': 115, '1104851': 116, '1104946': 117, '1104999': 118, '1105011': 119, '1105033': 120, '1105062': 121, '1105116': 122, '1105148': 123, '1105221': 124, '1105231': 125, '1105344': 126, '1105360': 127, '1105394': 128, '1105428': 129, '1105433': 130, '1105450': 131, '1105505': 132, '1105530': 133, '1105531': 134, '1105574': 135, '1105603': 136, '1105622': 137, '1105672': 138, '1105698': 139, '1105718': 140, '1105764': 141, '1105810': 142, '1105877': 143, '1105887': 144, '1105932': 145, '1106052': 146, '1106103': 147, '1106112': 148, '1106172': 149, '1106236': 150, '1106287': 151, '1106298': 152, '1106330': 153, '1106370': 154, '1106388': 155, '1106401': 156, '1106406': 157, '1106418': 158, '1106492': 159, '1106546': 160, '1106547': 161, '1106568': 162, '1106630': 163, '1106671': 164, '1106764': 165, '1106771': 166, '1106789': 167, '1106849': 168, '1106854': 169, '1106966': 170, '1107010': 171, '1107041': 172, '1107062': 173, '1107067': 174, '1107095': 175, '1107136': 176, '1107140': 177, '1107171': 178, '1107215': 179, '1107312': 180, '1107319': 181, '1107325': 182, '1107355': 183, '1107367': 184, '1107385': 185, '1107418': 186, '1107455': 187, '1107558': 188, '1107567': 189, '1107572': 190, '1107674': 191, '1107728': 192, '1107808': 193, '1107861': 194, '1108050': 195, '1108167': 196, '1108169': 197, '1108175': 198, '1108209': 199, '1108258': 200, '1108267': 201, '1108329': 202, '1108363': 203, '1108389': 204, '1108551': 205, '1108570': 206, '1108597': 207, '1108656': 208, '1108728': 209, '1108834': 210, '1108841': 211, '1109017': 212, '1109185': 213, '1109199': 214, '1109208': 215, '11093': 216, '1109392': 217, '1109439': 218, '1109542': 219, '1109566': 220, '1109581': 221, '1109830': 222, '1109873': 223, '1109891': 224, '1109957': 225, '1110000': 226, '1110024': 227, '1110028': 228, '1110209': 229, '1110256': 230, '1110390': 231, '1110426': 232, '1110438': 233, '1110494': 234, '1110515': 235, '1110520': 236, '1110531': 237, '1110546': 238, '1110563': 239, '1110579': 240, '1110628': 241, '1110768': 242, '1110947': 243, '1110950': 244, '1110998': 245, '1111052': 246, '1111186': 247, '1111230': 248, '1111240': 249, '1111265': 250, '1111304': 251, '1111614': 252, '1111733': 253, '1111788': 254, '1111899': 255, '1111978': 256, '1112026': 257, '1112071': 258, '1112075': 259, '1112099': 260, '1112106': 261, '1112194': 262, '1112319': 263, '1112369': 264, '1112417': 265, '1112426': 266, '1112574': 267, '1112650': 268, '1112665': 269, '1112686': 270, '1112723': 271, '1112767': 272, '1112911': 273, '1112929': 274, '1113035': 275, '1113084': 276, '1113182': 277, '1113438': 278, '1113459': 279, '1113534': 280, '1113541': 281, '1113551': 282, '1113614': 283, '1113739': 284, '1113742': 285, '1113828': 286, '1113831': 287, '1113852': 288, '1113926': 289, '1113934': 290, '1113995': 291, '1114118': 292, '1114125': 293, '1114153': 294, '1114184': 295, '1114192': 296, '1114222': 297, '1114239': 298, '1114331': 299, '1114336': 300, '1114352': 301, '1114364': 302, '1114388': 303, '1114398': 304, '1114442': 305, '1114502': 306, '1114512': 307, '1114526': 308, '1114605': 309, '1114629': 310, '1114664': 311, '1114777': 312, '11148': 313, '1114838': 314, '1114864': 315, '1114992': 316, '1115166': 317, '1115291': 318, '1115375': 319, '1115456': 320, '1115471': 321, '1115670': 322, '1115677': 323, '1115701': 324, '1115790': 325, '1115886': 326, '1115959': 327, '1116044': 328, '1116146': 329, '1116181': 330, '1116268': 331, '1116328': 332, '1116336': 333, '1116347': 334, '1116397': 335, '1116410': 336, '1116530': 337, '1116569': 338, '1116594': 339, '1116629': 340, '111676': 341, '1116835': 342, '1116839': 343, '1116842': 344, '1116922': 345, '1116974': 346, '1117049': 347, '1117089': 348, '1117184': 349, '1117219': 350, '1117249': 351, '1117348': 352, '1117476': 353, '1117501': 354, '1117618': 355, '1117653': 356, '111770': 357, '1117760': 358, '1117786': 359, '1117833': 360, '1117920': 361, '1117942': 362, '1118017': 363, '1118083': 364, '1118092': 365, '1118120': 366, '1118209': 367, '1118245': 368, '1118286': 369, '1118302': 370, '1118332': 371, '1118347': 372, '1118388': 373, '1118546': 374, '1118658': 375, '111866': 376, '1118764': 377, '1118823': 378, '1118848': 379, '1119004': 380, '1119078': 381, '1119140': 382, '1119178': 383, '1119180': 384, '1119211': 385, '1119216': 386, '1119295': 387, '1119471': 388, '1119505': 389, '1119623': 390, '1119654': 391, '1119671': 392, '1119708': 393, '1119742': 394, '1119751': 395, '1119987': 396, '1120019': 397, '1120020': 398, '1120049': 399, '1120059': 400, '1120084': 401, '1120138': 402, '1120169': 403, '1120170': 404, '1120197': 405, '1120211': 406, '1120252': 407, '1120431': 408, '1120444': 409, '1120563': 410, '1120643': 411, '1120650': 412, '1120713': 413, '1120731': 414, '1120777': 415, '1120786': 416, '1120858': 417, '1120866': 418, '1120880': 419, '1120962': 420, '112099': 421, '1121057': 422, '1121063': 423, '1121176': 424, '1121254': 425, '1121313': 426, '1121398': 427, '1121459': 428, '1121537': 429, '1121569': 430, '1121603': 431, '1121659': 432, '1121739': 433, '1121867': 434, '1122304': 435, '1122425': 436, '1122460': 437, '1122574': 438, '1122580': 439, '1122642': 440, '1122704': 441, '1123068': 442, '1123087': 443, '1123093': 444, '1123188': 445, '1123215': 446, '1123239': 447, '1123493': 448, '1123530': 449, '1123553': 450, '1123576': 451, '1123689': 452, '1123756': 453, '112378': 454, '1123867': 455, '1123926': 456, '1123991': 457, '1124837': 458, '1124844': 459, '1125082': 460, '1125092': 461, '1125258': 462, '1125386': 463, '1125393': 464, '1125402': 465, '1125467': 466, '1125469': 467, '1125492': 468, '1125597': 469, '1125895': 470, '1125906': 471, '1125909': 472, '1125944': 473, '1125953': 474, '1125992': 475, '1125993': 476, '1126011': 477, '1126012': 478, '1126029': 479, '1126037': 480, '1126044': 481, '1126050': 482, '1126315': 483, '1126350': 484, '1126503': 485, '1127430': 486, '1127530': 487, '1127541': 488, '1127551': 489, '1127558': 490, '1127566': 491, '1127619': 492, '1127657': 493, '1127810': 494, '1127812': 495, '1127851': 496, '1127863': 497, '112787': 498, '1127913': 499, '112813': 500, '1128151': 501, '1128198': 502, '1128201': 503, '1128204': 504, '1128208': 505, '1128227': 506, '1128256': 507, '1128267': 508, '1128291': 509, '1128314': 510, '1128319': 511, '1128369': 512, '1128407': 513, '1128425': 514, '1128430': 515, '1128437': 516, '1128453': 517, '1128531': 518, '1128536': 519, '1128542': 520, '1128839': 521, '1128846': 522, '1128853': 523, '1128856': 524, '1128868': 525, '1128881': 526, '1128927': 527, '1128935': 528, '1128943': 529, '1128945': 530, '1128946': 531, '1128959': 532, '1128974': 533, '1128975': 534, '1128977': 535, '1128978': 536, '1128982': 537, '1128985': 538, '1128990': 539, '1128997': 540, '1129015': 541, '1129018': 542, '1129021': 543, '1129027': 544, '1129040': 545, '1129096': 546, '1129106': 547, '1129111': 548, '1129208': 549, '1129243': 550, '1129367': 551, '1129368': 552, '1129369': 553, '1129442': 554, '1129443': 555, '1129494': 556, '1129518': 557, '1129570': 558, '1129572': 559, '1129573': 560, '1129608': 561, '1129610': 562, '1129621': 563, '1129629': 564, '1129683': 565, '1129778': 566, '1129798': 567, '1129835': 568, '1129907': 569, '1129994': 570, '1130069': 571, '1130080': 572, '1130243': 573, '1130356': 574, '1130454': 575, '1130539': 576, '1130567': 577, '1130568': 578, '1130586': 579, '1130600': 580, '1130634': 581, '1130637': 582, '1130653': 583, '1130657': 584, '1130676': 585, '1130678': 586, '1130680': 587, '1130780': 588, '1130808': 589, '1130847': 590, '1130856': 591, '1130915': 592, '1130927': 593, '1130929': 594, '1130931': 595, '1130934': 596, '1131116': 597, '1131137': 598, '1131149': 599, '1131150': 600, '1131163': 601, '1131164': 602, '1131165': 603, '1131167': 604, '1131172': 605, '1131180': 606, '1131184': 607, '1131189': 608, '1131192': 609, '1131195': 610, '1131198': 611, '1131223': 612, '1131230': 613, '1131236': 614, '1131257': 615, '1131258': 616, '1131266': 617, '1131267': 618, '1131270': 619, '1131274': 620, '1131277': 621, '1131300': 622, '1131301': 623, '1131305': 624, '1131312': 625, '1131314': 626, '1131330': 627, '1131334': 628, '1131335': 629, '1131345': 630, '1131348': 631, '1131359': 632, '1131360': 633, '1131374': 634, '1131414': 635, '1131420': 636, '1131421': 637, '1131464': 638, '1131466': 639, '1131471': 640, '1131549': 641, '1131550': 642, '1131557': 643, '1131565': 644, '1131607': 645, '1131611': 646, '1131634': 647, '1131639': 648, '1131647': 649, '1131719': 650, '1131728': 651, '1131734': 652, '1131741': 653, '1131745': 654, '1131748': 655, '1131752': 656, '1131754': 657, '1131828': 658, '1132073': 659, '1132083': 660, '1132157': 661, '1132285': 662, '1132385': 663, '1132406': 664, '1132416': 665, '1132418': 666, '1132434': 667, '1132443': 668, '1132459': 669, '1132461': 670, '1132486': 671, '11325': 672, '1132505': 673, '11326': 674, '1132706': 675, '1132731': 676, '1132809': 677, '1132815': 678, '1132857': 679, '1132864': 680, '1132887': 681, '1132922': 682, '1132948': 683, '1132968': 684, '1133004': 685, '1133008': 686, '1133010': 687, '1133028': 688, '1133047': 689, '1133196': 690, '1133338': 691, '1133390': 692, '1133417': 693, '1133428': 694, '1133469': 695, '11335': 696, '11337': 697, '1133846': 698, '11339': 699, '1133930': 700, '1134022': 701, '1134031': 702, '1134056': 703, '1134197': 704, '11342': 705, '1134320': 706, '1134346': 707, '1134348': 708, '1134865': 709, '1135082': 710, '1135108': 711, '1135115': 712, '1135122': 713, '1135125': 714, '1135137': 715, '1135345': 716, '1135358': 717, '1135368': 718, '1135455': 719, '1135589': 720, '1135746': 721, '1135750': 722, '1135894': 723, '1135899': 724, '1135955': 725, '1136040': 726, '1136110': 727, '1136310': 728, '1136342': 729, '1136393': 730, '1136397': 731, '1136422': 732, '1136442': 733, '1136446': 734, '1136447': 735, '1136449': 736, '1136631': 737, '1136634': 738, '1136791': 739, '1136814': 740, '1137140': 741, '1137466': 742, '1138027': 743, '1138043': 744, '1138091': 745, '1138619': 746, '1138755': 747, '1138968': 748, '1138970': 749, '1139009': 750, '1139195': 751, '1139928': 752, '114': 753, '1140040': 754, '1140230': 755, '1140231': 756, '1140289': 757, '1140543': 758, '1140547': 759, '1140548': 760, '114189': 761, '114308': 762, '114966': 763, '115188': 764, '1152075': 765, '1152143': 766, '1152150': 767, '1152162': 768, '1152179': 769, '1152194': 770, '1152244': 771, '1152259': 772, '1152272': 773, '1152277': 774, '1152290': 775, '1152307': 776, '1152308': 777, '1152358': 778, '1152379': 779, '1152394': 780, '1152421': 781, '1152436': 782, '1152448': 783, '1152490': 784, '1152508': 785, '1152564': 786, '1152569': 787, '1152633': 788, '1152663': 789, '1152673': 790, '1152676': 791, '1152711': 792, '1152714': 793, '1152740': 794, '1152761': 795, '1152821': 796, '1152858': 797, '1152859': 798, '1152896': 799, '1152904': 800, '1152910': 801, '1152917': 802, '1152944': 803, '1152958': 804, '1152959': 805, '1152975': 806, '1152991': 807, '1153003': 808, '1153014': 809, '1153024': 810, '1153031': 811, '1153056': 812, '1153064': 813, '1153065': 814, '1153091': 815, '1153097': 816, '1153101': 817, '1153106': 818, '1153148': 819, '1153150': 820, '1153160': 821, '1153166': 822, '1153169': 823, '1153183': 824, '1153195': 825, '1153254': 826, '1153262': 827, '1153264': 828, '1153275': 829, '1153280': 830, '1153287': 831, '1153577': 832, '1153703': 833, '1153724': 834, '1153728': 835, '1153736': 836, '1153784': 837, '1153786': 838, '1153811': 839, '1153816': 840, '1153853': 841, '1153860': 842, '1153861': 843, '1153866': 844, '1153877': 845, '1153879': 846, '1153889': 847, '1153891': 848, '1153896': 849, '1153897': 850, '1153899': 851, '1153900': 852, '1153922': 853, '1153933': 854, '1153942': 855, '1153943': 856, '1153945': 857, '1153946': 858, '1154012': 859, '1154042': 860, '1154068': 861, '1154071': 862, '1154074': 863, '1154076': 864, '1154103': 865, '1154123': 866, '1154124': 867, '1154169': 868, '1154173': 869, '1154176': 870, '1154229': 871, '1154230': 872, '1154232': 873, '1154233': 874, '1154251': 875, '1154276': 876, '1154459': 877, '1154500': 878, '1154520': 879, '1154524': 880, '1154525': 881, '1155073': 882, '116021': 883, '116081': 884, '116084': 885, '116087': 886, '116512': 887, '116528': 888, '116545': 889, '116552': 890, '116553': 891, '116790': 892, '117': 893, '117315': 894, '117316': 895, '117328': 896, '118079': 897, '118259': 898, '118260': 899, '118424': 900, '118435': 901, '118436': 902, '118558': 903, '118559': 904, '118682': 905, '118873': 906, '119686': 907, '119712': 908, '119761': 909, '119956': 910, '120013': 911, '120039': 912, '120084': 913, '120817': 914, '1213': 915, '12155': 916, '12158': 917, '12165': 918, '12169': 919, '121792': 920, '12182': 921, '12194': 922, '12195': 923, '12197': 924, '12198': 925, '12199': 926, '12210': 927, '12211': 928, '12238': 929, '12247': 930, '12275': 931, '12330': 932, '12337': 933, '12347': 934, '12350': 935, '123556': 936, '12359': 937, '1237': 938, '123825': 939, '124064': 940, '124224': 941, '124296': 942, '12439': 943, '1246': 944, '124734': 945, '124828': 946, '124952': 947, '12558': 948, '12576': 949, '126128': 950, '12631': 951, '12638': 952, '126793': 953, '126867': 954, '126868': 955, '126909': 956, '126912': 957, '126920': 958, '126926': 959, '126927': 960, '127033': 961, '1272': 962, '127940': 963, '128': 964, '128202': 965, '128203': 966, '128383': 967, '128540': 968, '129042': 969, '129045': 970, '129287': 971, '12946': 972, '129558': 973, '12960': 974, '129896': 975, '129897': 976, '130': 977, '13024': 978, '131042': 979, '131117': 980, '131122': 981, '131315': 982, '131317': 983, '131318': 984, '13136': 985, '13193': 986, '13195': 987, '13205': 988, '13208': 989, '13212': 990, '13213': 991, '13269': 992, '132806': 993, '132821': 994, '133550': 995, '133553': 996, '133563': 997, '133566': 998, '133567': 999, '133615': 1000, '133628': 1001, '134060': 1002, '134128': 1003, '134199': 1004, '134219': 1005, '134307': 1006, '134314': 1007, '134315': 1008, '134316': 1009, '134320': 1010, '135130': 1011, '135464': 1012, '135765': 1013, '135766': 1014, '135798': 1015, '1365': 1016, '13652': 1017, '13654': 1018, '13656': 1019, '13658': 1020, '136665': 1021, '136766': 1022, '136767': 1023, '136768': 1024, '13686': 1025, '137130': 1026, '13717': 1027, '137359': 1028, '137380': 1029, '137790': 1030, '137849': 1031, '137868': 1032, '137873': 1033, '137956': 1034, '1385': 1035, '13885': 1036, '13917': 1037, '139547': 1038, '13960': 1039, '13966': 1040, '13972': 1041, '139738': 1042, '13982': 1043, '139865': 1044, '140005': 1045, '140569': 1046, '14062': 1047, '14083': 1048, '14090': 1049, '141160': 1050, '141171': 1051, '141324': 1052, '141342': 1053, '141347': 1054, '141596': 1055, '141868': 1056, '142268': 1057, '143323': 1058, '143476': 1059, '143676': 1060, '143801': 1061, '144212': 1062, '14428': 1063, '14429': 1064, '14430': 1065, '14431': 1066, '144330': 1067, '144408': 1068, '144679': 1069, '144701': 1070, '145134': 1071, '145176': 1072, '145215': 1073, '14529': 1074, '14531': 1075, '145315': 1076, '145384': 1077, '14545': 1078, '14549': 1079, '147870': 1080, '14807': 1081, '1481': 1082, '148170': 1083, '148341': 1084, '148399': 1085, '149139': 1086, '149669': 1087, '15076': 1088, '151430': 1089, '151708': 1090, '152219': 1091, '152226': 1092, '152227': 1093, '152483': 1094, '152731': 1095, '153063': 1096, '153598': 1097, '154023': 1098, '154047': 1099, '154134': 1100, '15429': 1101, '15431': 1102, '154982': 1103, '155158': 1104, '155277': 1105, '155736': 1106, '155738': 1107, '15670': 1108, '156794': 1109, '156977': 1110, '157401': 1111, '157761': 1112, '157805': 1113, '158098': 1114, '158172': 1115, '158614': 1116, '158812': 1117, '15889': 1118, '15892': 1119, '159084': 1120, '159085': 1121, '15984': 1122, '15987': 1123, '159897': 1124, '16008': 1125, '160705': 1126, '160732': 1127, '161221': 1128, '162075': 1129, '162080': 1130, '162664': 1131, '163235': 1132, '164': 1133, '16437': 1134, '16451': 1135, '16461': 1136, '16470': 1137, '16471': 1138, '16474': 1139, '16476': 1140, '16485': 1141, '164885': 1142, '166420': 1143, '166825': 1144, '166989': 1145, '167205': 1146, '167656': 1147, '167670': 1148, '16819': 1149, '168332': 1150, '168410': 1151, '16843': 1152, '1688': 1153, '168958': 1154, '169279': 1155, '169280': 1156, '1694': 1157, '170338': 1158, '170798': 1159, '171225': 1160, '1717': 1161, '171954': 1162, '17201': 1163, '17208': 1164, '17242': 1165, '17363': 1166, '173863': 1167, '173884': 1168, '174418': 1169, '174425': 1170, '17476': 1171, '17477': 1172, '17488': 1173, '175256': 1174, '175291': 1175, '175548': 1176, '175576': 1177, '175909': 1178, '177115': 1179, '17798': 1180, '177993': 1181, '177998': 1182, '17811': 1183, '178209': 1184, '17821': 1185, '1786': 1186, '178718': 1187, '178727': 1188, '179180': 1189, '179702': 1190, '179706': 1191, '180187': 1192, '180301': 1193, '180373': 1194, '180399': 1195, '1817': 1196, '181782': 1197, '182093': 1198, '182094': 1199, '18251': 1200, '18313': 1201, '184157': 1202, '184918': 1203, '18532': 1204, '18536': 1205, '18582': 1206, '18615': 1207, '18619': 1208, '187260': 1209, '187354': 1210, '18770': 1211, '18773': 1212, '18774': 1213, '18777': 1214, '18781': 1215, '18785': 1216, '18811': 1217, '18812': 1218, '18815': 1219, '188318': 1220, '18832': 1221, '18833': 1222, '18834': 1223, '188471': 1224, '189566': 1225, '189571': 1226, '189572': 1227, '189574': 1228, '189577': 1229, '189620': 1230, '189623': 1231, '189655': 1232, '189708': 1233, '189721': 1234, '189774': 1235, '189856': 1236, '19045': 1237, '190697': 1238, '190698': 1239, '190706': 1240, '191216': 1241, '191222': 1242, '191404': 1243, '1919': 1244, '19231': 1245, '192734': 1246, '192850': 1247, '192870': 1248, '193347': 1249, '193352': 1250, '193354': 1251, '193742': 1252, '193918': 1253, '193931': 1254, '193932': 1255, '194223': 1256, '194609': 1257, '194617': 1258, '194645': 1259, '1949': 1260, '1951': 1261, '195150': 1262, '1952': 1263, '1953': 1264, '195361': 1265, '1955': 1266, '1956': 1267, '195792': 1268, '1959': 1269, '19621': 1270, '19697': 1271, '197054': 1272, '197452': 1273, '197783': 1274, '198443': 1275, '198653': 1276, '198866': 1277, '199571': 1278, '1997': 1279, '1999': 1280, '200480': 1281, '200630': 1282, '20178': 1283, '20179': 1284, '20180': 1285, '20193': 1286, '202520': 1287, '202522': 1288, '202639': 1289, '203646': 1290, '205192': 1291, '205196': 1292, '20526': 1293, '20528': 1294, '20534': 1295, '20584': 1296, '20592': 1297, '20593': 1298, '20601': 1299, '20602': 1300, '206259': 1301, '206371': 1302, '206524': 1303, '207395': 1304, '20821': 1305, '20833': 1306, '208345': 1307, '20850': 1308, '20857': 1309, '20920': 1310, '20923': 1311, '20924': 1312, '20942': 1313, '20972': 1314, '210309': 1315, '210871': 1316, '210872': 1317, '211432': 1318, '211875': 1319, '211906': 1320, '212097': 1321, '212107': 1322, '212777': 1323, '212930': 1324, '213246': 1325, '213279': 1326, '214472': 1327, '215912': 1328, '216877': 1329, '216878': 1330, '217115': 1331, '217139': 1332, '217852': 1333, '217984': 1334, '218410': 1335, '218666': 1336, '218682': 1337, '219218': 1338, '219239': 1339, '219446': 1340, '219976': 1341, '220420': 1342, '221302': 1343, '22229': 1344, '22241': 1345, '22386': 1346, '22431': 1347, '22563': 1348, '22564': 1349, '22566': 1350, '226698': 1351, '227178': 1352, '227286': 1353, '22835': 1354, '22869': 1355, '22874': 1356, '22875': 1357, '22876': 1358, '22883': 1359, '22886': 1360, '228990': 1361, '228992': 1362, '229635': 1363, '230300': 1364, '23069': 1365, '23070': 1366, '230879': 1367, '230884': 1368, '23116': 1369, '231198': 1370, '231249': 1371, '23258': 1372, '232605': 1373, '232606': 1374, '232860': 1375, '233106': 1376, '23448': 1377, '23502': 1378, '23507': 1379, '2354': 1380, '23545': 1381, '23546': 1382, '235670': 1383, '235678': 1384, '235679': 1385, '235683': 1386, '235776': 1387, '236759': 1388, '237376': 1389, '23738': 1390, '237489': 1391, '237521': 1392, '23774': 1393, '238099': 1394, '238401': 1395, '239800': 1396, '239810': 1397, '239829': 1398, '240321': 1399, '24043': 1400, '240791': 1401, '241133': 1402, '241821': 1403, '242637': 1404, '242663': 1405, '243274': 1406, '243483': 1407, '2440': 1408, '24476': 1409, '245288': 1410, '24530': 1411, '245955': 1412, '246618': 1413, '248119': 1414, '248395': 1415, '248425': 1416, '248431': 1417, '248823': 1418, '249421': 1419, '24966': 1420, '24974': 1421, '249858': 1422, '250566': 1423, '251756': 1424, '25181': 1425, '25184': 1426, '252715': 1427, '252725': 1428, '253762': 1429, '253971': 1430, '25413': 1431, '254923': 1432, '255233': 1433, '255628': 1434, '256106': 1435, '25702': 1436, '25772': 1437, '25791': 1438, '25794': 1439, '25805': 1440, '258259': 1441, '259126': 1442, '259701': 1443, '259702': 1444, '259772': 1445, '260121': 1446, '260979': 1447, '261040': 1448, '262108': 1449, '262121': 1450, '262178': 1451, '263069': 1452, '263279': 1453, '263482': 1454, '263486': 1455, '263498': 1456, '263553': 1457, '264347': 1458, '264556': 1459, '265203': 1460, '2653': 1461, '2654': 1462, '2658': 1463, '2663': 1464, '2665': 1465, '267003': 1466, '267824': 1467, '26850': 1468, '2695': 1469, '2696': 1470, '2698': 1471, '270085': 1472, '2702': 1473, '270456': 1474, '270600': 1475, '27174': 1476, '27199': 1477, '27203': 1478, '27230': 1479, '272345': 1480, '27241': 1481, '27243': 1482, '27246': 1483, '27249': 1484, '27250': 1485, '272720': 1486, '273152': 1487, '273949': 1488, '27510': 1489, '27514': 1490, '27530': 1491, '27531': 1492, '27535': 1493, '27543': 1494, '27606': 1495, '27612': 1496, '27623': 1497, '27627': 1498, '27631': 1499, '27632': 1500, '277263': 1501, '278394': 1502, '278403': 1503, '27895': 1504, '28026': 1505, '280876': 1506, '28202': 1507, '28227': 1508, '28230': 1509, '28249': 1510, '28254': 1511, '28265': 1512, '28267': 1513, '282700': 1514, '28278': 1515, '28287': 1516, '28290': 1517, '28336': 1518, '28350': 1519, '28359': 1520, '28385': 1521, '28387': 1522, '28389': 1523, '284023': 1524, '284025': 1525, '28412': 1526, '284414': 1527, '28447': 1528, '28456': 1529, '28471': 1530, '28473': 1531, '28485': 1532, '28487': 1533, '28489': 1534, '28491': 1535, '28504': 1536, '28542': 1537, '285675': 1538, '285687': 1539, '28632': 1540, '28640': 1541, '28641': 1542, '28649': 1543, '286500': 1544, '286513': 1545, '286562': 1546, '28674': 1547, '287787': 1548, '288': 1549, '288107': 1550, '28851': 1551, '289085': 1552, '289088': 1553, '28957': 1554, '28964': 1555, '289779': 1556, '289780': 1557, '289781': 1558, '289885': 1559, '289945': 1560, '292277': 1561, '293271': 1562, '293285': 1563, '293974': 1564, '294030': 1565, '294126': 1566, '294145': 1567, '294239': 1568, '29492': 1569, '29708': 1570, '29723': 1571, '29738': 1572, '299195': 1573, '299197': 1574, '300071': 1575, '300806': 1576, '302545': 1577, '307015': 1578, '307336': 1579, '307656': 1580, '308003': 1581, '30817': 1582, '308232': 1583, '3084': 1584, '3085': 1585, '308529': 1586, '308920': 1587, '30895': 1588, '30901': 1589, '30934': 1590, '309476': 1591, '3095': 1592, '3097': 1593, '30973': 1594, '3101': 1595, '31043': 1596, '310530': 1597, '31055': 1598, '310653': 1599, '310742': 1600, '31083': 1601, '31097': 1602, '31105': 1603, '3112': 1604, '312409': 1605, '31336': 1606, '31349': 1607, '31353': 1608, '314459': 1609, '31479': 1610, '31483': 1611, '31489': 1612, '315266': 1613, '315789': 1614, '31769': 1615, '318071': 1616, '318187': 1617, '31863': 1618, '3187': 1619, '3191': 1620, '3192': 1621, '31927': 1622, '31932': 1623, '32083': 1624, '321004': 1625, '3217': 1626, '3218': 1627, '321861': 1628, '3220': 1629, '3222': 1630, '3223': 1631, '32260': 1632, '32276': 1633, '3229': 1634, '3231': 1635, '323128': 1636, '3232': 1637, '3233': 1638, '3235': 1639, '3236': 1640, '3237': 1641, '3240': 1642, '3243': 1643, '325314': 1644, '325497': 1645, '32688': 1646, '32698': 1647, '328370': 1648, '32872': 1649, '33013': 1650, '330148': 1651, '330208': 1652, '33231': 1653, '33301': 1654, '33303': 1655, '33325': 1656, '33412': 1657, '334153': 1658, '335042': 1659, '335733': 1660, '337766': 1661, '33818': 1662, '33823': 1663, '33895': 1664, '33904': 1665, '33907': 1666, '340075': 1667, '340078': 1668, '340299': 1669, '34082': 1670, '341188': 1671, '34257': 1672, '34263': 1673, '34266': 1674, '342802': 1675, '34315': 1676, '34355': 1677, '345340': 1678, '346243': 1679, '346292': 1680, '34708': 1681, '348305': 1682, '348437': 1683, '34961': 1684, '34979': 1685, '35': 1686, '350319': 1687, '350362': 1688, '350373': 1689, '35061': 1690, '35070': 1691, '35335': 1692, '35343': 1693, '353541': 1694, '354004': 1695, '35490': 1696, '35778': 1697, '35797': 1698, '35852': 1699, '35854': 1700, '35863': 1701, '358866': 1702, '358884': 1703, '358887': 1704, '358894': 1705, '35905': 1706, '359067': 1707, '35922': 1708, '360028': 1709, '36131': 1710, '36140': 1711, '36145': 1712, '36162': 1713, '36167': 1714, '362926': 1715, '365294': 1716, '36620': 1717, '367312': 1718, '36802': 1719, '368431': 1720, '368605': 1721, '368657': 1722, '370366': 1723, '372862': 1724, '37483': 1725, '37541': 1726, '375605': 1727, '375825': 1728, '376704': 1729, '377303': 1730, '37879': 1731, '37884': 1732, '37888': 1733, '379288': 1734, '37998': 1735, '38000': 1736, '380341': 1737, '38205': 1738, '3828': 1739, '384428': 1740, '38480': 1741, '385067': 1742, '385251': 1743, '38537': 1744, '385572': 1745, '38722': 1746, '38771': 1747, '387795': 1748, '38829': 1749, '38839': 1750, '38845': 1751, '38846': 1752, '389715': 1753, '390693': 1754, '390889': 1755, '390894': 1756, '390896': 1757, '390922': 1758, '39124': 1759, '39126': 1760, '39127': 1761, '39130': 1762, '39131': 1763, '39165': 1764, '39199': 1765, '39210': 1766, '3932': 1767, '39403': 1768, '39474': 1769, '395075': 1770, '395540': 1771, '395547': 1772, '395553': 1773, '395725': 1774, '396412': 1775, '397488': 1776, '397590': 1777, '39890': 1778, '39904': 1779, '399173': 1780, '399339': 1781, '399370': 1782, '40': 1783, '400356': 1784, '400455': 1785, '400473': 1786, '40124': 1787, '40125': 1788, '40131': 1789, '40135': 1790, '40151': 1791, '40583': 1792, '40605': 1793, '40886': 1794, '408885': 1795, '40922': 1796, '409255': 1797, '409725': 1798, '411005': 1799, '411092': 1800, '41216': 1801, '41417': 1802, '415693': 1803, '416455': 1804, '41666': 1805, '416867': 1806, '416964': 1807, '417017': 1808, '41714': 1809, '41732': 1810, '421481': 1811, '42156': 1812, '42207': 1813, '42209': 1814, '42221': 1815, '423463': 1816, '423816': 1817, '424': 1818, '424540': 1819, '4274': 1820, '427606': 1821, '42847': 1822, '42848': 1823, '428610': 1824, '429781': 1825, '429805': 1826, '430329': 1827, '430574': 1828, '430711': 1829, '431206': 1830, '43165': 1831, '43186': 1832, '4329': 1833, '4330': 1834, '4335': 1835, '434': 1836, '43639': 1837, '436796': 1838, '43698': 1839, '44017': 1840, '440815': 1841, '44121': 1842, '44368': 1843, '444191': 1844, '444240': 1845, '44455': 1846, '44514': 1847, '445938': 1848, '446271': 1849, '446610': 1850, '447224': 1851, '447250': 1852, '449841': 1853, '45052': 1854, '45061': 1855, '45188': 1856, '45189': 1857, '45212': 1858, '4553': 1859, '45533': 1860, '45599': 1861, '45603': 1862, '45605': 1863, '4584': 1864, '458439': 1865, '459206': 1866, '459213': 1867, '459214': 1868, '459216': 1869, '46079': 1870, '463': 1871, '4637': 1872, '463825': 1873, '46431': 1874, '46452': 1875, '46468': 1876, '46470': 1877, '46476': 1878, '4649': 1879, '46491': 1880, '46500': 1881, '46501': 1882, '46536': 1883, '46547': 1884, '4660': 1885, '466170': 1886, '467383': 1887, '46887': 1888, '469504': 1889, '470511': 1890, '47570': 1891, '47682': 1892, '47683': 1893, '47684': 1894, '47839': 1895, '4804': 1896, '48066': 1897, '48075': 1898, '481073': 1899, '48550': 1900, '48555': 1901, '486840': 1902, '48764': 1903, '48766': 1904, '48768': 1905, '4878': 1906, '48781': 1907, '49482': 1908, '49660': 1909, '49720': 1910, '49753': 1911, '49811': 1912, '4983': 1913, '49843': 1914, '49844': 1915, '49847': 1916, '49895': 1917, '502574': 1918, '50336': 1919, '50337': 1920, '50354': 1921, '5038': 1922, '50381': 1923, '503871': 1924, '503877': 1925, '503883': 1926, '503893': 1927, '504': 1928, '5055': 1929, '506': 1930, '5062': 1931, '5064': 1932, '5069': 1933, '5075': 1934, '50807': 1935, '50838': 1936, '5086': 1937, '509233': 1938, '509315': 1939, '509379': 1940, '50980': 1941, '51045': 1942, '51049': 1943, '51052': 1944, '510715': 1945, '510718': 1946, '51180': 1947, '513189': 1948, '51831': 1949, '51834': 1950, '51866': 1951, '51879': 1952, '51909': 1953, '519318': 1954, '51934': 1955, '519353': 1956, '5194': 1957, '52000': 1958, '52003': 1959, '52007': 1960, '520471': 1961, '521183': 1962, '521207': 1963, '521251': 1964, '521252': 1965, '521269': 1966, '521855': 1967, '522338': 1968, '523010': 1969, '523394': 1970, '523574': 1971, '52515': 1972, '52784': 1973, '52835': 1974, '52847': 1975, '529165': 1976, '531348': 1977, '531351': 1978, '5348': 1979, '53942': 1980, '54129': 1981, '54131': 1982, '54132': 1983, '5454': 1984, '54550': 1985, '545647': 1986, '5462': 1987, '54844': 1988, '552469': 1989, '55403': 1990, '55770': 1991, '55801': 1992, '55968': 1993, '559804': 1994, '5600': 1995, '560936': 1996, '56112': 1997, '56115': 1998, '56119': 1999, '561238': 2000, '561364': 2001, '561568': 2002, '561581': 2003, '561582': 2004, '561593': 2005, '561595': 2006, '561610': 2007, '561611': 2008, '561613': 2009, '56167': 2010, '561674': 2011, '561789': 2012, '561809': 2013, '562067': 2014, '562123': 2015, '562940': 2016, '563613': 2017, '566488': 2018, '566653': 2019, '566664': 2020, '567005': 2021, '567018': 2022, '56708': 2023, '56709': 2024, '568045': 2025, '568857': 2026, '57119': 2027, '573535': 2028, '573553': 2029, '573964': 2030, '573978': 2031, '574009': 2032, '574264': 2033, '574462': 2034, '574710': 2035, '575077': 2036, '575292': 2037, '575331': 2038, '575402': 2039, '575795': 2040, '576257': 2041, '576362': 2042, '576691': 2043, '576725': 2044, '576795': 2045, '576973': 2046, '577086': 2047, '577227': 2048, '577331': 2049, '57764': 2050, '57773': 2051, '578306': 2052, '578309': 2053, '578337': 2054, '578347': 2055, '578365': 2056, '578645': 2057, '578646': 2058, '578649': 2059, '578650': 2060, '578669': 2061, '578780': 2062, '578845': 2063, '578898': 2064, '579008': 2065, '579108': 2066, '57922': 2067, '57932': 2068, '57948': 2069, '582139': 2070, '582343': 2071, '582349': 2072, '582511': 2073, '58268': 2074, '583318': 2075, '58436': 2076, '58453': 2077, '58454': 2078, '58540': 2079, '58552': 2080, '5869': 2081, '58758': 2082, '589923': 2083, '590022': 2084, '59045': 2085, '591016': 2086, '591017': 2087, '59244': 2088, '592826': 2089, '592830': 2090, '592973': 2091, '592975': 2092, '592986': 2093, '592993': 2094, '592996': 2095, '593022': 2096, '593060': 2097, '593068': 2098, '593091': 2099, '593104': 2100, '593105': 2101, '593155': 2102, '593201': 2103, '593209': 2104, '593210': 2105, '593240': 2106, '593248': 2107, '593260': 2108, '593328': 2109, '593329': 2110, '593544': 2111, '593559': 2112, '593560': 2113, '593813': 2114, '593859': 2115, '593921': 2116, '593942': 2117, '594011': 2118, '594025': 2119, '594039': 2120, '594047': 2121, '594119': 2122, '594387': 2123, '594483': 2124, '594511': 2125, '594543': 2126, '594649': 2127, '594900': 2128, '595056': 2129, '595063': 2130, '595157': 2131, '595193': 2132, '5959': 2133, '596075': 2134, '59626': 2135, '5966': 2136, '59715': 2137, '59772': 2138, '59798': 2139, '601462': 2140, '601561': 2141, '601567': 2142, '60159': 2143, '60169': 2144, '60170': 2145, '604073': 2146, '60560': 2147, '606479': 2148, '606647': 2149, '60682': 2150, '608190': 2151, '608191': 2152, '608292': 2153, '608326': 2154, '610529': 2155, '61069': 2156, '61073': 2157, '612306': 2158, '6125': 2159, '6130': 2160, '61312': 2161, '613409': 2162, '61417': 2163, '6151': 2164, '6152': 2165, '6155': 2166, '6163': 2167, '616336': 2168, '6169': 2169, '6170': 2170, '617378': 2171, '617575': 2172, '6184': 2173, '6196': 2174, '6209': 2175, '6210': 2176, '6213': 2177, '6214': 2178, '6215': 2179, '621555': 2180, '6216': 2181, '6217': 2182, '6220': 2183, '6224': 2184, '62274': 2185, '62329': 2186, '62333': 2187, '62347': 2188, '6238': 2189, '62389': 2190, '62417': 2191, '62607': 2192, '62634': 2193, '626530': 2194, '626531': 2195, '626574': 2196, '62676': 2197, '626999': 2198, '627024': 2199, '62718': 2200, '628458': 2201, '628459': 2202, '628500': 2203, '628667': 2204, '628668': 2205, '628751': 2206, '628764': 2207, '628766': 2208, '628815': 2209, '628888': 2210, '630817': 2211, '630890': 2212, '631015': 2213, '631052': 2214, '6311': 2215, '6318': 2216, '632796': 2217, '632874': 2218, '632935': 2219, '633030': 2220, '633031': 2221, '633081': 2222, '6334': 2223, '633585': 2224, '633721': 2225, '6343': 2226, '6344': 2227, '6346': 2228, '63477': 2229, '63486': 2230, '634902': 2231, '634904': 2232, '634938': 2233, '634975': 2234, '63549': 2235, '636098': 2236, '636500': 2237, '636511': 2238, '6378': 2239, '63812': 2240, '63832': 2241, '63835': 2242, '6385': 2243, '63915': 2244, '63931': 2245, '640617': 2246, '641956': 2247, '641976': 2248, '642593': 2249, '642621': 2250, '642641': 2251, '642681': 2252, '64271': 2253, '642798': 2254, '642827': 2255, '642847': 2256, '642894': 2257, '642920': 2258, '642930': 2259, '643003': 2260, '643069': 2261, '64319': 2262, '643199': 2263, '643221': 2264, '643239': 2265, '643485': 2266, '643597': 2267, '643695': 2268, '643734': 2269, '643735': 2270, '643777': 2271, '644093': 2272, '644334': 2273, '644361': 2274, '644363': 2275, '644427': 2276, '644441': 2277, '644448': 2278, '644470': 2279, '644494': 2280, '644577': 2281, '64484': 2282, '644843': 2283, '645016': 2284, '645046': 2285, '645084': 2286, '645088': 2287, '64519': 2288, '645452': 2289, '645571': 2290, '645870': 2291, '645897': 2292, '646195': 2293, '646286': 2294, '646289': 2295, '646334': 2296, '646357': 2297, '646412': 2298, '646440': 2299, '646809': 2300, '646836': 2301, '646837': 2302, '646900': 2303, '646913': 2304, '647315': 2305, '647408': 2306, '647413': 2307, '647447': 2308, '648106': 2309, '648112': 2310, '648121': 2311, '648232': 2312, '648369': 2313, '649730': 2314, '649731': 2315, '649739': 2316, '649944': 2317, '65057': 2318, '65074': 2319, '650807': 2320, '650814': 2321, '650834': 2322, '65212': 2323, '653441': 2324, '653628': 2325, '6539': 2326, '654177': 2327, '654326': 2328, '654339': 2329, '654519': 2330, '656048': 2331, '656231': 2332, '65650': 2333, '65653': 2334, '662250': 2335, '662279': 2336, '662416': 2337, '662572': 2338, '6639': 2339, '66556': 2340, '66563': 2341, '66564': 2342, '66594': 2343, '66596': 2344, '66751': 2345, '66782': 2346, '66794': 2347, '66805': 2348, '66809': 2349, '66982': 2350, '66986': 2351, '66990': 2352, '671052': 2353, '671269': 2354, '671293': 2355, '672064': 2356, '672070': 2357, '672071': 2358, '67245': 2359, '67246': 2360, '67292': 2361, '6741': 2362, '67415': 2363, '675649': 2364, '675756': 2365, '67584': 2366, '675847': 2367, '67633': 2368, '6767': 2369, '6771': 2370, '6775': 2371, '6782': 2372, '6784': 2373, '6786': 2374, '68115': 2375, '6814': 2376, '6818': 2377, '68224': 2378, '682508': 2379, '682666': 2380, '682815': 2381, '683294': 2382, '683355': 2383, '683360': 2384, '683404': 2385, '684372': 2386, '684531': 2387, '68463': 2388, '68495': 2389, '684972': 2390, '684986': 2391, '68505': 2392, '686015': 2393, '686030': 2394, '686061': 2395, '686532': 2396, '686559': 2397, '687401': 2398, '688361': 2399, '688824': 2400, '688849': 2401, '689152': 2402, '689439': 2403, '6898': 2404, '6910': 2405, '6913': 2406, '6917': 2407, '69198': 2408, '6923': 2409, '6925': 2410, '69284': 2411, '69296': 2412, '693143': 2413, '6935': 2414, '6939': 2415, '69392': 2416, '69397': 2417, '6941': 2418, '69418': 2419, '694759': 2420, '695284': 2421, '696342': 2422, '696343': 2423, '696345': 2424, '696346': 2425, '7022': 2426, '70281': 2427, '7032': 2428, '703953': 2429, '7041': 2430, '70441': 2431, '70442': 2432, '70444': 2433, '7047': 2434, '70520': 2435, '708945': 2436, '709113': 2437, '709518': 2438, '70970': 2439, '711527': 2440, '711598': 2441, '711994': 2442, '71336': 2443, '714208': 2444, '714256': 2445, '714260': 2446, '714289': 2447, '714748': 2448, '714879': 2449, '714975': 2450, '71736': 2451, '71904': 2452, '72056': 2453, '72101': 2454, '72406': 2455, '7272': 2456, '7276': 2457, '72805': 2458, '72908': 2459, '7296': 2460, '7297': 2461, '73119': 2462, '73146': 2463, '73162': 2464, '733167': 2465, '73323': 2466, '73327': 2467, '733534': 2468, '733576': 2469, '734406': 2470, '735303': 2471, '735311': 2472, '73712': 2473, '737204': 2474, '738941': 2475, '739280': 2476, '739707': 2477, '73972': 2478, '739816': 2479, '7419': 2480, '7430': 2481, '7432': 2482, '74427': 2483, '746058': 2484, '74698': 2485, '74700': 2486, '74749': 2487, '74821': 2488, '74920': 2489, '74921': 2490, '74937': 2491, '74975': 2492, '75121': 2493, '751408': 2494, '752684': 2495, '753047': 2496, '753070': 2497, '75318': 2498, '7532': 2499, '753264': 2500, '753265': 2501, '7537': 2502, '754594': 2503, '755082': 2504, '755217': 2505, '756061': 2506, '75674': 2507, '75691': 2508, '75693': 2509, '75694': 2510, '75695': 2511, '75969': 2512, '75972': 2513, '75983': 2514, '762980': 2515, '763009': 2516, '763010': 2517, '763181': 2518, '767763': 2519, '77108': 2520, '77112': 2521, '77438': 2522, '77515': 2523, '77758': 2524, '77826': 2525, '77829': 2526, '779960': 2527, '782486': 2528, '78508': 2529, '78511': 2530, '78549': 2531, '78552': 2532, '78555': 2533, '78557': 2534, '785678': 2535, '7867': 2536, '787016': 2537, '78994': 2538, '79809': 2539, '79817': 2540, '801170': 2541, '80491': 2542, '80515': 2543, '80656': 2544, '8079': 2545, '81350': 2546, '814836': 2547, '815073': 2548, '815096': 2549, '81714': 2550, '81722': 2551, '817774': 2552, '820661': 2553, '820662': 2554, '82087': 2555, '82090': 2556, '82098': 2557, '8213': 2558, '8224': 2559, '824245': 2560, '82664': 2561, '82666': 2562, '82920': 2563, '83449': 2564, '83461': 2565, '83725': 2566, '83746': 2567, '83826': 2568, '83847': 2569, '84020': 2570, '84021': 2571, '84459': 2572, '84695': 2573, '851968': 2574, '85299': 2575, '853114': 2576, '853115': 2577, '853116': 2578, '853118': 2579, '853150': 2580, '853155': 2581, '85324': 2582, '85352': 2583, '854434': 2584, '85449': 2585, '85452': 2586, '85688': 2587, '8581': 2588, '8591': 2589, '8594': 2590, '8617': 2591, '8619': 2592, '86258': 2593, '86359': 2594, '86840': 2595, '8687': 2596, '86923': 2597, '8696': 2598, '8699': 2599, '8703': 2600, '87363': 2601, '87417': 2602, '87482': 2603, '8766': 2604, '87915': 2605, '8821': 2606, '8832': 2607, '88356': 2608, '884094': 2609, '8865': 2610, '887': 2611, '8872': 2612, '8874': 2613, '8875': 2614, '892139': 2615, '89308': 2616, '89335': 2617, '89416': 2618, '89547': 2619, '8961': 2620, '899085': 2621, '899119': 2622, '90470': 2623, '906': 2624, '90655': 2625, '907845': 2626, '90888': 2627, '910': 2628, '91038': 2629, '911198': 2630, '91581': 2631, '917493': 2632, '91852': 2633, '91853': 2634, '91975': 2635, '919885': 2636, '92065': 2637, '92589': 2638, '928873': 2639, '93273': 2640, '93318': 2641, '93320': 2642, '93555': 2643, '936': 2644, '93755': 2645, '93923': 2646, '940': 2647, '941': 2648, '94229': 2649, '943': 2650, '943087': 2651, '94416': 2652, '94639': 2653, '94641': 2654, '94713': 2655, '948147': 2656, '948299': 2657, '948846': 2658, '949217': 2659, '949318': 2660, '949511': 2661, '94953': 2662, '950052': 2663, '950305': 2664, '950986': 2665, '9513': 2666, '9515': 2667, '95188': 2668, '95198': 2669, '95225': 2670, '954315': 2671, '95435': 2672, '95579': 2673, '95586': 2674, '95588': 2675, '95589': 2676, '9559': 2677, '95594': 2678, '95597': 2679, '95642': 2680, '95718': 2681, '95719': 2682, '9581': 2683, '9586': 2684, '96335': 2685, '964248': 2686, '96845': 2687, '96847': 2688, '96851': 2689, '9708': 2690, '9716': 2691, '97377': 2692, '97390': 2693, '975567': 2694, '976284': 2695, '976334': 2696, '97645': 2697, '97892': 2698, '98693': 2699, '98698': 2700, '987188': 2701, '987197': 2702, '989397': 2703, '990075': 2704, '99023': 2705, '99025': 2706, '99030': 2707}\n",
            "2708\n"
          ]
        }
      ],
      "source": [
        "mynode=list(data_s['ID'])\n",
        "\n",
        "#print(mynode)\n",
        "node_ID1=np.unique(mynode)\n",
        "node_ID= {node: i for i, node in enumerate(node_ID1)}\n",
        "print(node_ID)\n",
        "n=len(node_ID)\n",
        "print(n)"
      ],
      "id": "66c45ee1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1cbZCFFLW05o",
        "outputId": "626778f6-350f-4911-d37e-c86ca4e88408"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "19717"
            ]
          },
          "execution_count": 114,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(set(node_ID.keys()))"
      ],
      "id": "1cbZCFFLW05o"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da77ba61",
        "outputId": "22d529db-874a-49a4-cdb2-037e9d7f3c0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " The adjacency Matrix\n",
            "=======================\n",
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "A=np.zeros((n,n), dtype=int)\n",
        "i=0\n",
        "for d in mylist:\n",
        "    d=mylist[i].split(\"\\t\")\n",
        "    \n",
        "    if d[0] in node_ID and d[1] in node_ID:\n",
        "      \n",
        "      A[node_ID[d[0]]][node_ID[d[1]]] = 1\n",
        "    i=i+1;\n",
        "print(\" The adjacency Matrix\")\n",
        "print(\"=======================\")\n",
        "print(A)"
      ],
      "id": "da77ba61"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "vugA69VeABUC",
        "outputId": "24a47aa3-2415-4c80-c5f2-3347e7024177"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-dcf7b567-0bf0-4e96-8305-e1a2e264bf87\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ID</th>\n",
              "      <th>w_0</th>\n",
              "      <th>w_1</th>\n",
              "      <th>w_2</th>\n",
              "      <th>w_3</th>\n",
              "      <th>w_4</th>\n",
              "      <th>w_5</th>\n",
              "      <th>w_6</th>\n",
              "      <th>w_7</th>\n",
              "      <th>w_8</th>\n",
              "      <th>...</th>\n",
              "      <th>w_3694</th>\n",
              "      <th>w_3695</th>\n",
              "      <th>w_3696</th>\n",
              "      <th>w_3697</th>\n",
              "      <th>w_3698</th>\n",
              "      <th>w_3699</th>\n",
              "      <th>w_3700</th>\n",
              "      <th>w_3701</th>\n",
              "      <th>w_3702</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>100157</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>100598</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2177</th>\n",
              "      <td>101570</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1011</th>\n",
              "      <td>10227</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2178</th>\n",
              "      <td>102637</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1008</th>\n",
              "      <td>zhu00incorporating</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2175</th>\n",
              "      <td>zhu00segmenting</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2176</th>\n",
              "      <td>zhu98line</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1009</th>\n",
              "      <td>zini01caselp</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1010</th>\n",
              "      <td>zunino01representing</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3312 rows × 3705 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-dcf7b567-0bf0-4e96-8305-e1a2e264bf87')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-dcf7b567-0bf0-4e96-8305-e1a2e264bf87 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-dcf7b567-0bf0-4e96-8305-e1a2e264bf87');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                        ID  w_0  w_1  w_2  w_3  w_4  w_5  w_6  w_7  w_8  ...  \\\n",
              "0                   100157    0    0    0    0    0    0    0    0    0  ...   \n",
              "1                   100598    0    0    0    0    0    0    0    0    0  ...   \n",
              "2177                101570    0    0    0    0    0    0    0    0    0  ...   \n",
              "1011                 10227    0    0    0    0    0    0    0    0    0  ...   \n",
              "2178                102637    0    0    0    0    0    0    0    0    0  ...   \n",
              "...                    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
              "1008    zhu00incorporating    0    0    0    0    0    0    0    0    0  ...   \n",
              "2175       zhu00segmenting    0    0    0    0    0    0    0    0    0  ...   \n",
              "2176             zhu98line    0    0    0    0    0    0    0    0    0  ...   \n",
              "1009          zini01caselp    0    0    0    0    0    0    0    0    0  ...   \n",
              "1010  zunino01representing    0    0    0    0    0    0    0    0    0  ...   \n",
              "\n",
              "      w_3694  w_3695  w_3696  w_3697  w_3698  w_3699  w_3700  w_3701  w_3702  \\\n",
              "0          0       0       0       0       0       0       0       0       0   \n",
              "1          0       0       0       0       0       0       0       0       0   \n",
              "2177       0       0       0       0       0       0       0       0       0   \n",
              "1011       0       0       0       0       0       0       0       0       0   \n",
              "2178       0       0       0       0       0       0       0       0       0   \n",
              "...      ...     ...     ...     ...     ...     ...     ...     ...     ...   \n",
              "1008       0       0       0       0       0       0       0       0       0   \n",
              "2175       0       0       0       0       0       0       0       0       0   \n",
              "2176       0       0       0       0       0       0       0       0       0   \n",
              "1009       0       0       0       0       1       0       0       0       0   \n",
              "1010       0       0       0       0       0       0       0       0       0   \n",
              "\n",
              "      class  \n",
              "0         0  \n",
              "1         3  \n",
              "2177      4  \n",
              "1011      4  \n",
              "2178      1  \n",
              "...     ...  \n",
              "1008      3  \n",
              "2175      5  \n",
              "2176      4  \n",
              "1009      0  \n",
              "1010      0  \n",
              "\n",
              "[3312 rows x 3705 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_s"
      ],
      "id": "vugA69VeABUC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pJQMqGlutVF",
        "outputId": "d41e2b1c-86d4-40d1-8f07-03f5c96790c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "NODE\tpaper\n",
            "\n",
            "cat=1,2,3:label\tnumeric:w-rat:0.0\tnumeric:w-common:0.0\tnumeric:w-use:0.0\tnumeric:w-examin:0.0\tnumeric:w-pathogenesi:0.0\tnumeric:w-retinopathi:0.0\tnumeric:w-mous:0.0\tnumeric:w-studi:0.0\tnumeric:w-anim:0.0\tnumeric:w-model:0.0\tnumeric:w-metabol:0.0\tnumeric:w-abnorm:0.0\tnumeric:w-contribut:0.0\tnumeric:w-develop:0.0\tnumeric:w-investig:0.0\tnumeric:w-mice:0.0\tnumeric:w-2:0.0\tnumeric:w-month:0.0\tnumeric:w-compar:0.0\tnumeric:w-obtain:0.0\tnumeric:w-method:0.0\tnumeric:w-induc:0.0\tnumeric:w-6:0.0\tnumeric:w-inject:0.0\tnumeric:w-experiment:0.0\tnumeric:w-normal:0.0\tnumeric:w-diet:0.0\tnumeric:w-30:0.0\tnumeric:w-hyperglycemia:0.0\tnumeric:w-level:0.0\tnumeric:w-lipid:0.0\tnumeric:w-oxid:0.0\tnumeric:w-activ:0.0\tnumeric:w-protein:0.0\tnumeric:w-kinas:0.0\tnumeric:w-c:0.0\tnumeric:w-measur:0.0\tnumeric:w-result:0.0\tnumeric:w-increas:0.0\tnumeric:w-retin:0.0\tnumeric:w-stress:0.0\tnumeric:w-3:0.0\tnumeric:w-similar:0.0\tnumeric:w-observ:0.0\tnumeric:w-conclus:0.0\tnumeric:w-play:0.0\tnumeric:w-import:0.0\tnumeric:w-role:0.0\tnumeric:w-present:0.0\tnumeric:w-p:0.0\tnumeric:w-m:0.0\tnumeric:w-r:0.0\tnumeric:w-muscl:0.0\tnumeric:w-control:0.0\tnumeric:w-chang:0.0\tnumeric:w-dure:0.0\tnumeric:w-lower:0.0\tnumeric:w-higher:0.0\tnumeric:w-mass:0.0\tnumeric:w-correl:0.0\tnumeric:w-decreas:0.0\tnumeric:w-determin:0.0\tnumeric:w-concentr:0.0\tnumeric:w-stimul:0.0\tnumeric:w-period:0.0\tnumeric:w-caus:0.0\tnumeric:w-mark:0.0\tnumeric:w-group:0.0\tnumeric:w-evid:0.0\tnumeric:w-fast:0.0\tnumeric:w-type:0.0\tnumeric:w-signific:0.0\tnumeric:w-differ:0.0\tnumeric:w-ratio:0.0\tnumeric:w-suggest:0.0\tnumeric:w-degre:0.0\tnumeric:w-occur:0.0\tnumeric:w-vivo:0.0\tnumeric:w-respect:0.0\tnumeric:w-dysfunct:0.0\tnumeric:w-region:0.0\tnumeric:w-high:0.0\tnumeric:w-appear:0.0\tnumeric:w-sever:0.0\tnumeric:w-affect:0.0\tnumeric:w-cardiovascular:0.0\tnumeric:w-complic:0.0\tnumeric:w-primari:0.0\tnumeric:w-death:0.0\tnumeric:w-patient:0.0\tnumeric:w-clinic:0.0\tnumeric:w-suscept:0.0\tnumeric:w-cardiac:0.0\tnumeric:w-tissu:0.0\tnumeric:w-specif:0.0\tnumeric:w-function:0.0\tnumeric:w-defect:0.0\tnumeric:w-possibl:0.0\tnumeric:w-indic:0.0\tnumeric:w-state:0.0\tnumeric:w-onli:0.0\tnumeric:w-bodi:0.0\tnumeric:w-weight:0.0\tnumeric:w-loss:0.0\tnumeric:w-valu:0.0\tnumeric:w-howev:0.0\tnumeric:w-4:0.0\tnumeric:w-condit:0.0\tnumeric:w-durat:0.0\tnumeric:w-8:0.0\tnumeric:w-week:0.0\tnumeric:w-onset:0.0\tnumeric:w-data:0.0\tnumeric:w-direct:0.0\tnumeric:w-report:0.0\tnumeric:w-provid:0.0\tnumeric:w-addit:0.0\tnumeric:w-evalu:0.0\tnumeric:w-sensit:0.0\tnumeric:w-heart:0.0\tnumeric:w-object:0.0\tnumeric:w-mean:0.0\tnumeric:w-blood:0.0\tnumeric:w-glucos:0.0\tnumeric:w-strong:0.0\tnumeric:w-hba:0.0\tnumeric:w-1c:0.0\tnumeric:w-a1c:0.0\tnumeric:w-variabl:0.0\tnumeric:w-independ:0.0\tnumeric:w-assess:0.0\tnumeric:w-relat:0.0\tnumeric:w-trial:0.0\tnumeric:w-research:0.0\tnumeric:w-design:0.0\tnumeric:w-profil:0.0\tnumeric:w-sampl:0.0\tnumeric:w-particip:0.0\tnumeric:w-n:0.0\tnumeric:w-1:0.0\tnumeric:w-consist:0.0\tnumeric:w-befor:0.0\tnumeric:w-min:0.0\tnumeric:w-predict:0.0\tnumeric:w-adjust:0.0\tnumeric:w-sex:0.0\tnumeric:w-treatment:0.0\tnumeric:w-7:0.0\tnumeric:w-gt:0.0\tnumeric:w-0:0.0\tnumeric:w-larg:0.0\tnumeric:w-influenc:0.0\tnumeric:w-base:0.0\tnumeric:w-standard:0.0\tnumeric:w-14:0.0\tnumeric:w-10:0.0\tnumeric:w-wherea:0.0\tnumeric:w-enhanc:0.0\tnumeric:w-manag:0.0\tnumeric:w-day:0.0\tnumeric:w-secret:0.0\tnumeric:w-cholesterol:0.0\tnumeric:w-insulin:0.0\tnumeric:w-24:0.0\tnumeric:w-h:0.0\tnumeric:w-low:0.0\tnumeric:w-rate:0.0\tnumeric:w-fatti:0.0\tnumeric:w-acid:0.0\tnumeric:w-effect:0.0\tnumeric:w-hormon:0.0\tnumeric:w-hepat:0.0\tnumeric:w-contrast:0.0\tnumeric:w-product:0.0\tnumeric:w-major:0.0\tnumeric:w-plasma:0.0\tnumeric:w-current:0.0\tnumeric:w-flow:0.0\tnumeric:w-chronic:0.0\tnumeric:w-mechan:0.0\tnumeric:w-test:0.0\tnumeric:w-therefor:0.0\tnumeric:w-analys:0.0\tnumeric:w-mrna:0.0\tnumeric:w-streptozotocin:0.0\tnumeric:w-did:0.0\tnumeric:w-15:0.0\tnumeric:w-g:0.0\tnumeric:w-25:0.0\tnumeric:w-mmol:0.0\tnumeric:w-l:0.0\tnumeric:w-5:0.0\tnumeric:w-reduc:0.0\tnumeric:w-number:0.0\tnumeric:w-densiti:0.0\tnumeric:w-posit:0.0\tnumeric:w-cell:0.0\tnumeric:w-17:0.0\tnumeric:w-mm:0.0\tnumeric:w-18:0.0\tnumeric:w-induct:0.0\tnumeric:w-associ:0.0\tnumeric:w-express:0.0\tnumeric:w-glycem:0.0\tnumeric:w-respons:0.0\tnumeric:w-therapi:0.0\tnumeric:w-random:0.0\tnumeric:w-initi:0.0\tnumeric:w-ani:0.0\tnumeric:w-singl:0.0\tnumeric:w-new:0.0\tnumeric:w-agent:0.0\tnumeric:w-metformin:0.0\tnumeric:w-medic:0.0\tnumeric:w-glycosyl:0.0\tnumeric:w-hemoglobin:0.0\tnumeric:w-analysi:0.0\tnumeric:w-baselin:0.0\tnumeric:w-health:0.0\tnumeric:w-factor:0.0\tnumeric:w-process:0.0\tnumeric:w-care:0.0\tnumeric:w-9:0.0\tnumeric:w-01:0.0\tnumeric:w-95:0.0\tnumeric:w-interv:0.0\tnumeric:w-ci:0.0\tnumeric:w-12:0.0\tnumeric:w-reduct:0.0\tnumeric:w-achiev:0.0\tnumeric:w-target:0.0\tnumeric:w-lt:0.0\tnumeric:w-diseas:0.0\tnumeric:w-class:0.0\tnumeric:w-age:0.0\tnumeric:w-obes:0.0\tnumeric:w-renal:0.0\tnumeric:w-improv:0.0\tnumeric:w-progress:0.0\tnumeric:w-noninsulindepend:0.0\tnumeric:w-mellitus:0.0\tnumeric:w-becaus:0.0\tnumeric:w-s:0.0\tnumeric:w-index:0.0\tnumeric:w-hypertens:0.0\tnumeric:w-need:0.0\tnumeric:w-followup:0.0\tnumeric:w-year:0.0\tnumeric:w-mg:0.0\tnumeric:w-dl:0.0\tnumeric:w-remain:0.0\tnumeric:w-subject:0.0\tnumeric:w-treat:0.0\tnumeric:w-oral:0.0\tnumeric:w-requir:0.0\tnumeric:w-0001:0.0\tnumeric:w-mortal:0.0\tnumeric:w-includ:0.0\tnumeric:w-vs:0.0\tnumeric:w-background:0.0\tnumeric:w-poor:0.0\tnumeric:w-drug:0.0\tnumeric:w-13:0.0\tnumeric:w-rang:0.0\tnumeric:w-combin:0.0\tnumeric:w-intervent:0.0\tnumeric:w-daili:0.0\tnumeric:w-dose:0.0\tnumeric:w-100:0.0\tnumeric:w-toler:0.0\tnumeric:w-receiv:0.0\tnumeric:w-11:0.0\tnumeric:w-postprandi:0.0\tnumeric:w-kg:0.0\tnumeric:w-hypoglycemia:0.0\tnumeric:w-frequent:0.0\tnumeric:w-event:0.0\tnumeric:w-versus:0.0\tnumeric:w-symptom:0.0\tnumeric:w-incid:0.0\tnumeric:w-parent:0.0\tnumeric:w-complex:0.0\tnumeric:w-longterm:0.0\tnumeric:w-inhibitor:0.0\tnumeric:w-peripher:0.0\tnumeric:w-nerv:0.0\tnumeric:w-stz:0.0\tnumeric:w-conduct:0.0\tnumeric:w-demonstr:0.0\tnumeric:w-frequenc:0.0\tnumeric:w-inhibit:0.0\tnumeric:w-neuropathi:0.0\tnumeric:w-pathway:0.0\tnumeric:w-shown:0.0\tnumeric:w-time:0.0\tnumeric:w-ii:0.0\tnumeric:w-individu:0.0\tnumeric:w-adult:0.0\tnumeric:w-50:0.0\tnumeric:w-60:0.0\tnumeric:w-diagnosi:0.0\tnumeric:w-healthi:0.0\tnumeric:w-follow:0.0\tnumeric:w-young:0.0\tnumeric:w-seen:0.0\tnumeric:w-alter:0.0\tnumeric:w-gene:0.0\tnumeric:w-e:0.0\tnumeric:w-identifi:0.0\tnumeric:w-previous:0.0\tnumeric:w-mediat:0.0\tnumeric:w-vascular:0.0\tnumeric:w-lipoprotein:0.0\tnumeric:w-involv:0.0\tnumeric:w-phenotyp:0.0\tnumeric:w-confirm:0.0\tnumeric:w-variant:0.0\tnumeric:w-endotheli:0.0\tnumeric:w-potenti:0.0\tnumeric:w-disord:0.0\tnumeric:w-popul:0.0\tnumeric:w-nonobes:0.0\tnumeric:w-aim:0.0\tnumeric:w-serum:0.0\tnumeric:w-hba1c:0.0\tnumeric:w-hypoglycaemia:0.0\tnumeric:w-continu:0.0\tnumeric:w-case:0.0\tnumeric:w-impair:0.0\tnumeric:w-risk:0.0\tnumeric:w-known:0.0\tnumeric:w-men:0.0\tnumeric:w-women:0.0\tnumeric:w-40:0.0\tnumeric:w-complet:0.0\tnumeric:w-estim:0.0\tnumeric:w-like:0.0\tnumeric:w-particular:0.0\tnumeric:w-human:0.0\tnumeric:w-character:0.0\tnumeric:w-elev:0.0\tnumeric:w-synthesi:0.0\tnumeric:w-greater:0.0\tnumeric:w-small:0.0\tnumeric:w-reveal:0.0\tnumeric:w-liver:0.0\tnumeric:w-niddm:0.0\tnumeric:w-genet:0.0\tnumeric:w-receptor:0.0\tnumeric:w-growth:0.0\tnumeric:w-pancreat:0.0\tnumeric:w-betacel:0.0\tnumeric:w-molecul:0.0\tnumeric:w-enzym:0.0\tnumeric:w-regul:0.0\tnumeric:w-polymorph:0.0\tnumeric:w-total:0.0\tnumeric:w-allel:0.0\tnumeric:w-02:0.0\tnumeric:w-resist:0.0\tnumeric:w-cpeptid:0.0\tnumeric:w-hypothesi:0.0\tnumeric:w-perform:0.0\tnumeric:w-score:0.0\tnumeric:w-001:0.0\tnumeric:w-05:0.0\tnumeric:w-histori:0.0\tnumeric:w-action:0.0\tnumeric:w-approxim:0.0\tnumeric:w-suppress:0.0\tnumeric:w-glucagon:0.0\tnumeric:w-ml:0.0\tnumeric:w-x:0.0\tnumeric:w-free:0.0\tnumeric:w-peopl:0.0\tnumeric:w-uptak:0.0\tnumeric:w-intens:0.0\tnumeric:w-relationship:0.0\tnumeric:w-prevent:0.0\tnumeric:w-autoimmun:0.0\tnumeric:w-recent:0.0\tnumeric:w-preval:0.0\tnumeric:w-nondiabet:0.0\tnumeric:w-genotyp:0.0\tnumeric:w-conclud:0.0\tnumeric:w-linkag:0.0\tnumeric:w-islet:0.0\tnumeric:w-peptid:0.0\tnumeric:w-form:0.0\tnumeric:w-membran:0.0\tnumeric:w-transgen:0.0\tnumeric:w-failur:0.0\tnumeric:w-isol:0.0\tnumeric:w-negat:0.0\tnumeric:w-earli:0.0\tnumeric:w-famili:0.0\tnumeric:w-chromosom:0.0\tnumeric:w-immun:0.0\tnumeric:w-support:0.0\tnumeric:w-16:0.0\tnumeric:w-cohort:0.0\tnumeric:w-insulindepend:0.0\tnumeric:w-outcom:0.0\tnumeric:w-screen:0.0\tnumeric:w-approach:0.0\tnumeric:w-infus:0.0\tnumeric:w-multipl:0.0\tnumeric:w-depend:0.0\tnumeric:w-physic:0.0\tnumeric:w-transport:0.0\tnumeric:w-acut:0.0\tnumeric:w-releas:0.0\tnumeric:w-presenc:0.0\tnumeric:w-glycaem:0.0\tnumeric:w-male:0.0\tnumeric:w-antibodi:0.0\tnumeric:w-femal:0.0\tnumeric:w-pattern:0.0\tnumeric:w-t2dm:0.0\tnumeric:w-promot:0.0\tnumeric:w-fat:0.0\tnumeric:w-d:0.0\tnumeric:w-bmi:0.0\tnumeric:w-haplotyp:0.0\tnumeric:w-triglycerid:0.0\tnumeric:w-interact:0.0\tnumeric:w-marker:0.0\tnumeric:w-describ:0.0\tnumeric:w-area:0.0\tnumeric:w-20:0.0\tnumeric:w-cytokin:0.0\tnumeric:w-bind:0.0\tnumeric:w-bb:0.0\tnumeric:w-alpha:0.0\tnumeric:w-beta:0.0\tnumeric:w-cd4:0.0\tnumeric:w-spontan:0.0\tnumeric:w-given:0.0\tnumeric:w-vitro:0.0\tnumeric:w-basal:0.0\tnumeric:w-protect:0.0\tnumeric:w-pressur:0.0\tnumeric:w-detect:0.0\tnumeric:w-exercis:0.0\tnumeric:w-children:0.0\tnumeric:w-adolesc:0.0\tnumeric:w-life:0.0\tnumeric:w-b:0.0\tnumeric:w-antigen:0.0\tnumeric:w-iddm:0.0\tnumeric:w-american:0.0\tnumeric:w-hla:0.0\tnumeric:w-arteri:0.0\tnumeric:w-nephropathi:0.0\tnumeric:w-review:0.0\tnumeric:w-destruct:0.0\tnumeric:w-content:0.0\tnumeric:w-autoantibodi:0.0\tnumeric:w-dm:0.0\tnumeric:w-select:0.0\tnumeric:w-infect:0.0\tnumeric:w-recipi:0.0\tnumeric:w-intak:0.0\tnumeric:w-placebo:0.0\tnumeric:w-db:0.0\tnumeric:w-pancrea:0.0\tnumeric:w-diagnos:0.0\tnumeric:w-glomerular:0.0\tnumeric:w-albumin:0.0\tnumeric:w-excret:0.0\tnumeric:w-syndrom:0.0\tnumeric:w-t:0.0\tnumeric:w-lymphocyt:0.0\tnumeric:w-produc:0.0\tnumeric:w-coronari:0.0\tnumeric:w-status:0.0\tnumeric:w-microalbuminuria:0.0\tnumeric:w-nod:0.0\tnumeric:w-mhc:0.0\tnumeric:w-insul:0.0\tnumeric:w-administr:0.0\tnumeric:w-revers:0.0\tnumeric:w-transplant:0.0\tnumeric:w-graft:0.0\tnumeric:w-t1d:0.0\tnumeric:w-lead:0.0\tnumeric:w-v:0.0\tnumeric:w-dietari:0.0\tnumeric:w-general:0.0\tnumeric:w-macrophag:0.0\tnumeric:w-kidney:0.0\tnumeric:w-urinari:0.0\tnumeric:w-myocardi:0.0\tnumeric:w-meal:0.0\tnumeric:w-ica:0.0\tnumeric:w-locus:0.0\tnumeric:w-tcell:0.0\tnumeric:w-depress:0.0\tnumeric:w-bone:0.0\tnumeric:w-mutat:0.0\tstring:summary\n",
            "\n",
            "12187484\tlabel=1\tw-rat=0.09393489570187145\tw-common=0.028698458467273157\tw-use=0.01176012652514843\tw-examin=0.019375414753592942\tw-pathogenesi=0.06316131961800078\tw-retinopathi=0.17089058531360632\tw-mous=0.06770248034355311\tw-studi=0.017554610474374233\tw-anim=0.09840151241009497\tw-model=0.06269133038832954\tw-metabol=0.06232233318170418\tw-abnorm=0.11247870345628387\tw-contribut=0.02534773765067718\tw-develop=0.030388826051908086\tw-investig=0.02014612607562432\tw-mice=0.12119873074191996\tw-2=0.020571546813213402\tw-month=0.10361986739277738\tw-compar=0.02367140886552208\tw-obtain=0.03061978039959059\tw-method=0.014469342700659771\tw-induc=0.023516442702830022\tw-6=0.014872498687869398\tw-inject=0.028054999329982466\tw-experiment=0.06866787644053303\tw-normal=0.01777754779525323\tw-diet=0.031956203604979944\tw-30=0.02512131278693402\tw-hyperglycemia=0.02896081409449482\tw-level=0.03654889376239291\tw-lipid=0.030348254033687905\tw-oxid=0.09357481262838539\tw-activ=0.03623879368519283\tw-protein=0.022816081905882666\tw-kinas=0.04216587194300068\tw-c=0.031475602330090724\tw-measur=0.015735336508945104\tw-result=0.0075446006836769695\tw-increas=0.008769967077523864\tw-retin=0.04575957596508121\tw-stress=0.03732992842799811\tw-3=0.01261883005795486\tw-similar=0.01996113997855104\tw-observ=0.01828742887023866\tw-conclus=0.012866895687595546\tw-play=0.03099778146368732\tw-import=0.023158771568589955\tw-role=0.021716016285633605\tw-present=0.020784310286111652\tsummary=w-rat,w-common,w-use,w-examin,w-pathogenesi,w-retinopathi,w-mous,w-studi,w-anim,w-model,w-metabol,w-abnorm,w-contribut,w-develop,w-investig,w-mice,w-2,w-month,w-compar,w-obtain,w-method,w-induc,w-6,w-inject,w-experiment,w-normal,w-diet,w-30,w-hyperglycemia,w-level,w-lipid,w-oxid,w-activ,w-protein,w-kinas,w-c,w-measur,w-result,w-increas,w-retin,w-stress,w-3,w-similar,w-observ,w-conclus,w-play,w-import,w-role,w-present\n",
            "\n",
            "2344352\tlabel=1\tw-rat=0.023617916633613394\tw-use=0.014784159060186597\tw-anim=0.030926189614601272\tw-metabol=0.05223205066657113\tw-investig=0.02532655849507057\tw-2=0.008620457712203712\tw-compar=0.014879171286899591\tw-obtain=0.038493438216628165\tw-method=0.01819003082368657\tw-result=0.009484640859479618\tw-observ=0.045979821159457195\tw-present=0.026128847216826075\tw-p=0.014294054581628576\tw-m=0.09146697293950078\tw-r=0.084025394089456\tw-muscl=0.4062632862830699\tw-control=0.02040786400257234\tw-chang=0.09037543606387229\tw-dure=0.045181041684535406\tw-lower=0.024551616775906053\tw-higher=0.023107092005331186\tw-mass=0.03621600607107118\tw-correl=0.02913497797028972\tw-decreas=0.04210805971073984\tw-determin=0.021704050013012812\tw-concentr=0.026942629508125954\tw-stimul=0.07571505907890856\tw-period=0.03296082694484282\tw-caus=0.0628011984005355\tw-mark=0.037304711111544514\tw-group=0.01744219207350692\tw-evid=0.030374639739571847\tw-fast=0.030225769225453863\tw-type=0.018081686873962367\tw-signific=0.011233087785814973\tw-differ=0.017535608880382365\tw-ratio=0.0306594914964437\tw-suggest=0.01808250868959585\tw-degre=0.04213016817587703\tw-occur=0.03322538477008907\tw-vivo=0.04160267530597802\tw-respect=0.025632999122820185\tw-dysfunct=0.041228682369095576\tw-region=0.038677508663826174\tw-high=0.02549735472250127\tw-appear=0.035051895200369464\tw-sever=0.027258424614748894\tw-affect=0.033338260136862426\tsummary=w-rat,w-use,w-anim,w-metabol,w-investig,w-2,w-compar,w-obtain,w-method,w-result,w-observ,w-present,w-p,w-m,w-r,w-muscl,w-control,w-chang,w-dure,w-lower,w-higher,w-mass,w-correl,w-decreas,w-determin,w-concentr,w-stimul,w-period,w-caus,w-mark,w-group,w-evid,w-fast,w-type,w-signific,w-differ,w-ratio,w-suggest,w-degre,w-occur,w-vivo,w-respect,w-dysfunct,w-region,w-high,w-appear,w-sever,w-affect\n",
            "\n",
            "14654069\tlabel=1\tw-rat=0.10226314418677966\tw-use=0.010668980765083111\tw-anim=0.04463573758808431\tw-contribut=0.02299588570370713\tw-develop=0.013784622126638718\tw-investig=0.01827689788304062\tw-2=0.0062209488644769054\tw-compar=0.01073754628951517\tw-experiment=0.09344494525928207\tw-hyperglycemia=0.05254745650135142\tw-level=0.011052586429864524\tw-result=0.013689172374506666\tw-increas=0.023868776375941238\tw-3=0.034344032322681274\tw-similar=0.03621815088891735\tw-observ=0.04977197135817532\tw-present=0.03771173825108918\tw-control=0.007363662268969401\tw-decreas=0.015193629792534994\tw-caus=0.04532045245399469\tw-mark=0.053841851088827136\tw-group=0.02517429783805123\tw-differ=0.02530912621910857\tw-dysfunct=0.029752657379759694\tw-cardiovascular=0.026407658278637884\tw-complic=0.04514270517473888\tw-primari=0.028787044226206756\tw-death=0.034229983748296305\tw-patient=0.008268602615393734\tw-clinic=0.017856752723253204\tw-suscept=0.026261063287964296\tw-cardiac=0.17954331408026017\tw-tissu=0.02483315898206693\tw-specif=0.02376627128731735\tw-function=0.053444483962051766\tw-defect=0.05973748805474371\tw-possibl=0.024449186584364038\tw-indic=0.037260040732570957\tw-state=0.13922486537513504\tw-onli=0.03672368965084312\tw-bodi=0.021953125717341735\tw-weight=0.045617127453007056\tw-loss=0.028762815593784553\tw-valu=0.022193426462176275\tw-howev=0.01694340388868025\tw-4=0.02532942673604324\tw-condit=0.02839048463004391\tw-durat=0.022522796701481145\tw-8=0.014983797153490505\tw-week=0.021496672043327617\tw-onset=0.024043216052982794\tw-data=0.016859162690567718\tw-direct=0.02847642833494872\tw-report=0.020523519988428204\tw-provid=0.022858487690171445\tw-addit=0.01981563946961861\tw-evalu=0.020505398559345817\tw-sensit=0.024966255333354042\tw-heart=0.027705705910112545\tsummary=w-rat,w-use,w-anim,w-contribut,w-develop,w-investig,w-2,w-compar,w-experiment,w-hyperglycemia,w-level,w-result,w-increas,w-3,w-similar,w-observ,w-present,w-control,w-decreas,w-caus,w-mark,w-group,w-differ,w-dysfunct,w-cardiovascular,w-complic,w-primari,w-death,w-patient,w-clinic,w-suscept,w-cardiac,w-tissu,w-specif,w-function,w-defect,w-possibl,w-indic,w-state,w-onli,w-bodi,w-weight,w-loss,w-valu,w-howev,w-4,w-condit,w-durat,w-8,w-week,w-onset,w-data,w-direct,w-report,w-provid,w-addit,w-evalu,w-sensit,w-heart\n",
            "\n",
            "16443886\tlabel=2\tw-model=0.038714646134547365\tw-develop=0.014074824697725849\tw-month=0.023996179817274765\tw-method=0.013403180606926945\tw-level=0.02257054491993387\tw-result=0.00698868273856393\tw-conclus=0.01191880863693061\tw-import=0.021452335768799118\tw-p=0.010532461270673686\tw-control=0.00751868673778981\tw-group=0.012852141527847206\tw-signific=0.00827701205270577\tw-ratio=0.022591204260537463\tw-suggest=0.013323953771281153\tw-respect=0.018887473037867506\tw-high=0.018787524532369357\tw-complic=0.0460930779152597\tw-patient=0.008442678459928339\tw-onli=0.01874840997964096\tw-durat=0.022996960842564956\tw-data=0.03442818486284355\tw-object=0.019495049534990004\tw-mean=0.035221719689700955\tw-blood=0.04611271525200553\tw-glucos=0.1031637637780166\tw-strong=0.029124107531762505\tw-hba=0.035963683031197805\tw-1c=0.035134177121961895\tw-a1c=0.3326886630569466\tw-variabl=0.02992650085163497\tw-independ=0.024915235862226003\tw-assess=0.01822410894904615\tw-relat=0.03155931828314955\tw-trial=0.02727626187870321\tw-research=0.022695555209681164\tw-design=0.01932872324828978\tw-profil=0.0960730763207182\tw-sampl=0.0809892958275116\tw-particip=0.027022764203363886\tw-n=0.023242616352755794\tw-1=0.0198538353328394\tw-consist=0.027640340412365714\tw-befor=0.04908835852907286\tw-min=0.027745709818629928\tw-predict=0.059035321001874905\tw-adjust=0.027859278723796437\tw-sex=0.028956449356014068\tw-treatment=0.014318432621414691\tw-7=0.030490561788546026\tw-gt=0.024060652153225174\tw-0=0.008321530265771041\tw-larg=0.05788124061346253\tw-influenc=0.056952980118129236\tw-base=0.02918059230549554\tw-standard=0.03160865556226256\tw-14=0.02475992235627369\tw-10=0.016962721093724507\tw-wherea=0.0236700204726859\tw-enhanc=0.028544818349547016\tw-manag=0.0317833275725911\tsummary=w-model,w-develop,w-month,w-method,w-level,w-result,w-conclus,w-import,w-p,w-control,w-group,w-signific,w-ratio,w-suggest,w-respect,w-high,w-complic,w-patient,w-onli,w-durat,w-data,w-object,w-mean,w-blood,w-glucos,w-strong,w-hba,w-1c,w-a1c,w-variabl,w-independ,w-assess,w-relat,w-trial,w-research,w-design,w-profil,w-sampl,w-particip,w-n,w-1,w-consist,w-befor,w-min,w-predict,w-adjust,w-sex,w-treatment,w-7,w-gt,w-0,w-larg,w-influenc,w-base,w-standard,w-14,w-10,w-wherea,w-enhanc,w-manag\n",
            "\n",
            "2684155\tlabel=1\tw-rat=0.030615817858387732\tw-anim=0.08017901011192922\tw-compar=0.019287814631166137\tw-normal=0.057941637258603115\tw-level=0.019853720068460347\tw-result=0.01229490481784395\tw-increas=0.014291798200409258\tw-3=0.020564019353704215\tw-similar=0.03252926515023132\tw-observ=0.05960347187337044\tw-present=0.03387072787366343\tw-chang=0.02928833576144009\tw-decreas=0.054584521847255346\tw-stimul=0.04907457532892222\tw-period=0.04272699789146292\tw-suggest=0.023440289042068695\tw-occur=0.04306994322048584\tw-addit=0.03559475978801862\tw-relat=0.05552102290554087\tw-day=0.03838664446416178\tw-secret=0.3230115081079322\tw-cholesterol=0.15499961736650564\tw-insulin=0.03447368620488266\tw-24=0.045396282850545346\tw-h=0.10394672366460497\tw-low=0.0870248353486404\tw-rate=0.09883909959886525\tw-fatti=0.059816454571435404\tw-acid=0.044538557180858765\tw-effect=0.019997285311920878\tw-hormon=0.06226677056017838\tw-hepat=0.060621234401420414\tw-contrast=0.04665696857917435\tw-product=0.04544799614446522\tw-major=0.040120550358321135\tw-plasma=0.03420631878839778\tsummary=w-rat,w-anim,w-compar,w-normal,w-level,w-result,w-increas,w-3,w-similar,w-observ,w-present,w-chang,w-decreas,w-stimul,w-period,w-suggest,w-occur,w-addit,w-relat,w-day,w-secret,w-cholesterol,w-insulin,w-24,w-h,w-low,w-rate,w-fatti,w-acid,w-effect,w-hormon,w-hepat,w-contrast,w-product,w-major,w-plasma\n",
            "\n",
            "15032912\tlabel=1\tw-rat=0.11689675909566226\tw-studi=0.005201366066481255\tw-metabol=0.018465876498282723\tw-2=0.012190546259682019\tw-compar=0.021041252324908517\tw-6=0.03965999650098506\tw-inject=0.07481333154661991\tw-30=0.02233005581060802\tw-3=0.02243347565858642\tw-import=0.020585574727635517\tw-control=0.021644704245152486\tw-chang=0.015975455869876417\tw-decreas=0.014886687776524188\tw-caus=0.0222024438789772\tw-group=0.024665726164555247\tw-differ=0.012398915369967331\tw-respect=0.018124342814115284\tw-tissu=0.024331479002631235\tw-possibl=0.023955263623063754\tw-weight=0.044695569322643285\tw-4=0.012408860572708054\tw-week=0.021062395840432114\tw-blood=0.04424957524182349\tw-glucos=0.04399801373248744\tw-research=0.02177856307999708\tw-1=0.031752766946460315\tw-befor=0.02355249525384809\tw-7=0.014629309949049863\tw-0=0.007985306820689384\tw-influenc=0.02732592480415292\tw-wherea=0.022713656009143036\tw-current=0.029669249780616965\tw-flow=0.03476520279792114\tw-chronic=0.08607713308363434\tw-mechan=0.02247276561187138\tw-test=0.017796806964759312\tw-therefor=0.02745748830067284\tw-analys=0.02743544170748781\tw-mrna=0.09795520115400466\tw-streptozotocin=0.07747193632438448\tw-did=0.018553857019339585\tw-15=0.02268609525507027\tw-g=0.054856207046584386\tw-25=0.02480112107169836\tw-mmol=0.05427772743118993\tw-l=0.04938846215692122\tw-5=0.06006856163818164\tw-reduc=0.015350735067963788\tw-number=0.02354749846418349\tw-densiti=0.03595493043081391\tw-posit=0.02374932356206716\tw-cell=0.01526896296100756\tw-17=0.05341259161260166\tw-mm=0.06748397687566152\tw-18=0.025193218636266477\tw-induct=0.03219470530858916\tw-associ=0.010602114208960439\tw-express=0.02002396100147944\tsummary=w-rat,w-studi,w-metabol,w-2,w-compar,w-6,w-inject,w-30,w-3,w-import,w-control,w-chang,w-decreas,w-caus,w-group,w-differ,w-respect,w-tissu,w-possibl,w-weight,w-4,w-week,w-blood,w-glucos,w-research,w-1,w-befor,w-7,w-0,w-influenc,w-wherea,w-current,w-flow,w-chronic,w-mechan,w-test,w-therefor,w-analys,w-mrna,w-streptozotocin,w-did,w-15,w-g,w-25,w-mmol,w-l,w-5,w-reduc,w-number,w-densiti,w-posit,w-cell,w-17,w-mm,w-18,w-induct,w-associ,w-express\n",
            "\n",
            "17988185\tlabel=3\tw-use=0.007445259958367352\tw-studi=0.01111371022838081\tw-model=0.013229825117920863\tw-2=0.017364950787172945\tw-month=0.032800533563181335\tw-method=0.009160447177396116\tw-30=0.015904140469425856\tw-level=0.007712955997819129\tw-result=0.004776437842903406\tw-3=0.007988899605036171\tw-conclus=0.008145948348981353\tw-control=0.015416012376043856\tw-chang=0.011378202382142195\tw-dure=0.011376521287472945\tw-type=0.009105885476096158\tw-signific=0.0056569506835039435\tw-differ=0.026492646509930193\tw-suggest=0.009106299340084242\tw-sever=0.01372726419447786\tw-patient=0.023080703703401213\tw-clinic=0.024922374304396557\tw-function=0.012431930322107964\tw-durat=0.015717347338443675\tw-8=0.010456318876896253\tw-provid=0.015951606517601656\tw-evalu=0.014309522735658593\tw-object=0.013323954718158637\tw-mean=0.024072398349076193\tw-glucos=0.00783417870416593\tw-a1c=0.15158476254393252\tw-trial=0.018642049485444644\tw-design=0.013210278479046975\tw-1=0.01356916803323556\tw-befor=0.016774798777920583\tw-adjust=0.03808102847137643\tw-treatment=0.009785979129743855\tw-7=0.04167774632966723\tw-14=0.016922249092417272\tw-insulin=0.013392655072400459\tw-effect=0.023306188636914985\tw-did=0.013214617589313806\tw-15=0.01615772251979825\tw-17=0.019021030826070373\tw-glycem=0.11221374899513156\tw-respons=0.022874579105046162\tw-therapi=0.10526272335805657\tw-random=0.01670747957335482\tw-initi=0.07190780138238492\tw-ani=0.019109124717869658\tw-singl=0.019910467869858704\tw-new=0.019872039917194433\tw-agent=0.020567038312494002\tw-metformin=0.05136294903536071\tw-medic=0.04199130528174022\tw-glycosyl=0.022626649888918358\tw-hemoglobin=0.020682537749255686\tw-analysi=0.013414867941784625\tw-baselin=0.05206974285515926\tw-health=0.019561294526969437\tw-factor=0.010998349713138027\tw-process=0.0201340832173629\tw-care=0.0206336802127233\tw-9=0.023378279406269593\tw-01=0.015293683180046152\tw-95=0.06529185111519156\tw-interv=0.019893973606758163\tw-ci=0.07805702429137525\tw-12=0.013781066307007914\tw-reduct=0.01619375113625263\tw-achiev=0.042029760064207514\tw-target=0.060097785975653135\tw-lt=0.010108410193782849\tw-diseas=0.00954612667815425\tw-class=0.045157269460028175\tw-age=0.009135783990388429\tw-obes=0.017700452657409573\tw-renal=0.0196616195637683\tw-improv=0.013887509480775129\tw-progress=0.016987950413016355\tsummary=w-use,w-studi,w-model,w-2,w-month,w-method,w-30,w-level,w-result,w-3,w-conclus,w-control,w-chang,w-dure,w-type,w-signific,w-differ,w-suggest,w-sever,w-patient,w-clinic,w-function,w-durat,w-8,w-provid,w-evalu,w-object,w-mean,w-glucos,w-a1c,w-trial,w-design,w-1,w-befor,w-adjust,w-treatment,w-7,w-14,w-insulin,w-effect,w-did,w-15,w-17,w-glycem,w-respons,w-therapi,w-random,w-initi,w-ani,w-singl,w-new,w-agent,w-metformin,w-medic,w-glycosyl,w-hemoglobin,w-analysi,w-baselin,w-health,w-factor,w-process,w-care,w-9,w-01,w-95,w-interv,w-ci,w-12,w-reduct,w-achiev,w-target,w-lt,w-diseas,w-class,w-age,w-obes,w-renal,w-improv,w-progress\n",
            "\n",
            "9834350\tlabel=3\tw-2=0.006157469794431223\tw-compar=0.0318839384719277\tw-6=0.026709793561887893\tw-level=0.010939804935682232\tw-increas=0.00787507247777653\tw-p=0.020420077973755106\tw-control=0.036442614290307755\tw-mass=0.025868575765050843\tw-decreas=0.015038592753835657\tw-determin=0.015502892866437723\tw-group=0.12458708623933518\tw-differ=0.012525434914558832\tw-cardiovascular=0.026138192377835455\tw-death=0.0677613963996886\tw-patient=0.02455268735795486\tw-bodi=0.021729114230430085\tw-weight=0.022575823280314714\tw-4=0.02507096319792035\tw-8=0.029661802528338346\tw-mean=0.03414350378083256\tw-glucos=0.011111743264072082\tw-1=0.012830709908977842\tw-sex=0.028070027436952413\tw-0=0.024200368630048437\tw-10=0.016443454121467634\tw-wherea=0.02294542800923633\tw-manag=0.061620737130533766\tw-insulin=0.009497852321753385\tw-rate=0.036308240668970905\tw-did=0.018743182090965496\tw-5=0.03640890368681622\tw-number=0.023787779060756787\tw-initi=0.025497919367733428\tw-medic=0.05955909626695806\tw-9=0.03315898813746401\tw-age=0.012957897700652974\tw-obes=0.02510574407530541\tw-improv=0.01969758997783411\tw-noninsulindepend=0.025527799569560864\tw-mellitus=0.011826894656611717\tw-becaus=0.028062352309375326\tw-s=0.032800164637243674\tw-index=0.024884216207891253\tw-hypertens=0.029405207965552488\tw-need=0.029035672417548865\tw-followup=0.08034199588673512\tw-year=0.05945809060348699\tw-mg=0.038703987556153126\tw-dl=0.06457673162957968\tw-remain=0.02267489281222248\tw-subject=0.014016721272157246\tw-treat=0.022358565453632365\tw-oral=0.025497919367733428\tw-requir=0.026794207542599414\tw-0001=0.031178491590509838\tw-mortal=0.06596982934861537\tw-includ=0.0184071027413318\tw-vs=0.02166333426607988\tsummary=w-2,w-compar,w-6,w-level,w-increas,w-p,w-control,w-mass,w-decreas,w-determin,w-group,w-differ,w-cardiovascular,w-death,w-patient,w-bodi,w-weight,w-4,w-8,w-mean,w-glucos,w-1,w-sex,w-0,w-10,w-wherea,w-manag,w-insulin,w-rate,w-did,w-5,w-number,w-initi,w-medic,w-9,w-age,w-obes,w-improv,w-noninsulindepend,w-mellitus,w-becaus,w-s,w-index,w-hypertens,w-need,w-followup,w-year,w-mg,w-dl,w-remain,w-subject,w-treat,w-oral,w-requir,w-0001,w-mortal,w-includ,w-vs\n",
            "\n",
            "16230722\tlabel=3\tw-common=0.010479105166473187\tw-use=0.0042941540838716255\tw-studi=0.004273321498602856\tw-2=0.020030939082299082\tw-compar=0.004321750996194902\tw-6=0.03801435349264543\tw-inject=0.010244149132939655\tw-level=0.031139859692439886\tw-measur=0.00574568304061066\tw-result=0.0027548749384380634\tw-increas=0.0032023116299672197\tw-3=0.027646233487967492\tw-similar=0.014577430025829804\tw-conclus=0.004698285562275552\tw-control=0.02074658373705487\tw-higher=0.006711603487025656\tw-decreas=0.006115278381227778\tw-concentr=0.007825660023107124\tw-group=0.02026478747129435\tw-fast=0.03511707627853561\tw-type=0.010503884491098472\tw-differ=0.015279991140582145\tw-occur=0.009650526696706371\tw-complic=0.009084735273754505\tw-patient=0.023296187451669483\tw-clinic=0.007187157735085314\tw-onli=0.007390452066663449\tw-bodi=0.017671810743420318\tw-weight=0.027540630889678532\tw-4=0.025487078769670064\tw-8=0.036184937524196985\tw-week=0.008652187502916095\tw-object=0.007684770563585272\tw-mean=0.00694204018780413\tw-blood=0.01817721140639222\tw-glucos=0.027110809291594956\tw-a1c=0.058285703993379304\tw-assess=0.0071837773865534625\tw-relat=0.006220197586927816\tw-trial=0.021504106875326185\tw-design=0.0076192062596992915\tw-particip=0.010652126968130993\tw-1=0.015652401299748903\tw-7=0.018028651679949412\tw-0=0.026242170132721957\tw-standard=0.01245984347890018\tw-10=0.013373099617459156\tw-insulin=0.050208563725783466\tw-rate=0.007382173414023131\tw-effect=0.008961439060943797\tw-plasma=0.015328972735049627\tw-did=0.007621708900060659\tw-mmol=0.02229665981613196\tw-l=0.020288206446204153\tw-5=0.019740373783170066\tw-reduc=0.018917710851391056\tw-17=0.010970636036613203\tw-associ=0.004355225338950553\tw-glycem=0.04314719532592887\tw-therapi=0.017346198632803633\tw-random=0.009636264152266887\tw-metformin=0.02962427351002132\tw-hemoglobin=0.047715730243096104\tw-baselin=0.010010642125680686\tw-9=0.020225606872229088\tw-95=0.00941448890561372\tw-ci=0.03376533104720277\tw-reduct=0.009339964348295085\tw-achiev=0.024241230908401842\tw-target=0.011554069502926397\tw-lt=0.011660323791998472\tw-improv=0.008009808372729224\tw-mellitus=0.009618553330688367\tw-year=0.006044494687906354\tw-mg=0.01573855095644401\tw-dl=0.026259417841073898\tw-oral=0.020736897079152498\tw-includ=0.007485045928010442\tw-vs=0.02642747001754143\tw-background=0.009406783466828607\tw-poor=0.012317700566429612\tw-drug=0.012602869175199658\tw-13=0.01001286878395433\tw-rang=0.01087632794085571\tw-combin=0.020083849207155283\tw-intervent=0.011315642094950689\tw-daili=0.023600356879514436\tw-dose=0.01058256110710771\tw-100=0.02329757178843517\tw-toler=0.009599812577346835\tw-receiv=0.04132883681442526\tw-11=0.009084735273754505\tw-postprandi=0.014375664337452333\tw-kg=0.036779257473107715\tw-hypoglycemia=0.0276767147073734\tw-frequent=0.012388163388620874\tw-event=0.04818895265846483\tw-versus=0.012785615796903948\tw-symptom=0.014122235651107519\tw-incid=0.009749670350262618\tsummary=w-common,w-use,w-studi,w-2,w-compar,w-6,w-inject,w-level,w-measur,w-result,w-increas,w-3,w-similar,w-conclus,w-control,w-higher,w-decreas,w-concentr,w-group,w-fast,w-type,w-differ,w-occur,w-complic,w-patient,w-clinic,w-onli,w-bodi,w-weight,w-4,w-8,w-week,w-object,w-mean,w-blood,w-glucos,w-a1c,w-assess,w-relat,w-trial,w-design,w-particip,w-1,w-7,w-0,w-standard,w-10,w-insulin,w-rate,w-effect,w-plasma,w-did,w-mmol,w-l,w-5,w-reduc,w-17,w-associ,w-glycem,w-therapi,w-random,w-metformin,w-hemoglobin,w-baselin,w-9,w-95,w-ci,w-reduct,w-achiev,w-target,w-lt,w-improv,w-mellitus,w-year,w-mg,w-dl,w-oral,w-includ,w-vs,w-background,w-poor,w-drug,w-13,w-rang,w-combin,w-intervent,w-daili,w-dose,w-100,w-toler,w-receiv,w-11,w-postprandi,w-kg,w-hypoglycemia,w-frequent,w-event,w-versus,w-symptom,w-incid\n",
            "\n",
            "3542527\tlabel=2\tw-use=0.027970030654407077\tw-studi=0.013917168664368762\tw-obtain=0.07282542365308033\tw-6=0.07074485862337876\tw-measur=0.0748491682587659\tw-result=0.017943915139556038\tw-group=0.032998741760688774\tw-differ=0.06635095252036571\tw-high=0.0482382386641916\tw-appear=0.06631439632502331\tw-patient=0.06503144219133991\tw-independ=0.06397155153814785\tw-sampl=0.06931516309561804\tw-particip=0.06938277295458295\tw-consist=0.14193688319863476\tw-influenc=0.07311531231381457\tw-10=0.043552932537941305\tw-manag=0.08160584106476095\tw-analysi=0.050396395781298994\tw-factor=0.041318124598005025\tw-age=0.10296275470248581\tw-year=0.03937089783203869\tw-13=0.19565686839997246\tw-daili=0.07686062172922944\tw-parent=0.19305042704339057\tw-complex=0.08171245669424787\tsummary=w-use,w-studi,w-obtain,w-6,w-measur,w-result,w-group,w-differ,w-high,w-appear,w-patient,w-independ,w-sampl,w-particip,w-consist,w-influenc,w-10,w-manag,w-analysi,w-factor,w-age,w-year,w-13,w-daili,w-parent,w-complex\n",
            "\n",
            "10960717\tlabel=1\tw-rat=0.1033283852720586\tw-examin=0.03552159371492039\tw-metabol=0.038085870277708114\tw-abnorm=0.051552739084130106\tw-induc=0.0431134782885217\tw-normal=0.0651843419159285\tw-level=0.06700630523105368\tw-result=0.013831767920074444\tw-increas=0.03215654595092083\tw-chang=0.0329493777316201\tw-decreas=0.030703793539081134\tw-function=0.07200159644887529\tw-treatment=0.05667712912643315\tw-effect=0.04499389195182198\tw-streptozotocin=0.05326195622301433\tw-reduc=0.03166089107767531\tw-associ=0.021866860555980902\tw-analysi=0.03884722174808464\tw-reduct=0.046894404332064904\tw-progress=0.04919427307102653\tw-longterm=0.12082853464776007\tw-inhibitor=0.05950639193800009\tw-peripher=0.1169400641226806\tw-nerv=0.2864383906171961\tw-stz=0.06996361156560135\tw-conduct=0.061042269956144826\tw-demonstr=0.0852085440320818\tw-frequenc=0.05252643653937364\tw-inhibit=0.16962067935883\tw-neuropathi=0.06752259128191752\tw-pathway=0.06652319903637524\tsummary=w-rat,w-examin,w-metabol,w-abnorm,w-induc,w-normal,w-level,w-result,w-increas,w-chang,w-decreas,w-function,w-treatment,w-effect,w-streptozotocin,w-reduc,w-associ,w-analysi,w-reduct,w-progress,w-longterm,w-inhibitor,w-peripher,w-nerv,w-stz,w-conduct,w-demonstr,w-frequenc,w-inhibit,w-neuropathi,w-pathway\n",
            "\n",
            "15723700\tlabel=3\tw-metabol=0.01741068355552371\tw-2=0.005746971808135808\tw-compar=0.00991944752459973\tw-obtain=0.025662292144418782\tw-method=0.012126687215791047\tw-6=0.024929140657762038\tw-measur=0.013187710597973041\tw-result=0.006323093906319747\tw-increas=0.02940027058369905\tw-conclus=0.010783684004841983\tw-control=0.006802621334190781\tw-caus=0.020933732800178503\tw-group=0.06976876829402769\tw-type=0.012054457915974913\tw-signific=0.022466175571629946\tw-differ=0.03507121776076473\tw-affect=0.06667652027372485\tw-clinic=0.01649623823005296\tw-indic=0.01721059024313992\tw-bodi=0.04056101323013617\tw-howev=0.01565247787811414\tw-4=0.02339956565139233\tw-8=0.013842174513224564\tw-report=0.018959823227405106\tw-addit=0.036611752924819156\tw-sensit=0.02306406445081278\tw-mean=0.04780090529316558\tw-1=0.00598766462418966\tw-7=0.013793349380532727\tw-0=0.007529003573792848\tw-24=0.023346659751709038\tw-effect=0.020568636320832907\tw-did=0.017493636618234467\tw-5=0.011327214480342825\tw-mm=0.06362774962562372\tw-respons=0.015140792836197221\tw-age=0.04837615141577111\tw-year=0.0416206634224409\tw-subject=0.02616454637469353\tw-background=0.0215908077667209\tw-100=0.026736737147680364\tw-peripher=0.02672915751375557\tw-nerv=0.0327358160705367\tw-shown=0.05148785059455453\tw-time=0.20778679873143938\tw-ii=0.04880315641848506\tw-individu=0.02032294618489566\tw-adult=0.2391422667940373\tw-50=0.02327258530421289\tw-60=0.026782342068935577\tw-diagnosi=0.02728288731406582\tw-healthi=0.04970225432146473\tw-follow=0.020063571304661503\tw-young=0.029148640728921284\tw-seen=0.027355434669279095\tsummary=w-metabol,w-2,w-compar,w-obtain,w-method,w-6,w-measur,w-result,w-increas,w-conclus,w-control,w-caus,w-group,w-type,w-signific,w-differ,w-affect,w-clinic,w-indic,w-bodi,w-howev,w-4,w-8,w-report,w-addit,w-sensit,w-mean,w-1,w-7,w-0,w-24,w-effect,w-did,w-5,w-mm,w-respons,w-age,w-year,w-subject,w-background,w-100,w-peripher,w-nerv,w-shown,w-time,w-ii,w-individu,w-adult,w-50,w-60,w-diagnosi,w-healthi,w-follow,w-young,w-seen\n",
            "\n",
            "16118269\tlabel=1\tw-rat=0.09725024496193752\tw-use=0.02435037962854263\tw-examin=0.020059252921366812\tw-studi=0.006058061653901697\tw-anim=0.025468626741436344\tw-investig=0.02085716581946988\tw-compar=0.012253435177446722\tw-level=0.025225903145808443\tw-oxid=0.03229248435803104\tw-protein=0.04724271076982763\tw-increas=0.02723848598195647\tw-muscl=0.06691395303485857\tw-control=0.033612952474825035\tw-chang=0.03721341484982976\tw-decreas=0.017338612822069346\tw-type=0.007445400477513917\tw-dysfunct=0.06790606507851035\tw-tissu=0.028339016720711672\tw-indic=0.021260140888584605\tw-addit=0.022613141512388298\tw-relat=0.017636089628818866\tw-1=0.022189580666114622\tw-secret=0.029315330147610657\tw-mrna=0.03802966633037828\tw-g=0.09583702054609154\tw-5=0.013992441416894076\tw-associ=0.01234834478455392\tw-express=0.11661012583214497\tw-factor=0.017985536589719832\tw-includ=0.021222306690006076\tw-rang=0.03083758863230854\tw-versus=0.036250981259457075\tw-complex=0.03556895173749613\tw-demonstr=0.0481177660416462\tw-pathway=0.03756604180877661\tw-shown=0.031801319484871915\tw-alter=0.027484237838999438\tw-gene=0.21030539563529696\tw-e=0.10379607505168095\tw-identifi=0.024513312856952924\tw-previous=0.02644401897142371\tw-mediat=0.03464158947392918\tw-vascular=0.03203142248012508\tw-lipoprotein=0.039156695850950836\tw-involv=0.02744337799084405\tw-phenotyp=0.03923581320280582\tw-confirm=0.033953032539255176\tw-variant=0.03997288337882544\tw-endotheli=0.03923581320280582\tw-potenti=0.028065551128468653\tw-disord=0.03972202726742754\tsummary=w-rat,w-use,w-examin,w-studi,w-anim,w-investig,w-compar,w-level,w-oxid,w-protein,w-increas,w-muscl,w-control,w-chang,w-decreas,w-type,w-dysfunct,w-tissu,w-indic,w-addit,w-relat,w-1,w-secret,w-mrna,w-g,w-5,w-associ,w-express,w-factor,w-includ,w-rang,w-versus,w-complex,w-demonstr,w-pathway,w-shown,w-alter,w-gene,w-e,w-identifi,w-previous,w-mediat,w-vascular,w-lipoprotein,w-involv,w-phenotyp,w-confirm,w-variant,w-endotheli,w-potenti,w-disord\n",
            "\n",
            "8293860\tlabel=3\tw-studi=0.004951300390208117\tw-2=0.023208924609779226\tw-normal=0.04512762132641204\tw-30=0.021256495435098017\tw-c=0.026633201971615228\tw-measur=0.013314515507568936\tw-result=0.006383892886188206\tw-3=0.010677471587500268\tw-chang=0.015207405106901587\tw-determin=0.04382548560319895\tw-concentr=0.14507569735144746\tw-group=0.03521981091765821\tw-fast=0.06103280324370492\tw-type=0.01825554924775047\tw-signific=0.00756073216352931\tw-differ=0.023605627338976264\tw-degre=0.02835684396453262\tw-4=0.011812280737481704\tw-condit=0.026479586626098648\tw-addit=0.01848189450531736\tw-evalu=0.019125227502466775\tw-blood=0.04212219181673583\tw-glucos=0.05235340576341655\tw-relat=0.014414111715861572\tw-sampl=0.049320404510343606\tw-1=0.018135714967497527\tw-0=0.030405591355701884\tw-14=0.02261723676775001\tw-10=0.03098958661353516\tw-insulin=0.008949899303190692\tw-acid=0.0231257893054459\tw-effect=0.020766411670071684\tw-plasma=0.15984875895347422\tw-mmol=0.07750233195703561\tw-l=0.0705210252913731\tw-17=0.02542233927715175\tw-associ=0.010092397179683493\tw-process=0.05381995321564313\tw-obes=0.09462934305307424\tw-subject=0.026416128551373274\tw-13=0.02320289785512494\tw-toler=0.06673715859059386\tw-demonstr=0.01966351016124965\tw-individu=0.06155507738694358\tw-follow=0.020256490259514016\tw-popul=0.019455047948173147\tw-nonobes=0.05715989748828955\tsummary=w-studi,w-2,w-normal,w-30,w-c,w-measur,w-result,w-3,w-chang,w-determin,w-concentr,w-group,w-fast,w-type,w-signific,w-differ,w-degre,w-4,w-condit,w-addit,w-evalu,w-blood,w-glucos,w-relat,w-sampl,w-1,w-0,w-14,w-10,w-insulin,w-acid,w-effect,w-plasma,w-mmol,w-l,w-17,w-associ,w-process,w-obes,w-subject,w-13,w-toler,w-demonstr,w-individu,w-follow,w-popul,w-nonobes\n",
            "\n",
            "17039422\tlabel=3\tw-use=0.008999053340983146\tw-2=0.0682140566791772\tw-method=0.011072192675287477\tw-6=0.011380694648108756\tw-level=0.009322616379972686\tw-result=0.00577325965359629\tw-increas=0.006710931328887826\tw-3=0.01931229643652222\tw-conclus=0.009845972352247026\tw-control=0.006211089044261147\tw-lower=0.014944462385334121\tw-caus=0.019113408208858634\tw-group=0.021233972959051908\tw-fast=0.01839829431114583\tw-type=0.01650936627622651\tw-signific=0.020512595087140387\tw-primari=0.024281245999496135\tw-onli=0.015487816939703402\tw-bodi=0.03703396860142867\tw-weight=0.03847705532992769\tw-4=0.03204723121821123\tw-8=0.012638507164248513\tw-week=0.05439592664876815\tw-report=0.01731114294676118\tw-addit=0.01671406111785222\tw-object=0.01610460613760044\tw-glucos=0.00946913773807882\tw-1=0.016400994405389068\tw-treatment=0.059141352131930246\tw-0=0.027497230443417356\tw-effect=0.01878005924945613\tw-5=0.0206844786162782\tw-reduc=0.026429961247450694\tw-mm=0.05809490183209122\tw-associ=0.009127037449452898\tw-therapi=0.018175799436981197\tw-metformin=0.34145195250028926\tw-9=0.05651444934732997\tw-reduct=0.019573316590774915\tw-improv=0.016785772328936896\tw-mg=0.04947379278916964\tw-subject=0.035834052643602\tw-vs=0.03692185666218832\tw-drug=0.02641123018454885\tw-combin=0.02104438112575836\tw-daili=0.02472906959983904\tw-dose=0.0221773671896779\tw-toler=0.02011786809687467\tw-kg=0.019269132719606435\tw-frequent=0.025961281536153312\tw-event=0.02524682084932614\tw-follow=0.01831891293034311\tw-seen=0.024976701219776565\tw-confirm=0.025095719702927743\tw-aim=0.017418897140632168\tw-serum=0.02030148063611999\tw-hba1c=0.02430173046638708\tw-hypoglycaemia=0.03369234051556932\tw-continu=0.02540059412576019\tw-case=0.022039253744695265\tsummary=w-use,w-2,w-method,w-6,w-level,w-result,w-increas,w-3,w-conclus,w-control,w-lower,w-caus,w-group,w-fast,w-type,w-signific,w-primari,w-onli,w-bodi,w-weight,w-4,w-8,w-week,w-report,w-addit,w-object,w-glucos,w-1,w-treatment,w-0,w-effect,w-5,w-reduc,w-mm,w-associ,w-therapi,w-metformin,w-9,w-reduct,w-improv,w-mg,w-subject,w-vs,w-drug,w-combin,w-daili,w-dose,w-toler,w-kg,w-frequent,w-event,w-follow,w-seen,w-confirm,w-aim,w-serum,w-hba1c,w-hypoglycaemia,w-continu,w-case\n",
            "\n",
            "10492318\tlabel=2\tw-method=0.011789834793130184\tw-30=0.020469217826390683\tw-level=0.06948802023961122\tw-result=0.006147452408921975\tw-3=0.010282009676852107\tw-conclus=0.010484137226929703\tw-import=0.018870110166999222\tw-dure=0.05856801699847182\tw-decreas=0.027292260923627673\tw-type=0.01757941779413008\tw-appear=0.022718820963202428\tw-patient=0.051985010887521714\tw-durat=0.020228808148552508\tw-8=0.01345766966563499\tw-report=0.05529948441326488\tw-mean=0.015491034122785142\tw-glucos=0.0705801470291986\tw-a1c=0.03251586727408429\tw-1=0.02328536242740423\tw-befor=0.04317957463205483\tw-7=0.013410200786629039\tw-0=0.007319864585631934\tw-influenc=0.02504876440380684\tw-insulin=0.008618421551220666\tw-test=0.016313739717696035\tw-25=0.02273436098239016\tw-reduc=0.014071507145633471\tw-hemoglobin=0.026619192103208702\tw-factor=0.014155283427094312\tw-9=0.030088711458069198\tw-age=0.023516184715999842\tw-need=0.02634718423073879\tw-year=0.04046453388292865\tw-mg=0.1053608550139724\tw-dl=0.17579221388052246\tw-treat=0.020288327911629368\tw-hypoglycemia=0.15440019084437476\tw-symptom=0.031513507332564\tw-adult=0.025833269561084275\tw-50=0.022626124601318083\tw-60=0.02603838812257625\tw-serum=0.06485195203204996\tw-impair=0.038881265614045674\tw-risk=0.013816713487670051\tw-known=0.02465774532083698\tw-men=0.05386401514109405\tw-women=0.05077544193060931\tw-40=0.07208613678849708\tw-complet=0.024857709230919216\tw-estim=0.026439546684968597\tw-like=0.052894570611555634\tw-particular=0.02689943318599444\tsummary=w-method,w-30,w-level,w-result,w-3,w-conclus,w-import,w-dure,w-decreas,w-type,w-appear,w-patient,w-durat,w-8,w-report,w-mean,w-glucos,w-a1c,w-1,w-befor,w-7,w-0,w-influenc,w-insulin,w-test,w-25,w-reduc,w-hemoglobin,w-factor,w-9,w-age,w-need,w-year,w-mg,w-dl,w-treat,w-hypoglycemia,w-symptom,w-adult,w-50,w-60,w-serum,w-impair,w-risk,w-known,w-men,w-women,w-40,w-complet,w-estim,w-like,w-particular\n",
            "\n",
            "7152132\tlabel=1\tw-studi=0.016610814212311104\tw-anim=0.17458332846952332\tw-level=0.01729194973704611\tw-increas=0.03734308562042419\tw-3=0.017910597501613352\tw-observ=0.05191270130906458\tw-present=0.02950031137383589\tw-control=0.03456170516564671\tw-decreas=0.023770678868966038\tw-concentr=0.030419097831755112\tw-stimul=0.04274237206067419\tw-caus=0.03545228941965714\tw-mark=0.0842364444454231\tw-signific=0.012682518467855616\tw-occur=0.037512531192036055\tw-tissu=0.038851877762266\tw-specif=0.03718271475596424\tw-onli=0.028727402388159535\tw-onset=0.03761599930869889\tw-larg=0.04434449885708823\tw-enhanc=0.0874760562324828\tw-cholesterol=0.04499988891285648\tw-insulin=0.015012734315029546\tw-effect=0.017416990432963346\tw-major=0.034943705150795826\tw-plasma=0.02979260023505613\tw-cell=0.048762172036766076\tw-therapi=0.03371317637504577\tw-mellitus=0.018694123812063684\tw-remain=0.03584095960641618\tw-demonstr=0.0329839525285478\tw-alter=0.03768000348895084\tw-human=0.034248523309145\tw-character=0.045473741327902814\tw-elev=0.03892964983383176\tw-synthesi=0.3348326512332951\tw-greater=0.06991448379417214\tw-small=0.1998170885335827\tw-reveal=0.043890466487840854\tw-liver=0.050944962659628036\tsummary=w-studi,w-anim,w-level,w-increas,w-3,w-observ,w-present,w-control,w-decreas,w-concentr,w-stimul,w-caus,w-mark,w-signific,w-occur,w-tissu,w-specif,w-onli,w-onset,w-larg,w-enhanc,w-cholesterol,w-insulin,w-effect,w-major,w-plasma,w-cell,w-therapi,w-mellitus,w-remain,w-demonstr,w-alter,w-human,w-character,w-elev,w-synthesi,w-greater,w-small,w-reveal,w-liver\n",
            "\n",
            "8104271\tlabel=3\tw-examin=0.01739833161547121\tw-metabol=0.018654303809489688\tw-2=0.006157469794431223\tw-compar=0.010627979490642564\tw-6=0.013354896780943946\tw-level=0.010939804935682232\tw-protein=0.02048791028283341\tw-kinas=0.03786323194881693\tw-result=0.00677474347105687\tw-3=0.011331194337755384\tw-role=0.019500096256487316\tw-p=0.010210038986877553\tw-control=0.0291540914322462\tw-lower=0.01753686912564718\tw-higher=0.016505065718093702\tw-determin=0.015502892866437723\tw-fast=0.021589835161038472\tw-signific=0.00802363413272498\tw-differ=0.012525434914558832\tw-suggest=0.012916077635425606\tw-degre=0.030092977268483593\tw-region=0.02762679190273298\tw-patient=0.057289603835228006\tw-tissu=0.024579759400617265\tw-function=0.017633044028295988\tw-howev=0.016770512012265145\tw-direct=0.028185852535612507\tw-sensit=0.024711497625870833\tw-glucos=0.011111743264072082\tw-variabl=0.029010383478625733\tw-relat=0.015296608351526565\tw-1=0.006415354954488921\tw-0=0.008066789543349478\tw-10=0.016443454121467634\tw-insulin=0.05698711393052032\tw-effect=0.011018912314731913\tw-product=0.025042773385725733\tw-plasma=0.018848379740545713\tw-did=0.018743182090965496\tw-analysi=0.038054421304246175\tw-factor=0.015599700103328425\tw-noninsulindepend=0.025527799569560864\tw-mellitus=0.011826894656611717\tw-includ=0.0184071027413318\tw-frequenc=0.025727234223366677\tw-time=0.020238973902412926\tw-gene=0.020267526790249703\tw-involv=0.0476058597800356\tw-phenotyp=0.034031062471821374\tw-variant=0.10401107409796415\tw-known=0.027173841782146876\tw-niddm=0.2215453771217371\tw-genet=0.02361266181511862\tw-receptor=0.02571503567842001\tw-growth=0.06360203110159159\tw-pancreat=0.024032933954501578\tw-betacel=0.02597431848990336\tw-molecul=0.034685029611936626\tw-enzym=0.030243872168018077\tw-regul=0.028557526196055536\tw-polymorph=0.202954912565367\tw-total=0.019573291808654802\tw-allel=0.02804701935136987\tw-02=0.02876910165642831\tw-resist=0.022271201207358784\tw-cpeptid=0.03492268999975393\tsummary=w-examin,w-metabol,w-2,w-compar,w-6,w-level,w-protein,w-kinas,w-result,w-3,w-role,w-p,w-control,w-lower,w-higher,w-determin,w-fast,w-signific,w-differ,w-suggest,w-degre,w-region,w-patient,w-tissu,w-function,w-howev,w-direct,w-sensit,w-glucos,w-variabl,w-relat,w-1,w-0,w-10,w-insulin,w-effect,w-product,w-plasma,w-did,w-analysi,w-factor,w-noninsulindepend,w-mellitus,w-includ,w-frequenc,w-time,w-gene,w-involv,w-phenotyp,w-variant,w-known,w-niddm,w-genet,w-receptor,w-growth,w-pancreat,w-betacel,w-molecul,w-enzym,w-regul,w-polymorph,w-total,w-allel,w-02,w-resist,w-cpeptid\n",
            "\n",
            "17764005\tlabel=3\tw-use=0.02631079154778971\tw-retinopathi=0.025488765267114164\tw-studi=0.008727715942061766\tw-metabol=0.03098511480220321\tw-abnorm=0.02097060572913767\tw-2=0.025569154231112706\tw-method=0.01079069625133949\tw-level=0.009085600709295413\tw-result=0.005626481865792995\tw-conclus=0.009595651021257695\tw-play=0.023116989566139695\tw-import=0.017270948288439968\tw-role=0.01619499519606574\tw-present=0.015500163603201908\tw-p=0.07631571513920342\tw-control=0.012106360001525965\tw-determin=0.0257505678120491\tw-group=0.02069412618890652\tw-type=0.02681606104189334\tw-sever=0.04851075567031583\tw-complic=0.01855441695741386\tw-patient=0.02039121492440319\tw-valu=0.01824374887144999\tw-condit=0.023337940755205587\tw-data=0.013858803228687022\tw-addit=0.016289127360618688\tw-glucos=0.009228396948127663\tw-0=0.060295833705374925\tw-insulin=0.0078880468434901\tw-rate=0.015077150786267582\tw-chronic=0.02407241857423672\tw-test=0.01493121940263705\tw-analys=0.023017870585095704\tw-associ=0.05336996474680085\tw-express=0.016799763891071733\tw-factor=0.025911366273325184\tw-01=0.054046321068468184\tw-lt=0.0952589164024282\tw-diseas=0.022490027258702385\tw-hypertens=0.048842548824138035\tw-includ=0.015287254819072175\tw-event=0.024604952522648355\tw-identifi=0.017657894854584733\tw-vascular=0.11536741147502678\tw-aim=0.03395208764699491\tw-impair=0.035586243104380785\tw-risk=0.01264580556498615\tw-02=0.023892982731609953\tw-hypothesi=0.02213546137555889\tw-perform=0.07738747214781877\tw-score=0.027477234220226875\tw-001=0.017299129059444387\tw-05=0.0668577748314402\tw-histori=0.025149413729479916\tsummary=w-use,w-retinopathi,w-studi,w-metabol,w-abnorm,w-2,w-method,w-level,w-result,w-conclus,w-play,w-import,w-role,w-present,w-p,w-control,w-determin,w-group,w-type,w-sever,w-complic,w-patient,w-valu,w-condit,w-data,w-addit,w-glucos,w-0,w-insulin,w-rate,w-chronic,w-test,w-analys,w-associ,w-express,w-factor,w-01,w-lt,w-diseas,w-hypertens,w-includ,w-event,w-identifi,w-vascular,w-aim,w-impair,w-risk,w-02,w-hypothesi,w-perform,w-score,w-001,w-05,w-histori\n",
            "\n",
            "17914032\tlabel=3\tw-2=0.024756186250431175\tw-month=0.011690446577646679\tw-compar=0.010682481949568938\tw-method=0.006529754654656717\tw-6=0.020135075146653955\tw-result=0.0034047428726337093\tw-3=0.017083954540000428\tw-conclus=0.005806599079530297\tw-p=0.010262398161169233\tw-determin=0.007791197440568702\tw-concentr=0.009671713156763163\tw-stimul=0.027179764797556922\tw-type=0.006490861954755722\tw-differ=0.006294833957060337\tw-4=0.012599766119980484\tw-8=0.02981391433617598\tw-object=0.00949758823499513\tw-glucos=0.027921816407155492\tw-relat=0.007687526248459505\tw-research=0.011056808948306207\tw-design=0.009416557479936048\tw-n=0.022646651830890263\tw-1=0.0515860336853263\tw-befor=0.023914841334676522\tw-min=0.08110284408522596\tw-treatment=0.0139512933234297\tw-7=0.014854376255958321\tw-0=0.08108157694853835\tw-14=0.012062526276133337\tw-enhanc=0.027812899930327862\tw-insulin=0.014319838885105107\tw-24=0.025142556655686654\tw-fatti=0.016564556650551343\tw-acid=0.012333754296237813\tw-effect=0.011075419557371565\tw-hepat=0.033574837514632846\tw-contrast=0.012920391298848283\tw-product=0.025171197864626894\tw-mechan=0.011409250233719314\tw-did=0.01883930097348327\tw-15=0.011517556052574136\tw-mmol=0.055112769391669765\tw-l=0.06268535581455385\tw-5=0.024397077342276852\tw-therapi=0.010719061206424808\tw-random=0.011909434157417024\tw-metformin=0.10983769101407906\tw-9=0.016664517115238326\tw-01=0.021803302174629897\tw-12=0.00982342675217487\tw-lt=0.014410964276264778\tw-improv=0.009899301629885861\tw-mg=0.019451234771810287\tw-subject=0.007044300947032872\tw-vs=0.08709771315182886\tw-drug=0.01557585369858009\tw-11=0.011227801030640184\tw-kg=0.06818308500783817\tw-alter=0.011980308801615139\tw-greater=0.011114610244201725\tw-cpeptid=0.017550890358850695\tw-05=0.010114381320653775\tw-action=0.02847978229129173\tw-approxim=0.05847120064738262\tw-suppress=0.08294770932516699\tw-glucagon=0.01876364403314759\tw-ml=0.014925233264136284\tw-x=0.193958355988209\tw-free=0.01618629639451026\tw-peopl=0.01717948827380587\tw-uptak=0.018008788028723605\tsummary=w-2,w-month,w-compar,w-method,w-6,w-result,w-3,w-conclus,w-p,w-determin,w-concentr,w-stimul,w-type,w-differ,w-4,w-8,w-object,w-glucos,w-relat,w-research,w-design,w-n,w-1,w-befor,w-min,w-treatment,w-7,w-0,w-14,w-enhanc,w-insulin,w-24,w-fatti,w-acid,w-effect,w-hepat,w-contrast,w-product,w-mechan,w-did,w-15,w-mmol,w-l,w-5,w-therapi,w-random,w-metformin,w-9,w-01,w-12,w-lt,w-improv,w-mg,w-subject,w-vs,w-drug,w-11,w-kg,w-alter,w-greater,w-cpeptid,w-05,w-action,w-approxim,w-suppress,w-glucagon,w-ml,w-x,w-free,w-peopl,w-uptak\n",
            "\n",
            "12112937\tlabel=2\tw-common=0.027752355440879537\tw-studi=0.0056586290173807055\tw-obtain=0.029610337089713976\tw-hyperglycemia=0.028006061981489496\tw-measur=0.015216589151507354\tw-play=0.029975876580269057\tw-role=0.0210001036608325\tw-present=0.04019822648742473\tw-chang=0.03475978310148934\tw-dure=0.017377323724821314\tw-determin=0.016695423086932934\tw-period=0.02535448226526371\tw-evid=0.023365107491978346\tw-type=0.020863484854571963\tw-suggest=0.013909622068919885\tw-complic=0.02405957363708611\tw-clinic=0.038068242069352984\tw-function=0.03797886406094521\tw-possibl=0.026061220864651777\tw-indic=0.019858373357469136\tw-state=0.059361810731376266\tw-condit=0.030262384715541314\tw-onset=0.05125696609097432\tw-data=0.03594151167000151\tw-evalu=0.021857402859962027\tw-sensit=0.0798371461758904\tw-independ=0.026010411064961214\tw-trial=0.056950436889600116\tw-design=0.02017833745700582\tw-sampl=0.028183088291624917\tw-particip=0.028210578014500762\tw-1=0.020726531391425745\tw-treatment=0.01494781427510325\tw-manag=0.033180396916441264\tw-secret=0.08214735371033756\tw-insulin=0.08182765077202918\tw-effect=0.03559956286298003\tw-current=0.03227753547561626\tw-reduc=0.016700250238773792\tw-associ=0.023068336410705128\tw-therapi=0.022969416870910305\tw-process=0.06150851796073501\tw-diseas=0.04374433873395958\tw-improv=0.04242557841379655\tw-year=0.01600794747016958\tw-subject=0.015094930600784729\tw-intervent=0.029967799394319955\tw-diagnosi=0.03148025459315287\tw-aim=0.022012891990908785\tw-known=0.029264137303850486\tw-betacel=0.1118893719565068\tw-resist=0.04796874106200354\tw-peopl=0.03681318915815544\tw-intens=0.03584824251367436\tw-relationship=0.02867437749315273\tw-prevent=0.022666136299364637\tw-autoimmun=0.0258205555430032\tsummary=w-common,w-studi,w-obtain,w-hyperglycemia,w-measur,w-play,w-role,w-present,w-chang,w-dure,w-determin,w-period,w-evid,w-type,w-suggest,w-complic,w-clinic,w-function,w-possibl,w-indic,w-state,w-condit,w-onset,w-data,w-evalu,w-sensit,w-independ,w-trial,w-design,w-sampl,w-particip,w-1,w-treatment,w-manag,w-secret,w-insulin,w-effect,w-current,w-reduc,w-associ,w-therapi,w-process,w-diseas,w-improv,w-year,w-subject,w-intervent,w-diagnosi,w-aim,w-known,w-betacel,w-resist,w-peopl,w-intens,w-relationship,w-prevent,w-autoimmun\n",
            "\n",
            "11756346\tlabel=3\tw-studi=0.02553397887181707\tw-develop=0.022100964401387698\tw-2=0.01994816660675239\tw-increas=0.00637815787456281\tw-3=0.018354661902479797\tw-p=0.02480786332348761\tw-control=0.01180620231553772\tw-type=0.02092096001946059\tw-signific=0.006498480537248332\tw-differ=0.010144567120882362\tw-ratio=0.017736895907033548\tw-high=0.01475053578987677\tw-suscept=0.06315677203964967\tw-possibl=0.01959976114614307\tw-evalu=0.016438212068235904\tw-relat=0.024777977164456257\tw-particip=0.02121621982908735\tw-1=0.03117544391107013\tw-0=0.026133731413165256\tw-larg=0.022721974620987358\tw-10=0.01331783887523825\tw-did=0.0303608569407375\tw-g=0.2917352829295624\tw-25=0.020291826331389567\tw-5=0.009829400995338815\tw-posit=0.0194312647326004\tw-baselin=0.019938551671810294\tw-9=0.02685603997910309\tw-95=0.03750234423558523\tw-ci=0.0448344065144676\tw-becaus=0.02272818616792382\tw-followup=0.04338025122258976\tw-year=0.03611710462277929\tw-subject=0.034057157471191986\tw-vs=0.0175455103973209\tw-incid=0.058256294737519615\tw-conduct=0.024215115354503734\tw-frequenc=0.0208369335032226\tw-individu=0.035271228915934615\tw-gene=0.04924506509366458\tw-identifi=0.017220095808603293\tw-variant=0.02808012468760465\tw-case=0.04189279637421414\tw-risk=0.012332273195606328\tw-40=0.02144711507756938\tw-like=0.047211682859900905\tw-allel=0.09086307095650405\tw-001=0.016870225033177172\tw-05=0.016300036012623854\tw-approxim=0.047115223662147154\tw-recent=0.020009797482090516\tw-preval=0.04080727175270177\tw-nondiabet=0.01693162548506577\tw-genotyp=0.14529069212702242\tw-conclud=0.021957463081257848\tw-linkag=0.028044578556204725\tsummary=w-studi,w-develop,w-2,w-increas,w-3,w-p,w-control,w-type,w-signific,w-differ,w-ratio,w-high,w-suscept,w-possibl,w-evalu,w-relat,w-particip,w-1,w-0,w-larg,w-10,w-did,w-g,w-25,w-5,w-posit,w-baselin,w-9,w-95,w-ci,w-becaus,w-followup,w-year,w-subject,w-vs,w-incid,w-conduct,w-frequenc,w-individu,w-gene,w-identifi,w-variant,w-case,w-risk,w-40,w-like,w-allel,w-001,w-05,w-approxim,w-recent,w-preval,w-nondiabet,w-genotyp,w-conclud,w-linkag\n",
            "\n",
            "11731221\tlabel=3\tw-rat=0.02328526992046391\tw-anim=0.06098121895836871\tw-model=0.07770193062215493\tw-mice=0.030043629028982978\tw-2=0.01699808562969746\tw-induc=0.02914714025139496\tw-experiment=0.042554740329344413\tw-level=0.015100012446434632\tw-result=0.018702108737002066\tw-import=0.028703829549801635\tw-chang=0.022275635649545985\tw-evid=0.029946827912253934\tw-type=0.017827015227850223\tw-suggest=0.017827825468615627\tw-dysfunct=0.04064799670192522\tw-region=0.07626551004134739\tw-appear=0.03455820653557553\tw-affect=0.03286870717718831\tw-primari=0.039328778731578244\tw-onli=0.025085900676984383\tw-howev=0.023148030664816684\tw-provid=0.031229201492206062\tw-addit=0.027072070824690216\tw-wherea=0.03167115415359381\tw-secret=0.07019163556470158\tw-product=0.0345660815746637\tw-cell=0.04258105163773939\tw-new=0.038904416175915855\tw-factor=0.04306396084862495\tw-target=0.03921874296063749\tw-obes=0.03465299886450606\tw-progress=0.03325810010435597\tw-remain=0.03129773937461695\tw-background=0.03193006782402386\tw-intervent=0.038409433026522756\tw-gene=0.027974896133020722\tw-identifi=0.029346923842830965\tw-involv=0.032854748298897805\tw-human=0.08972148359860521\tw-small=0.04362204045451454\tw-pancreat=0.03317221869776275\tw-betacel=0.07170375245100083\tw-hypothesi=0.03678851327205562\tw-islet=0.24025764856396903\tw-peptid=0.04685886707608119\tw-form=0.130197186213543\tw-membran=0.0468966271711975\tw-transgen=0.0543778286923681\tw-failur=0.046634408030126084\tsummary=w-rat,w-anim,w-model,w-mice,w-2,w-induc,w-experiment,w-level,w-result,w-import,w-chang,w-evid,w-type,w-suggest,w-dysfunct,w-region,w-appear,w-affect,w-primari,w-onli,w-howev,w-provid,w-addit,w-wherea,w-secret,w-product,w-cell,w-new,w-factor,w-target,w-obes,w-progress,w-remain,w-background,w-intervent,w-gene,w-identifi,w-involv,w-human,w-small,w-pancreat,w-betacel,w-hypothesi,w-islet,w-peptid,w-form,w-membran,w-transgen,w-failur\n",
            "\n",
            "10218793\tlabel=1\tw-rat=0.0688855901813724\tw-use=0.021560231962772118\tw-anim=0.045100693187960186\tw-abnorm=0.051552739084130106\tw-develop=0.02785642388091574\tw-investig=0.03693456447197792\tw-month=0.047492439221689636\tw-inject=0.051434165438301184\tw-hyperglycemia=0.05309482583990716\tw-activ=0.06643778842285351\tw-result=0.027663535840148887\tw-observ=0.03352695292877087\tw-import=0.04245774787574825\tw-present=0.038104568857871356\tw-control=0.044642202505627\tw-chang=0.0329493777316201\tw-dure=0.0329445095616404\tw-period=0.04806787262789578\tw-signific=0.016381586354313502\tw-suggest=0.02637032517232728\tw-high=0.037183642303647686\tw-function=0.03600079822443764\tw-indic=0.03764816615686857\tw-week=0.04344119142089123\tw-provid=0.04619319387388813\tw-mean=0.03485482677626657\tw-particip=0.05348255415249102\tw-effect=0.02249694597591099\tw-current=0.12238565534504497\tw-mechan=0.04635007907448471\tw-densiti=0.07415704401355366\tw-conduct=0.061042269956144826\tw-neuropathi=0.06752259128191752\tw-potenti=0.09939882691332648\tw-perform=0.04756105059084695\tw-membran=0.13873585538145924\tw-isol=0.05943683149496854\tw-negat=0.12601456368808792\tw-earli=0.04838207626450666\tsummary=w-rat,w-use,w-anim,w-abnorm,w-develop,w-investig,w-month,w-inject,w-hyperglycemia,w-activ,w-result,w-observ,w-import,w-present,w-control,w-chang,w-dure,w-period,w-signific,w-suggest,w-high,w-function,w-indic,w-week,w-provid,w-mean,w-particip,w-effect,w-current,w-mechan,w-densiti,w-conduct,w-neuropathi,w-potenti,w-perform,w-membran,w-isol,w-negat,w-earli\n",
            "\n",
            "7567975\tlabel=2\tw-use=0.025872278355326545\tw-6=0.03271949711331267\tw-type=0.031642952029434145\tw-region=0.1353712803233916\tw-affect=0.05834195523950925\tw-primari=0.0698085822485514\tw-suscept=0.06368307847331342\tw-data=0.040883469524626716\tw-1=0.031435239276995715\tw-insulin=0.023269738188295797\tw-major=0.10832548596746708\tw-5=0.029733938010899914\tw-respons=0.039744581195017704\tw-analysi=0.04661666609770157\tw-diseas=0.03317279020658602\tw-class=0.07846075568679896\tw-11=0.0547355300243709\tw-complex=0.07558402244217928\tw-shown=0.06757780390535283\tw-ii=0.06405414279926164\tw-gene=0.14896632190833534\tw-identifi=0.05209078982102497\tw-involv=0.05831717823054361\tw-allel=0.06871519741085619\tw-genotyp=0.0732507239473738\tw-linkag=0.25450455039755787\tw-famili=0.12645372286720688\tw-chromosom=0.26459608412722396\tw-immun=0.0757078463704005\tsummary=w-use,w-6,w-type,w-region,w-affect,w-primari,w-suscept,w-data,w-1,w-insulin,w-major,w-5,w-respons,w-analysi,w-diseas,w-class,w-11,w-complex,w-shown,w-ii,w-gene,w-identifi,w-involv,w-allel,w-genotyp,w-linkag,w-famili,w-chromosom,w-immun\n",
            "\n",
            "16306557\tlabel=3\tw-studi=0.01144300534625876\tw-investig=0.01969843438505489\tw-2=0.04693360309977577\tw-compar=0.011572688778699682\tw-method=0.014147801751756221\tw-6=0.014541998717027856\tw-result=0.007376942890706371\tw-observ=0.017881041562011132\tw-conclus=0.012580964672315644\tw-p=0.011117598007933337\tw-dure=0.01757040509954155\tw-decreas=0.016375356554176605\tw-evid=0.023624719797444772\tw-type=0.049222369823564226\tw-ratio=0.023846271163900656\tw-week=0.1158431771223766\tw-object=0.02057810784248945\tw-blood=0.01622484425533528\tw-relat=0.016656306871662262\tw-research=0.023956419387996787\tw-design=0.020402541206528103\tw-sampl=0.028496233717087417\tw-1=0.00698560872822127\tw-adjust=0.02940701643067402\tw-gt=0.025397355050626576\tw-0=0.10540605003309986\tw-influenc=0.03005851728456821\tw-24=0.02723776971032721\tw-did=0.020409242721273542\tw-g=0.15085456937810704\tw-associ=0.03498697688956944\tw-95=0.025209909180587853\tw-ci=0.03013868437916989\tw-12=0.021284091296378888\tw-age=0.014109710829599907\tw-followup=0.029161168877407564\tw-year=0.03237162710634292\tw-rang=0.029124389263846955\tw-follow=0.02340749985543842\tw-potenti=0.02650635384355373\tw-popul=0.022481388740111193\tw-aim=0.02225747967969666\tw-case=0.028161268673777287\tw-risk=0.033160112370408126\tw-women=0.15232632579182795\tw-02=0.03132635513699972\tw-free=0.035070308854772234\tw-support=0.02943718918814605\tw-16=0.02688403816623427\tw-cohort=0.03204762883351019\tsummary=w-studi,w-investig,w-2,w-compar,w-method,w-6,w-result,w-observ,w-conclus,w-p,w-dure,w-decreas,w-evid,w-type,w-ratio,w-week,w-object,w-blood,w-relat,w-research,w-design,w-sampl,w-1,w-adjust,w-gt,w-0,w-influenc,w-24,w-did,w-g,w-associ,w-95,w-ci,w-12,w-age,w-followup,w-year,w-rang,w-follow,w-potenti,w-popul,w-aim,w-case,w-risk,w-women,w-02,w-free,w-support,w-16,w-cohort\n",
            "\n",
            "9682700\tlabel=2\tw-retinopathi=0.16017200422293043\tw-studi=0.006093908172563837\tw-abnorm=0.014642198083066538\tw-develop=0.015823767411644447\tw-2=0.007141207572239762\tw-month=0.05395590728144621\tw-compar=0.006162970355520541\tw-method=0.007534332293834674\tw-6=0.038721298358949906\tw-level=0.01903137663367205\tw-result=0.007857098936847022\tw-3=0.01314150349230802\tw-similar=0.010393966379363856\tw-observ=0.009522448169118354\tw-conclus=0.006699922014842651\tw-import=0.03617701594146597\tw-present=0.010822599438922042\tw-p=0.011841228647502961\tw-control=0.00845296142118381\tw-dure=0.009357020467211475\tw-higher=0.00957098485427919\tw-group=0.007224576598493992\tw-evid=0.012581211726449878\tw-signific=0.0046527582544795745\tw-ratio=0.01269919766124887\tw-suggest=0.007489796498649169\tw-occur=0.027523987383505744\tw-respect=0.010617218571582326\tw-high=0.010561034500444313\tw-sever=0.01129047173391966\tw-complic=0.012955155035354058\tw-patient=0.04745884341379835\tw-clinic=0.030747426286785104\tw-loss=0.016508834985781668\tw-howev=0.009724912291136003\tw-report=0.011779771827677726\tw-object=0.010958755655763612\tw-a1c=0.04155874160474678\tw-assess=0.010244321598576238\tw-trial=0.045998429795446244\tw-1=0.0037201466599994927\tw-consist=0.015537469462572442\tw-befor=0.013797023846928764\tw-treatment=0.08048823071209442\tw-7=0.00856983245536057\tw-larg=0.016268396030411067\tw-insulin=0.00550763034042504\tw-18=0.014758157662664976\tw-associ=0.012421411913456607\tw-glycem=0.015382358096965766\tw-random=0.027483309594039287\tw-initi=0.04435732718410431\tw-ani=0.015716972401088058\tw-hemoglobin=0.03402216268812473\tw-baselin=0.02855106215726681\tw-factor=0.018091959883150127\tw-interv=0.016362499002008195\tw-12=0.01133472317558639\tw-reduct=0.039957362271108555\tw-lt=0.00831401785169122\tw-progress=0.02794467582732868\tw-followup=0.01552961656193302\tw-treat=0.02593064395805884\tw-vs=0.012562170166129162\tw-poor=0.017565478322541638\tw-13=0.014278706372384577\tw-longterm=0.03431816368693777\tw-frequenc=0.014918751206449317\tw-previous=0.02660049245646172\tw-case=0.014997125329230506\tw-risk=0.052977694319586946\tw-complet=0.015885399981889203\tw-particular=0.03438034064008757\tw-greater=0.025649100563542443\tw-001=0.012078681828487797\tw-intens=0.1544231985204434\tw-earli=0.1511582027672161\tw-insulindepend=0.011711460321320302\tw-outcom=0.03186283119095741\tw-screen=0.018935882318004017\tw-approach=0.01806174927448175\tsummary=w-retinopathi,w-studi,w-abnorm,w-develop,w-2,w-month,w-compar,w-method,w-6,w-level,w-result,w-3,w-similar,w-observ,w-conclus,w-import,w-present,w-p,w-control,w-dure,w-higher,w-group,w-evid,w-signific,w-ratio,w-suggest,w-occur,w-respect,w-high,w-sever,w-complic,w-patient,w-clinic,w-loss,w-howev,w-report,w-object,w-a1c,w-assess,w-trial,w-1,w-consist,w-befor,w-treatment,w-7,w-larg,w-insulin,w-18,w-associ,w-glycem,w-random,w-initi,w-ani,w-hemoglobin,w-baselin,w-factor,w-interv,w-12,w-reduct,w-lt,w-progress,w-followup,w-treat,w-vs,w-poor,w-13,w-longterm,w-frequenc,w-previous,w-case,w-risk,w-complet,w-particular,w-greater,w-001,w-intens,w-earli,w-insulindepend,w-outcom,w-screen,w-approach\n",
            "\n",
            "9356032\tlabel=3\tw-common=0.01683642896746692\tw-examin=0.011366909988774527\tw-2=0.016091521062780265\tw-6=0.008725199230216713\tw-level=0.014294678449291453\tw-activ=0.010630046147656563\tw-increas=0.010290094704294667\tw-3=0.014806093934667038\tw-observ=0.010728624937206681\tw-conclus=0.007548578803389387\tw-p=0.03335279402380001\tw-muscl=0.05687686007962979\tw-control=0.004761834933933546\tw-correl=0.013596323052801871\tw-decreas=0.009825213932505964\tw-determin=0.030385670018217938\tw-group=0.008139689634303232\tw-type=0.02531436162354732\tw-signific=0.010484215266760643\tw-differ=0.008183284144178439\tw-primari=0.018615621932947038\tw-patient=0.016041089073863844\tw-bodi=0.014196354630547657\tw-weight=0.014749537876472284\tw-condit=0.018359180060761728\tw-8=0.02906856647777158\tw-glucos=0.007259672265860428\tw-variabl=0.018953450539368817\tw-relat=0.05996270473798414\tw-1=0.02095682618466381\tw-min=0.052716848655396865\tw-0=0.05797332751820492\tw-10=0.01074305669269219\tw-wherea=0.02998202593206881\tw-insulin=0.01241052703375776\tw-rate=0.01186069195186383\tw-mechan=0.014832025303835109\tw-did=0.012245545632764126\tw-25=0.01636873990732092\tw-5=0.03171620054495991\tw-reduc=0.0101314851448561\tw-number=0.0777067449318055\tw-densiti=0.0949210163373487\tw-posit=0.03134910710192865\tw-analysi=0.012431110959387086\tw-lt=0.05620276067743264\tw-s=0.021429440896332533\tw-mg=0.025286605203353376\tw-remain=0.014814263303985355\tw-subject=0.009157591231142735\tw-vs=0.042460135161516574\tw-kg=0.0443190052550948\tw-versus=0.020542222713692344\tw-estim=0.057109420839532174\tw-character=0.018795813082199832\tw-niddm=0.07237148985976746\tw-genet=0.015426939052544166\tw-resist=0.014550518122141073\tw-perform=0.015219536189071025\tw-001=0.02721729638685917\tw-05=0.03944608715054972\tw-histori=0.01978420546719087\tw-x=0.12607293139233586\tw-negat=0.020162330190094068\tw-famili=0.01686049638229425\tw-infus=0.020142577089223023\tw-multipl=0.01819026888922292\tw-depend=0.017939938637878454\tw-physic=0.022416037401664222\tsummary=w-common,w-examin,w-2,w-6,w-level,w-activ,w-increas,w-3,w-observ,w-conclus,w-p,w-muscl,w-control,w-correl,w-decreas,w-determin,w-group,w-type,w-signific,w-differ,w-primari,w-patient,w-bodi,w-weight,w-condit,w-8,w-glucos,w-variabl,w-relat,w-1,w-min,w-0,w-10,w-wherea,w-insulin,w-rate,w-mechan,w-did,w-25,w-5,w-reduc,w-number,w-densiti,w-posit,w-analysi,w-lt,w-s,w-mg,w-remain,w-subject,w-vs,w-kg,w-versus,w-estim,w-character,w-niddm,w-genet,w-resist,w-perform,w-001,w-05,w-histori,w-x,w-negat,w-famili,w-infus,w-multipl,w-depend,w-physic\n",
            "\n",
            "10613759\tlabel=1\tw-use=0.009763123907670394\tw-mous=0.028102916369022048\tw-studi=0.004857879628128719\tw-2=0.011385510185929431\tw-induc=0.019523084508009828\tw-oxid=0.025894916702194702\tw-activ=0.06017007253390507\tw-result=0.0062634420770148426\tw-increas=0.0072807273851141505\tw-present=0.01725489910545118\tw-muscl=0.02682870758473103\tw-dure=0.014918268480742822\tw-mass=0.02391623042429229\tw-decreas=0.013903604621470701\tw-concentr=0.017792302505366196\tw-stimul=0.10000102142497358\tw-signific=0.029672307358756533\tw-indic=0.01704822618424237\tw-howev=0.015504812992471551\tw-glucos=0.020546242261869133\tw-design=0.01732291234516537\tw-1=0.01779353166622399\tw-treatment=0.012832557538060336\tw-7=0.01366322344298053\tw-0=0.029831900952764112\tw-10=0.030404877432147703\tw-enhanc=0.025582620218933646\tw-secret=0.09403030424705305\tw-insulin=0.05268619967161312\tw-acid=0.022689453658173334\tw-effect=0.030561888872935684\tw-test=0.016621546127463884\tw-did=0.051985806931545815\tw-g=0.07685044100394132\tw-25=0.02316331118960507\tw-mmol=0.15208004761380572\tw-l=0.23063479969505665\tw-5=0.022440707932754652\tw-reduc=0.014337007280456745\tw-cell=0.028521270436599022\tw-lt=0.01325536808430015\tw-includ=0.017017887440099213\tw-demonstr=0.019292500535565693\tw-alter=0.022039247323725964\tw-potenti=0.022505394772828636\tw-known=0.025122985798588623\tw-reveal=0.025671782285340877\tw-pancreat=0.04443825523662556\tw-betacel=0.04802798513227414\tw-action=0.026196026164159843\tw-x=0.02973418193215468\tw-16=0.022826070141142303\tw-depend=0.025386705619639318\tw-transport=0.033344490155425316\tw-acut=0.029959449266690467\tw-releas=0.030145832270804394\tw-presenc=0.022805705400877795\tsummary=w-use,w-mous,w-studi,w-2,w-induc,w-oxid,w-activ,w-result,w-increas,w-present,w-muscl,w-dure,w-mass,w-decreas,w-concentr,w-stimul,w-signific,w-indic,w-howev,w-glucos,w-design,w-1,w-treatment,w-7,w-0,w-10,w-enhanc,w-secret,w-insulin,w-acid,w-effect,w-test,w-did,w-g,w-25,w-mmol,w-l,w-5,w-reduc,w-cell,w-lt,w-includ,w-demonstr,w-alter,w-potenti,w-known,w-reveal,w-pancreat,w-betacel,w-action,w-x,w-16,w-depend,w-transport,w-acut,w-releas,w-presenc\n",
            "\n",
            "16219016\tlabel=3\tw-use=0.016558258147408987\tw-studi=0.008238963849306308\tw-2=0.019309825275336315\tw-compar=0.008332335920663772\tw-method=0.010186417261264478\tw-result=0.010622797762617174\tw-3=0.017767312721600443\tw-observ=0.025748699849296033\tw-conclus=0.009058294564067265\tw-p=0.032018682262848014\tw-control=0.017142605762160767\tw-higher=0.012939971522985465\tw-group=0.07814102048931101\tw-fast=0.03385286153250833\tw-type=0.015188616974128391\tw-signific=0.018871587480169157\tw-differ=0.009819940973014125\tw-respect=0.014354479508779305\tw-patient=0.03208217814772769\tw-possibl=0.018972568789466494\tw-4=0.009827817573584778\tw-week=0.016681417505622232\tw-object=0.014816237646592404\tw-glucos=0.01742321343806503\tw-hba=0.054664798207420665\tw-1c=0.05340394922538208\tw-1=0.005029638284319314\tw-consist=0.021006658713397942\tw-treatment=0.032646026376825496\tw-0=0.044270541013901946\tw-larg=0.02199487143311576\tw-14=0.018817540990768006\tw-manag=0.02415532895516924\tw-day=0.01658303040851789\tw-effect=0.01727765450949964\tw-plasma=0.014777129716587841\tw-15=0.017967387442015655\tw-5=0.028544580490463916\tw-number=0.01864961878363332\tw-17=0.021151386278590256\tw-18=0.019953029159923048\tw-random=0.01857871728557056\tw-agent=0.06861163981047998\tw-9=0.025996646699771787\tw-01=0.01700657569621132\tw-interv=0.02212209865071508\tw-12=0.0153245457333928\tw-achiev=0.023368546595699376\tw-lt=0.04496220854194611\tw-treat=0.05258734594694333\tw-oral=0.03998073756860602\tw-0001=0.04888787481391943\tw-drug=0.02429833176978494\tw-daili=0.045501488063703834\tw-dose=0.08161271125801467\tw-receiv=0.019920499344552974\tw-longterm=0.023199078652369934\tw-demonstr=0.03272008090831942\tw-frequenc=0.020170151631119477\tw-time=0.015867355539491736\tw-previous=0.017981932900568123\tw-aim=0.016025385369381594\tw-05=0.01577843486021989\tw-16=0.019356507479688673\tw-depend=0.021527926365454142\tw-glycaem=0.05388750251095938\tsummary=w-use,w-studi,w-2,w-compar,w-method,w-result,w-3,w-observ,w-conclus,w-p,w-control,w-higher,w-group,w-fast,w-type,w-signific,w-differ,w-respect,w-patient,w-possibl,w-4,w-week,w-object,w-glucos,w-hba,w-1c,w-1,w-consist,w-treatment,w-0,w-larg,w-14,w-manag,w-day,w-effect,w-plasma,w-15,w-5,w-number,w-17,w-18,w-random,w-agent,w-9,w-01,w-interv,w-12,w-achiev,w-lt,w-treat,w-oral,w-0001,w-drug,w-daili,w-dose,w-receiv,w-longterm,w-demonstr,w-frequenc,w-time,w-previous,w-aim,w-05,w-16,w-depend,w-glycaem\n",
            "\n",
            "3089169\tlabel=1\tw-rat=0.04862512248096876\tw-use=0.010145991511892763\tw-examin=0.01671604410113901\tw-anim=0.10611927808931809\tw-2=0.005916000390728038\tw-compar=0.010211195981205602\tw-oxid=0.026910403631692534\tw-activ=0.06252968322150919\tw-3=0.010886833775490467\tw-play=0.026743184007887097\tw-role=0.018735386599370168\tw-p=0.04904822650558825\tw-r=0.057664486139822745\tw-control=0.035013492161276076\tw-dure=0.031006597234485084\tw-correl=0.019994592724708632\tw-decreas=0.043346532055173366\tw-determin=0.029789872566880333\tw-concentr=0.018490039858517812\tw-group=0.07182079089091085\tw-fast=0.14520222471051367\tw-type=0.006204500397928264\tw-signific=0.00770898181379459\tw-degre=0.028912860512856787\tw-specif=0.04520251597783888\tw-function=0.016941552105617715\tw-indic=0.017716784073820505\tw-bodi=0.04175398420749311\tw-valu=0.02110551340030489\tw-data=0.016032733146912437\tw-relat=0.014696741357349054\tw-1=0.006163772407254062\tw-0=0.05425311398762493\tw-major=0.021240291366170012\tw-plasma=0.018109227593857646\tw-streptozotocin=0.02506444998730086\tw-5=0.01166036784741173\tw-cell=0.01481987581509557\tw-induct=0.03124780221127771\tw-mg=0.07437236824515699\tw-dose=0.0250038943805192\tw-receiv=0.04882475329547297\tw-kg=0.08690001030410745\tw-small=0.060728722985696704\tw-enzym=0.029057837965350705\tw-05=0.07734526892264651\tw-isol=0.02797027364469108\tw-negat=0.029650485573667746\tw-male=0.02573044312712432\tw-antibodi=0.02616240381137952\tsummary=w-rat,w-use,w-examin,w-anim,w-2,w-compar,w-oxid,w-activ,w-3,w-play,w-role,w-p,w-r,w-control,w-dure,w-correl,w-decreas,w-determin,w-concentr,w-group,w-fast,w-type,w-signific,w-degre,w-specif,w-function,w-indic,w-bodi,w-valu,w-data,w-relat,w-1,w-0,w-major,w-plasma,w-streptozotocin,w-5,w-cell,w-induct,w-mg,w-dose,w-receiv,w-kg,w-small,w-enzym,w-05,w-isol,w-negat,w-male,w-antibodi\n",
            "\n",
            "1563583\tlabel=2\tw-studi=0.004119481924653154\tw-contribut=0.017844807306076734\tw-2=0.004827456318834079\tw-30=0.0353708084040031\tw-3=0.026650969082400666\tw-observ=0.025748699849296033\tw-present=0.014632154441422601\tw-dure=0.02530138334333983\tw-higher=0.02587994304597093\tw-period=0.036916126178223964\tw-group=0.019535255122327753\tw-type=0.015188616974128391\tw-differ=0.009819940973014125\tw-ratio=0.01716931523800847\tw-degre=0.02359289417849114\tw-patient=0.006416435629545537\tw-specif=0.018442626518958265\tw-onli=0.01424879158452713\tw-4=0.04913908786792389\tw-8=0.023254853182217265\tw-onset=0.01865753565711465\tw-data=0.013082710247880549\tw-report=0.015926251511020287\tw-evalu=0.015912189282052354\tw-blood=0.011681887863841401\tw-glucos=0.008711606719032514\tw-independ=0.018935579255291764\tw-1=0.02514819142159657\tw-consist=0.021006658713397942\tw-sex=0.022006901510570692\tw-7=0.03475924043894247\tw-0=0.018973089005957976\tw-standard=0.024022578227319544\tw-14=0.018817540990768006\tw-10=0.012891668031230626\tw-insulin=0.007446316220254655\tw-rate=0.028465660684473194\tw-15=0.017967387442015655\tw-25=0.0196424878887851\tw-l=0.019557831014140804\tw-5=0.03805944065395189\tw-17=0.021151386278590256\tw-9=0.07798994009931537\tw-95=0.018151134610023253\tw-interv=0.02212209865071508\tw-12=0.0153245457333928\tw-age=0.05079495898655966\tw-mellitus=0.009272285410783588\tw-year=0.10488407182455105\tw-includ=0.014431168549204133\tw-100=0.11229429602025752\tw-11=0.05254610882339606\tw-incid=0.1315815510471443\tw-identifi=0.016669052742727987\tw-popul=0.032373199785760115\tw-case=0.06082834033535894\tw-risk=0.011937640453346925\tw-total=0.015345460777985367\tw-recent=0.01936948396266362\tw-conclud=0.021254824262657596\tw-insulindepend=0.01583389435442505\tw-male=0.04199208318346689\tw-femal=0.04639815730473987\tw-pattern=0.025341429252785524\tsummary=w-studi,w-contribut,w-2,w-30,w-3,w-observ,w-present,w-dure,w-higher,w-period,w-group,w-type,w-differ,w-ratio,w-degre,w-patient,w-specif,w-onli,w-4,w-8,w-onset,w-data,w-report,w-evalu,w-blood,w-glucos,w-independ,w-1,w-consist,w-sex,w-7,w-0,w-standard,w-14,w-10,w-insulin,w-rate,w-15,w-25,w-l,w-5,w-17,w-9,w-95,w-interv,w-12,w-age,w-mellitus,w-year,w-includ,w-100,w-11,w-incid,w-identifi,w-popul,w-case,w-risk,w-total,w-recent,w-conclud,w-insulindepend,w-male,w-femal,w-pattern\n",
            "\n",
            "12915397\tlabel=3\tw-common=0.034833990967172934\tw-examin=0.011758872402180544\tw-studi=0.02485894264876903\tw-metabol=0.02521547273558606\tw-2=0.004161600274856964\tw-level=0.03696899598954686\tw-protein=0.013847001432535685\tw-3=0.007658324448965709\tw-observ=0.01109857752124829\tw-p=0.0483040465172276\tw-chang=0.010907380214605276\tw-mass=0.017483589137758503\tw-decreas=0.010164014412937203\tw-fast=0.014591750660563934\tw-type=0.008729090215016316\tw-signific=0.010845739931131698\tw-region=0.01867190073426091\tw-suscept=0.01756774578574163\tw-function=0.011917505619124186\tw-bodi=0.014685884100566541\tw-valu=0.014846637012628268\tw-howev=0.011334552946220582\tw-4=0.008472256528952395\tw-onset=0.01608408246302987\tw-report=0.013729527164672661\tw-evalu=0.013717404553493408\tw-sensit=0.033403127825315064\tw-relat=0.010338397368617955\tw-0=0.04361629656542063\tw-influenc=0.018657010728352682\tw-secret=0.03436969741444008\tw-insulin=0.032096190604545924\tw-effect=0.022341794624352984\tw-test=0.012150923375939116\tw-mrna=0.08917301070571458\tw-g=0.018726774129696047\tw-associ=0.05067079411592816\tw-express=0.027343063988227098\tw-factor=0.010543245587077144\tw-lt=0.00969013115128149\tw-obes=0.033936040267309384\tw-index=0.05045489355255193\tw-subject=0.009473370239113174\tw-includ=0.012440662542417356\tw-combin=0.03338074247534085\tw-individu=0.0588664648114219\tw-gene=0.02739610517854443\tw-previous=0.015501666293593208\tw-confirm=0.019903501833356485\tw-variant=0.1640266593820768\tw-popul=0.069769827124483\tw-impair=0.014479919607989424\tw-particular=0.02003543989025793\tw-character=0.01944394456779293\tw-betacel=0.03511011326911075\tw-polymorph=0.03919129346089845\tw-allel=0.03791183305426548\tw-resist=0.015052260126352834\tw-hypothesi=0.018013685809075513\tw-05=0.013602099017430938\tw-nondiabet=0.014129149542710056\tw-membran=0.022963176063138083\tw-famili=0.034883785618539824\tw-t2dm=0.11884841777559536\tw-promot=0.02220975829433885\tw-fat=0.022437555023142174\tw-d=0.044238134252306965\tw-bmi=0.062147395892960144\tw-haplotyp=0.02397862085032388\tw-triglycerid=0.020345031893804284\tw-interact=0.04389834560622281\tsummary=w-common,w-examin,w-studi,w-metabol,w-2,w-level,w-protein,w-3,w-observ,w-p,w-chang,w-mass,w-decreas,w-fast,w-type,w-signific,w-region,w-suscept,w-function,w-bodi,w-valu,w-howev,w-4,w-onset,w-report,w-evalu,w-sensit,w-relat,w-0,w-influenc,w-secret,w-insulin,w-effect,w-test,w-mrna,w-g,w-associ,w-express,w-factor,w-lt,w-obes,w-index,w-subject,w-includ,w-combin,w-individu,w-gene,w-previous,w-confirm,w-variant,w-popul,w-impair,w-particular,w-character,w-betacel,w-polymorph,w-allel,w-resist,w-hypothesi,w-05,w-nondiabet,w-membran,w-famili,w-t2dm,w-promot,w-fat,w-d,w-bmi,w-haplotyp,w-triglycerid,w-interact\n",
            "\n",
            "11980626\tlabel=3\tw-use=0.008345896243653724\tw-examin=0.06875147170629754\tw-studi=0.004152703553077776\tw-2=0.00973277483635903\tw-method=0.020537131575129996\tw-protein=0.016192058126755438\tw-result=0.010708465486509246\tw-increas=0.006223847603404032\tw-observ=0.012978175327266145\tw-conclus=0.00913134532668071\tw-import=0.01643525724222513\tw-role=0.04623409918876832\tw-p=0.008069224360596776\tw-control=0.011520568388548902\tw-determin=0.012252286297668523\tw-group=0.049231993755866316\tw-type=0.005103701940231313\tw-signific=0.006341259233927808\tw-differ=0.03959653618150857\tw-suggest=0.010207867808642818\tw-sever=0.015387820347035666\tw-patient=0.006468181078170905\tw-suscept=0.06162878561933556\tw-howev=0.0397623429968222\tw-object=0.014935723434064923\tw-1=0.005070199883386405\tw-0=0.006375365929421363\tw-larg=0.022172249428544115\tw-influenc=0.02181666577105757\tw-10=0.0909694316719903\tw-product=0.019791869288718726\tw-test=0.014208741044444934\tw-17=0.021321961974385337\tw-associ=0.025393773548881046\tw-express=0.015986872089890844\tw-singl=0.022318992208954513\tw-9=0.013103148538191424\tw-mellitus=0.009347061906031842\tw-vs=0.017121022242547003\tw-combin=0.09758483183315368\tw-complex=0.024381942723283638\tw-shown=0.04359858316474375\tw-individu=0.03441789273248458\tw-gene=0.01601788407616509\tw-previous=0.01812694848847593\tw-confirm=0.02327425617610234\tw-variant=0.027400766832259373\tw-popul=0.0815856849439519\tw-aim=0.01615462234816693\tw-risk=0.03610173524197659\tw-genet=0.018661619821626004\tw-polymorph=0.06874279296568882\tw-allel=0.06649857813953824\tw-recent=0.01952568947849155\tw-preval=0.019909999524503685\tw-genotyp=0.07088779736842625\tw-form=0.024849462960111705\tw-famili=0.0203957617527753\tw-t2dm=0.27795194479776336\tw-haplotyp=0.2523556468521989\tw-marker=0.021678709173042286\tw-describ=0.024381942723283638\tsummary=w-use,w-examin,w-studi,w-2,w-method,w-protein,w-result,w-increas,w-observ,w-conclus,w-import,w-role,w-p,w-control,w-determin,w-group,w-type,w-signific,w-differ,w-suggest,w-sever,w-patient,w-suscept,w-howev,w-object,w-1,w-0,w-larg,w-influenc,w-10,w-product,w-test,w-17,w-associ,w-express,w-singl,w-9,w-mellitus,w-vs,w-combin,w-complex,w-shown,w-individu,w-gene,w-previous,w-confirm,w-variant,w-popul,w-aim,w-risk,w-genet,w-polymorph,w-allel,w-recent,w-preval,w-genotyp,w-form,w-famili,w-t2dm,w-haplotyp,w-marker,w-describ\n",
            "\n",
            "18513423\tlabel=2\tw-use=0.011127861658204966\tw-examin=0.036667451576692024\tw-studi=0.005536938070770368\tw-model=0.01977360958484946\tw-metabol=0.019657223369139675\tw-abnorm=0.02660786533374457\tw-compar=0.011199376237451307\tw-obtain=0.02897355564692443\tw-method=0.027382842100173332\tw-experiment=0.03248802756326294\tw-oxid=0.02951463624121117\tw-result=0.007138976991006165\tw-increas=0.008298463471205378\tw-conclus=0.012175127102240947\tw-muscl=0.09173687109617708\tw-control=0.038401894628496344\tw-chang=0.06802452176850603\tw-determin=0.0163363817302247\tw-stimul=0.02849491470711613\tw-signific=0.033820049247614975\tw-occur=0.025008354128024036\tw-sever=0.04103418759209511\tw-affect=0.050186628163018714\tw-patient=0.060369690062928444\tw-indic=0.019431311564835393\tw-howev=0.017672152443032094\tw-durat=0.02349151914025453\tw-onset=0.02507733287246593\tw-data=0.017584287967581384\tw-assess=0.018616025270531016\tw-relat=0.016119006649995736\tw-sampl=0.02757700037137492\tw-larg=0.029562999238058822\tw-influenc=0.02908888769474343\tw-hormon=0.03615489903494229\tw-contrast=0.027091143045972207\tw-major=0.02329580343386389\tw-mechan=0.023922621457798564\tw-therefor=0.029228939158780765\tw-5=0.012788790542322544\tw-cell=0.016254057345588692\tw-associ=0.011286121577280466\tw-analysi=0.020050178966753367\tw-process=0.030092877066811213\tw-care=0.030839586554500413\tw-reduct=0.02420356352622705\tw-target=0.02994119086242217\tw-subject=0.014770308437326992\tw-requir=0.02823475633521229\tw-includ=0.019396731920973298\tw-background=0.02437671844629779\tw-poor=0.09576018827450118\tw-intervent=0.02932333059014103\tw-frequent=0.03210265996406055\tw-symptom=0.10978899328764233\tw-nerv=0.036959792337702725\tw-seen=0.03088516817499253\tw-identifi=0.022404640783236545\tw-phenotyp=0.03586068948643543\tw-disord=0.03630507868528324\tw-aim=0.021539496464222575\tw-complet=0.02886701717139006\tw-like=0.030712976484129084\tw-reveal=0.029260310991893905\tw-conclud=0.028568312180991394\tw-form=0.03313261728014894\tw-area=0.03196037278915115\tsummary=w-use,w-examin,w-studi,w-model,w-metabol,w-abnorm,w-compar,w-obtain,w-method,w-experiment,w-oxid,w-result,w-increas,w-conclus,w-muscl,w-control,w-chang,w-determin,w-stimul,w-signific,w-occur,w-sever,w-affect,w-patient,w-indic,w-howev,w-durat,w-onset,w-data,w-assess,w-relat,w-sampl,w-larg,w-influenc,w-hormon,w-contrast,w-major,w-mechan,w-therefor,w-5,w-cell,w-associ,w-analysi,w-process,w-care,w-reduct,w-target,w-subject,w-requir,w-includ,w-background,w-poor,w-intervent,w-frequent,w-symptom,w-nerv,w-seen,w-identifi,w-phenotyp,w-disord,w-aim,w-complet,w-like,w-reveal,w-conclud,w-form,w-area\n",
            "\n",
            "15220206\tlabel=1\tw-studi=0.00804586313408819\tw-contribut=0.03485313926968112\tw-mice=0.09998895286208398\tw-30=0.03454180508203428\tw-increas=0.024117409463190624\tw-present=0.02857842664340352\tw-muscl=0.04443504693721077\tw-chang=0.02471203329871508\tw-decreas=0.02302784515431085\tw-group=0.019077397580398198\tw-signific=0.012286189765735128\tw-region=0.04230352510105988\tw-tissu=0.03763775658219519\tw-specif=0.03602075491984036\tw-week=0.03258089356566842\tw-addit=0.06006615714228142\tw-n=0.034500758648621885\tw-treatment=0.08501569368964973\tw-contrast=0.03936681723867836\tw-product=0.038346746746892535\tw-plasma=0.028861581477710625\tw-reduc=0.023745668308256484\tw-random=0.03628655719838\tw-factor=0.023887040783221652\tw-reduct=0.10551240974714604\tw-age=0.13889246597887409\tw-progress=0.036895704803269896\tw-treat=0.03423655335087456\tw-receiv=0.038907225282330024\tw-complex=0.04724001402636205\tw-inhibitor=0.04462979395350007\tw-demonstr=0.06390640802406136\tw-seen=0.08976002000857203\tw-identifi=0.0325567436381406\tw-vascular=0.08508346596283226\tw-40=0.04054845194352961\tw-particular=0.045392793501365616\tw-receptor=0.03937614838258064\tw-growth=0.09739061012431212\tw-total=0.029971603082002668\tw-area=0.09288483341847051\tw-20=0.06762487820653204\tw-cytokin=0.054620920078021384\tsummary=w-studi,w-contribut,w-mice,w-30,w-increas,w-present,w-muscl,w-chang,w-decreas,w-group,w-signific,w-region,w-tissu,w-specif,w-week,w-addit,w-n,w-treatment,w-contrast,w-product,w-plasma,w-reduc,w-random,w-factor,w-reduct,w-age,w-progress,w-treat,w-receiv,w-complex,w-inhibitor,w-demonstr,w-seen,w-identifi,w-vascular,w-40,w-particular,w-receptor,w-growth,w-total,w-area,w-20,w-cytokin\n",
            "\n",
            "16732009\tlabel=3\tw-2=0.01711863942848964\tw-compar=0.022160467874105774\tw-method=0.009030511756440141\tw-result=0.004708686951514704\tw-increas=0.010946909259887942\tw-similar=0.012458016440514124\tw-conclus=0.008030402982329135\tw-p=0.014192678308000004\tw-control=0.015197345533830467\tw-chang=0.0448672377622061\tw-group=0.025977732875435842\tw-type=0.008976723979981317\tw-signific=0.011153420496553875\tw-differ=0.008705621429977061\tw-ratio=0.015221024147170631\tw-respect=0.025451204802800186\tw-high=0.025316522419504807\tw-primari=0.019803853120156423\tw-patient=0.022753317835267864\tw-clinic=0.012284432724507523\tw-function=0.012255590884914942\tw-8=0.010308002297082121\tw-week=0.02957698139294722\tw-addit=0.013632035663496491\tw-object=0.01313496245265284\tw-mean=0.023730945890224047\tw-hba=0.024230850269246748\tw-1c=0.023671963309123262\tw-a1c=0.17434039474615406\tw-trial=0.018377623251608546\tw-research=0.015291331524253266\tw-design=0.013022898642464746\tw-profil=0.021576695627584463\tw-n=0.031319837638465256\tw-1=0.022294495941131712\tw-consist=0.018622924391310233\tw-adjust=0.03754087203915832\tw-gt=0.06484431076755721\tw-0=0.05606704788994674\tw-14=0.016682217190397166\tw-insulin=0.01320268833378485\tw-24=0.017385810453400347\tw-did=0.013027176205068219\tw-15=0.015928534966325934\tw-5=0.042175798597021155\tw-17=0.018751228970381433\tw-associ=0.014888075272157209\tw-glycem=0.01843701076870365\tw-random=0.01647049404749163\tw-ani=0.018838073303431787\tw-metformin=0.12658599141693508\tw-baselin=0.034220776628213406\tw-9=0.05761668151545166\tw-95=0.03218286278372917\tw-ci=0.038474916228727514\tw-12=0.013585590189178013\tw-reduct=0.015964052538575284\tw-lt=0.029895085466719488\tw-improv=0.013690523530693211\tw-poor=0.042107316829922505\tw-toler=0.016408190291777213\tw-hypoglycemia=0.023652795193180812\tw-event=0.04118275741379441\tw-demonstr=0.014503581962907543\tw-risk=0.010583014586300465\tw-human=0.015059634362886452\tw-greater=0.03074253897332392\tw-small=0.021965708313975405\tw-perform=0.016190995945820238\tw-16=0.017160024361426128\tw-antibodi=0.018925994246529864\tw-bind=0.023038884319244585\tsummary=w-2,w-compar,w-method,w-result,w-increas,w-similar,w-conclus,w-p,w-control,w-chang,w-group,w-type,w-signific,w-differ,w-ratio,w-respect,w-high,w-primari,w-patient,w-clinic,w-function,w-8,w-week,w-addit,w-object,w-mean,w-hba,w-1c,w-a1c,w-trial,w-research,w-design,w-profil,w-n,w-1,w-consist,w-adjust,w-gt,w-0,w-14,w-insulin,w-24,w-did,w-15,w-5,w-17,w-associ,w-glycem,w-random,w-ani,w-metformin,w-baselin,w-9,w-95,w-ci,w-12,w-reduct,w-lt,w-improv,w-poor,w-toler,w-hypoglycemia,w-event,w-demonstr,w-risk,w-human,w-greater,w-small,w-perform,w-16,w-antibodi,w-bind\n",
            "\n",
            "1446803\tlabel=2\tw-rat=0.061231635716775465\tw-studi=0.009535837788548966\tw-anim=0.3608055455036815\tw-develop=0.04952253134385021\tw-2=0.011174667404708516\tw-increas=0.014291798200409258\tw-similar=0.03252926515023132\tw-decreas=0.08187678277088302\tw-suggest=0.023440289042068695\tw-24=0.045396282850545346\tw-number=0.12951124155300917\tw-associ=0.019437209383094133\tw-express=0.07342119033875795\tw-respons=0.058880861029655854\tw-analysi=0.034530863776075234\tw-mellitus=0.021463623636073115\tw-background=0.04198212621306841\tw-gene=0.036781807878601316\tw-phenotyp=0.1852802290132497\tw-case=0.046935447789628805\tw-reveal=0.05039275781937283\tw-genet=0.042852608479289345\tw-pattern=0.05866071586292945\tw-haplotyp=0.06438703746846226\tw-describ=0.05598816477198465\tw-bb=0.2919725829313653\tw-alpha=0.19087594639719832\tw-beta=0.0949181095965971\tw-cd4=0.06786154302534698\tw-spontan=0.05919690035113347\tsummary=w-rat,w-studi,w-anim,w-develop,w-2,w-increas,w-similar,w-decreas,w-suggest,w-24,w-number,w-associ,w-express,w-respons,w-analysi,w-mellitus,w-background,w-gene,w-phenotyp,w-case,w-reveal,w-genet,w-pattern,w-haplotyp,w-describ,w-bb,w-alpha,w-beta,w-cd4,w-spontan\n",
            "\n",
            "12234175\tlabel=3\tw-rat=0.050098611040998116\tw-pathogenesi=0.08421509282400105\tw-correl=0.06180146842182668\tw-type=0.019177546684505543\tw-suggest=0.03835683661429423\tw-dysfunct=0.08745478078293001\tw-appear=0.07435250497048068\tw-patient=0.02430468041494522\tw-report=0.06032671026901624\tw-insulin=0.02820574325854036\tw-mechan=0.13483659367122824\tw-cell=0.04580688888302268\tw-diseas=0.12062832802394916\tw-noninsulindepend=0.07580982902475651\tw-mellitus=0.0351222932226651\tw-ii=0.07764138521122622\tw-variant=0.10296045718788371\tw-like=0.08655475190981832\tw-character=0.08543551400999924\tw-niddm=0.16448065877219878\tw-pancreat=0.07137053113761076\tw-betacel=0.15427170981881996\tw-islet=0.12922949278819545\tw-form=0.18674747921538493\tw-earli=0.0703739291120097\tw-vitro=0.08851722195340672\tsummary=w-rat,w-pathogenesi,w-correl,w-type,w-suggest,w-dysfunct,w-appear,w-patient,w-report,w-insulin,w-mechan,w-cell,w-diseas,w-noninsulindepend,w-mellitus,w-ii,w-variant,w-like,w-character,w-niddm,w-pancreat,w-betacel,w-islet,w-form,w-earli,w-vitro\n",
            "\n",
            "3905463\tlabel=2\tw-use=0.012620623587964168\tw-2=0.007358927315295852\tw-compar=0.0254034631727554\tw-6=0.015960730299176915\tw-level=0.013074401020693401\tw-result=0.008096644636141138\tw-3=0.013542159086585705\tw-present=0.022305113477778356\tw-control=0.008710673659634537\tw-higher=0.01972556634601443\tw-type=0.007717793177910767\tw-signific=0.019178442561147518\tw-differ=0.014969422214960558\tw-suggest=0.015436287905752556\tw-respect=0.06564548555844194\tw-patient=0.07824921499445778\tw-bodi=0.025968941397343275\tw-weight=0.026980861969156614\tw-durat=0.02664282048833745\tw-addit=0.023440451567719577\tw-blood=0.017807755890002136\tw-glucos=0.0132798882912081\tw-a1c=0.04282577640976955\tw-trial=0.03160054729849762\tw-n=0.053854842768580505\tw-1=0.015334263061949129\tw-treatment=0.08294214018502412\tw-0=0.04820398629562494\tw-insulin=0.03405327539750604\tw-effect=0.013168943985899116\tw-did=0.022400388352617304\tw-l=0.05962753357969757\tw-cell=0.018434479672435954\tw-hemoglobin=0.035059423745689516\tw-12=0.0700817640246622\tw-age=0.015486267983707214\tw-becaus=0.03353793324779003\tw-mg=0.023127992564042722\tw-treat=0.10688484948565717\tw-requir=0.03202234559969199\tw-drug=0.03704013989296485\tw-13=0.02942806557235358\tw-11=0.026700258548473608\tw-kg=0.027023783692130977\tw-symptom=0.04150559502337698\tw-diagnosi=0.03493540448752331\tw-cpeptid=0.04173687341434007\tw-islet=0.02600349550006372\tw-depend=0.03281696092294839\tw-presenc=0.02948054600601276\tw-antibodi=0.03254347791171599\tw-given=0.03706460353597541\tw-basal=0.036362789044200185\tsummary=w-use,w-2,w-compar,w-6,w-level,w-result,w-3,w-present,w-control,w-higher,w-type,w-signific,w-differ,w-suggest,w-respect,w-patient,w-bodi,w-weight,w-durat,w-addit,w-blood,w-glucos,w-a1c,w-trial,w-n,w-1,w-treatment,w-0,w-insulin,w-effect,w-did,w-l,w-cell,w-hemoglobin,w-12,w-age,w-becaus,w-mg,w-treat,w-requir,w-drug,w-13,w-11,w-kg,w-symptom,w-diagnosi,w-cpeptid,w-islet,w-depend,w-presenc,w-antibodi,w-given,w-basal\n",
            "\n",
            "16129698\tlabel=1\tw-rat=0.0918474535751632\tw-model=0.02554091238043055\tw-contribut=0.030980568239716552\tw-investig=0.02462304298131861\tw-compar=0.014465860973374602\tw-induc=0.02874231885901447\tw-inject=0.03428944362553412\tw-experiment=0.04196370226921463\tw-level=0.014890290051345262\tw-oxid=0.11436921543469326\tw-measur=0.0384641559107547\tw-increas=0.010718848650306943\tw-stress=0.13687640423599307\tw-p=0.01389699750991667\tw-correl=0.02832567302667056\tw-signific=0.05460528784771168\tw-dysfunct=0.04008344119217625\tw-function=0.04800106429925019\tw-week=0.02896079428059415\tw-evalu=0.027625328614674226\tw-1=0.008732010910276587\tw-0=0.010979796878447901\tw-acid=0.03340391788564408\tw-effect=0.01499796398394066\tw-plasma=0.051309478182596666\tw-mechan=0.03090005271632314\tw-streptozotocin=0.03550797081534288\tw-express=0.16519767826220538\tw-lt=0.01951484745744189\tw-improv=0.02681060858094087\tw-stz=0.09328481542080182\tw-demonstr=0.028402848010693937\tw-inhibit=0.03769348430196222\tw-pathway=0.088697598715167\tw-alter=0.032446669671041\tw-gene=0.027586355908950987\tw-involv=0.032398432350302\tw-endotheli=0.046320057253312424\tw-impair=0.058321898421068503\tw-human=0.05898356792130527\tw-05=0.02739311607677064\tw-transgen=0.053622581071640765\tw-multipl=0.037896393519214415\tw-male=0.03645146109675945\tw-cytokin=0.09710385791648246\tw-protect=0.07776186513417944\tw-pressur=0.03351529690184713\tsummary=w-rat,w-model,w-contribut,w-investig,w-compar,w-induc,w-inject,w-experiment,w-level,w-oxid,w-measur,w-increas,w-stress,w-p,w-correl,w-signific,w-dysfunct,w-function,w-week,w-evalu,w-1,w-0,w-acid,w-effect,w-plasma,w-mechan,w-streptozotocin,w-express,w-lt,w-improv,w-stz,w-demonstr,w-inhibit,w-pathway,w-alter,w-gene,w-involv,w-endotheli,w-impair,w-human,w-05,w-transgen,w-multipl,w-male,w-cytokin,w-protect,w-pressur\n",
            "\n",
            "17853331\tlabel=3\tw-use=0.023790600786507167\tw-metabol=0.021012893946321717\tw-2=0.02080800137428482\tw-method=0.0146356569845754\tw-level=0.012322998663182285\tw-result=0.007631320231765211\tw-conclus=0.013014791040326528\tw-control=0.008210060230919908\tw-correl=0.023441936297934257\tw-type=0.02182272553754079\tw-signific=0.009038116609276416\tw-high=0.041030225990231935\tw-patient=0.05531410025470291\tw-possibl=0.027259437915900134\tw-valu=0.09897758008418846\tw-4=0.01412042754825399\tw-durat=0.07533487172564382\tw-week=0.023967553887388263\tw-data=0.03759399496517399\tw-provid=0.050971800136704146\tw-evalu=0.022862340922489014\tw-mean=0.01923024925587121\tw-blood=0.01678432164345029\tw-glucos=0.03755002896134704\tw-adjust=0.09126315444002281\tw-treatment=0.04690521031153089\tw-day=0.0953047724627465\tw-insulin=0.010698730201515308\tw-effect=0.024824216249281092\tw-test=0.040503077919797056\tw-analys=0.03121964056369302\tw-reduc=0.017468077835958794\tw-number=0.026795429286829486\tw-agent=0.032859980752145586\tw-achiev=0.06715099596465338\tw-lt=0.01615021858546915\tw-improv=0.022188089860088998\tw-mg=0.043597595178195475\tw-dl=0.07274160574366448\tw-oral=0.028721794230320414\tw-frequenc=0.05796020583655022\tw-50=0.02808760295336038\tw-identifi=0.023949788423459753\tw-aim=0.023024978978996545\tw-hypoglycaemia=0.22267926202818802\tw-peopl=0.03850574957922005\tw-detect=0.08476256773258345\tw-exercis=0.038411662400065584\tsummary=w-use,w-metabol,w-2,w-method,w-level,w-result,w-conclus,w-control,w-correl,w-type,w-signific,w-high,w-patient,w-possibl,w-valu,w-4,w-durat,w-week,w-data,w-provid,w-evalu,w-mean,w-blood,w-glucos,w-adjust,w-treatment,w-day,w-insulin,w-effect,w-test,w-analys,w-reduc,w-number,w-agent,w-achiev,w-lt,w-improv,w-mg,w-dl,w-oral,w-frequenc,w-50,w-identifi,w-aim,w-hypoglycaemia,w-peopl,w-detect,w-exercis\n",
            "\n",
            "12679437\tlabel=3\tw-examin=0.015787374984409065\tw-studi=0.004767918894274483\tw-metabol=0.033854106913518325\tw-2=0.022349334809417032\tw-month=0.04221550153039078\tw-compar=0.009643907315583069\tw-normal=0.057941637258603115\tw-result=0.006147452408921975\tw-increas=0.007145899100204629\tw-3=0.030846029030556325\tw-p=0.02779399501983334\tw-r=0.02723045178824963\tw-control=0.013227319260926518\tw-dure=0.04392601274885387\tw-mass=0.04694667453657375\tw-correl=0.037767564035560745\tw-concentr=0.017462815421933488\tw-fast=0.03918155269966241\tw-type=0.011719611862753387\tw-signific=0.007280705046361556\tw-differ=0.011365672422470052\tw-suggest=0.011720144521034348\tw-affect=0.021608131570188607\tw-bodi=0.0591514776272819\tw-valu=0.01993298487806573\tw-4=0.011374788858315714\tw-data=0.015142025749861746\tw-sensit=0.022423395993845758\tw-glucos=0.020165756294056744\tw-relat=0.013880255726385217\tw-1=0.005821340606851058\tw-treatment=0.012594917583651812\tw-0=0.043919187513791605\tw-secret=0.023072250579138014\tw-insulin=0.03447368620488266\tw-plasma=0.01710315939419889\tw-did=0.01700770226772795\tw-25=0.02273436098239016\tw-reduc=0.028143014291266942\tw-therapi=0.058061581534801045\tw-random=0.021503145006447403\tw-metformin=0.06610601773995498\tw-01=0.01968353668542977\tw-interv=0.02560428084573504\tw-lt=0.03902969491488378\tw-progress=0.02186412136490068\tw-mellitus=0.010731811818036558\tw-index=0.02258012211456799\tw-subject=0.03815663012976139\tw-0001=0.02829159422101819\tw-drug=0.05624613835598365\tw-nonobes=0.02752143212399126\tw-impair=0.038881265614045674\tw-total=0.017760949974520098\tw-001=0.0189009002686522\tw-nondiabet=0.05690907454702661\tw-releas=0.02958757611764135\tw-fat=0.2108714661897158\tw-bmi=0.055625755583205067\tsummary=w-examin,w-studi,w-metabol,w-2,w-month,w-compar,w-normal,w-result,w-increas,w-3,w-p,w-r,w-control,w-dure,w-mass,w-correl,w-concentr,w-fast,w-type,w-signific,w-differ,w-suggest,w-affect,w-bodi,w-valu,w-4,w-data,w-sensit,w-glucos,w-relat,w-1,w-treatment,w-0,w-secret,w-insulin,w-plasma,w-did,w-25,w-reduc,w-therapi,w-random,w-metformin,w-01,w-interv,w-lt,w-progress,w-mellitus,w-index,w-subject,w-0001,w-drug,w-nonobes,w-impair,w-total,w-001,w-nondiabet,w-releas,w-fat,w-bmi\n",
            "\n",
            "12145233\tlabel=2\tw-common=0.015983951551392643\tw-develop=0.0084627110524301\tw-investig=0.01122062718136038\tw-compar=0.006592037911917541\tw-method=0.00805887441555734\tw-level=0.02714179452397111\tw-measur=0.008763984891058034\tw-result=0.004202056076984641\tw-increas=0.009769077250912658\tw-3=0.007028209146202707\tw-observ=0.020370806842797493\tw-conclus=0.007166372281698785\tw-p=0.04432966294302533\tw-control=0.00452072936765843\tw-dure=0.02001691720200936\tw-higher=0.020474638485736496\tw-period=0.01460289801353796\tw-group=0.007727553450287877\tw-type=0.008010873931502314\tw-signific=0.014930053386209776\tw-patient=0.04568664609644765\tw-indic=0.011437417566643616\tw-4=0.023325516393001847\tw-durat=0.027654573165109757\tw-8=0.009198913442332778\tw-data=0.010350245449272585\tw-object=0.011721706998886396\tw-mean=0.02117761626912399\tw-research=0.013646061676707028\tw-design=0.011621700687262844\tw-1=0.019895721061389694\tw-7=0.009166466360480609\tw-0=0.08505867961531793\tw-larg=0.01740100588062956\tw-wherea=0.014231974334842789\tw-hormon=0.021281048166136917\tw-15=0.014214705254759218\tw-5=0.0301103169730632\tw-posit=0.01488090526990284\tw-17=0.016733691676099886\tw-associ=0.00664309687776635\tw-9=0.01028348366288441\tw-12=0.024247698945241772\tw-lt=0.062249893155384255\tw-age=0.03214870821934156\tw-year=0.046098836069159224\tw-subject=0.00869391572576842\tw-treat=0.013867970977569442\tw-rang=0.08294920992867805\tw-100=0.03553616962666377\tw-particular=0.018386954329667086\tw-elev=0.045828575120839925\tw-total=0.02428079237023001\tw-001=0.09043721900696876\tw-ml=0.12894267978193688\tw-autoimmun=0.04461399786860679\tw-preval=0.031251138494157685\tw-16=0.015313692626335975\tw-cohort=0.018254978449467833\tw-antibodi=0.11822757165395556\tw-children=0.01668679334553846\tw-adolesc=0.022318433322925667\tw-life=0.04137320703443235\tsummary=w-common,w-develop,w-investig,w-compar,w-method,w-level,w-measur,w-result,w-increas,w-3,w-observ,w-conclus,w-p,w-control,w-dure,w-higher,w-period,w-group,w-type,w-signific,w-patient,w-indic,w-4,w-durat,w-8,w-data,w-object,w-mean,w-research,w-design,w-1,w-7,w-0,w-larg,w-wherea,w-hormon,w-15,w-5,w-posit,w-17,w-associ,w-9,w-12,w-lt,w-age,w-year,w-subject,w-treat,w-rang,w-100,w-particular,w-elev,w-total,w-001,w-ml,w-autoimmun,w-preval,w-16,w-cohort,w-antibodi,w-children,w-adolesc,w-life\n",
            "\n",
            "14523186\tlabel=2\tw-use=0.006899274228087079\tw-studi=0.010298704811632885\tw-2=0.024137281594170393\tw-compar=0.03471806633609905\tw-method=0.008488681051053734\tw-result=0.004426165734423822\tw-increas=0.005145047352147334\tw-3=0.022209140902000556\tw-conclus=0.007548578803389387\tw-control=0.019047339735734185\tw-dure=0.02108448611944986\tw-period=0.015381719240926652\tw-group=0.05697782744012262\tw-type=0.01687624108236488\tw-signific=0.010484215266760643\tw-respect=0.011962066257316087\tw-high=0.02379753107433452\tw-sever=0.012720598153549484\tw-complic=0.029192282679664477\tw-clinic=0.011547366761037072\tw-4=0.008189847977987314\tw-8=0.009689522159257193\tw-evalu=0.01326015773504363\tw-object=0.012346864705493671\tw-mean=0.011153544568405303\tw-a1c=0.046822848874681376\tw-assess=0.01154193566772923\tw-trial=0.017274965856512035\tw-design=0.024483049447833725\tw-1=0.02095682618466381\tw-treatment=0.009068340660229305\tw-7=0.038621378265491633\tw-gt=0.015238413030375946\tw-larg=0.018329059527596468\tw-standard=0.04003763037886591\tw-manag=0.040258881591948736\tw-rate=0.02372138390372766\tw-chronic=0.018936969278399555\tw-25=0.01636873990732092\tw-5=0.015858100272479955\tw-reduc=0.0405259405794244\tw-number=0.015541348986361102\tw-glycem=0.06932316049032572\tw-random=0.015482264404642133\tw-medic=0.019455971447206302\tw-glycosyl=0.04193472446079535\tw-hemoglobin=0.03833163662862054\tw-baselin=0.016083765015260303\tw-health=0.018126799594991676\tw-care=0.13384380564653178\tw-age=0.008465826497759944\tw-improv=0.0643454605942581\tw-need=0.01896997264613193\tw-followup=0.017496701326444538\tw-year=0.009711488131902877\tw-remain=0.014814263303985355\tw-poor=0.01979043891006358\tw-intervent=0.10908278979532464\tw-hypoglycemia=0.022233627481589968\tw-event=0.0774235839379335\tw-incid=0.015664470362755274\tw-conduct=0.019533526385966348\tw-frequenc=0.03361691938519913\tw-60=0.018747639448254902\tw-case=0.03379352240853274\tw-risk=0.009948033711122437\tw-40=0.017300672829239302\tw-estim=0.019036473613177392\tw-like=0.019042045420160032\tw-particular=0.019367591893915997\tw-total=0.012787883981654473\tw-02=0.018795813082199832\tw-perform=0.015219536189071025\tw-16=0.01613042289974056\tw-outcom=0.05384818471271803\tw-approach=0.020349570849249436\tw-acut=0.021171344148461266\tw-children=0.017576755657300512\tsummary=w-use,w-studi,w-2,w-compar,w-method,w-result,w-increas,w-3,w-conclus,w-control,w-dure,w-period,w-group,w-type,w-signific,w-respect,w-high,w-sever,w-complic,w-clinic,w-4,w-8,w-evalu,w-object,w-mean,w-a1c,w-assess,w-trial,w-design,w-1,w-treatment,w-7,w-gt,w-larg,w-standard,w-manag,w-rate,w-chronic,w-25,w-5,w-reduc,w-number,w-glycem,w-random,w-medic,w-glycosyl,w-hemoglobin,w-baselin,w-health,w-care,w-age,w-improv,w-need,w-followup,w-year,w-remain,w-poor,w-intervent,w-hypoglycemia,w-event,w-incid,w-conduct,w-frequenc,w-60,w-case,w-risk,w-40,w-estim,w-like,w-particular,w-total,w-02,w-perform,w-16,w-outcom,w-approach,w-acut,w-children\n",
            "\n",
            "3867179\tlabel=2\tw-common=0.05373328393872421\tw-pathogenesi=0.05912974602536244\tw-c=0.058933042660595396\tw-increas=0.016420363889831913\tw-play=0.05803839933626562\tw-role=0.040659775173101216\tw-control=0.015197345533830467\tw-determin=0.032325180870444614\tw-group=0.025977732875435842\tw-signific=0.03346026148966162\tw-high=0.07594956725851443\tw-appear=0.052204950298422606\tw-patient=0.08532494188225449\tw-howev=0.034968301642595415\tw-report=0.042357051891011396\tw-insulin=0.019804032500677273\tw-low=0.04999299051943172\tw-major=0.0460959514755179\tw-did=0.039081528615204655\tw-associ=0.06699633872470744\tw-mellitus=0.024660333539318048\tw-frequenc=0.16093206088659157\tw-50=0.05199194589239049\tw-gene=0.08451989895508388\tw-peopl=0.07127660028493925\tw-depend=0.05725512331237804\tw-b=0.06207460495523534\tw-antigen=0.1256897651727669\tw-iddm=0.26434665853128664\tw-american=0.07691719071926355\tw-hla=0.07165891291350981\tsummary=w-common,w-pathogenesi,w-c,w-increas,w-play,w-role,w-control,w-determin,w-group,w-signific,w-high,w-appear,w-patient,w-howev,w-report,w-insulin,w-low,w-major,w-did,w-associ,w-mellitus,w-frequenc,w-50,w-gene,w-peopl,w-depend,w-b,w-antigen,w-iddm,w-american,w-hla\n",
            "\n",
            "1744436\tlabel=1\tw-model=0.040865459808688887\tw-metabol=0.04062492829622199\tw-develop=0.059427037612620255\tw-induc=0.04598771017442316\tw-observ=0.035762083124022265\tw-conclus=0.02516192934463129\tw-suggest=0.028128346850482436\tw-occur=0.05168393186458301\tw-patient=0.01782343230429316\tw-clinic=0.03849122253679024\tw-indic=0.04015804390065981\tw-condit=0.12239453373841153\tw-data=0.07268172359933639\tw-addit=0.04271371174562234\tw-glucos=0.04839781510573619\tw-befor=0.0518154895584658\tw-wherea=0.04997004322011468\tw-induct=0.07082835167889615\tw-associ=0.04664930251942593\tw-ani=0.05902596301741961\tw-improv=0.0428969737295054\tw-need=0.0632332421537731\tw-toler=0.05141232958090193\tw-frequent=0.06634549725905846\tw-time=0.04407598760969927\tw-diagnosi=0.06366007039948691\tw-alter=0.05191467147366561\tw-disord=0.07503049594958536\tw-impair=0.04665751873685481\tw-genet=0.05142313017514722\tw-pancreat=0.3663687265064019\tw-resist=0.04850172707380358\tw-prevent=0.04583596451649293\tw-nondiabet=0.04552725963762129\tsummary=w-model,w-metabol,w-develop,w-induc,w-observ,w-conclus,w-suggest,w-occur,w-patient,w-clinic,w-indic,w-condit,w-data,w-addit,w-glucos,w-befor,w-wherea,w-induct,w-associ,w-ani,w-improv,w-need,w-toler,w-frequent,w-time,w-diagnosi,w-alter,w-disord,w-impair,w-genet,w-pancreat,w-resist,w-prevent,w-nondiabet\n",
            "\n",
            "15551047\tlabel=2\tw-studi=0.009477335716226581\tw-2=0.011106111162961839\tw-month=0.027971007148970585\tw-compar=0.00638982816001823\tw-method=0.007811669678883803\tw-6=0.008029324444984704\tw-level=0.032886530174750274\tw-result=0.004073158651310266\tw-increas=0.018938824609131287\tw-conclus=0.006946544911094528\tw-play=0.0167349985816226\tw-role=0.011723984252366608\tw-p=0.024554204189300623\tw-r=0.01804226253454577\tw-chang=0.009702884239986288\tw-dure=0.04850725334229262\tw-correl=0.012511953729572273\tw-decreas=0.009041607913349046\tw-determin=0.009320757674299981\tw-period=0.014154956356680968\tw-group=0.04494307160044729\tw-type=0.007765141602315128\tw-suggest=0.015530989058548583\tw-patient=0.014761738411531146\tw-valu=0.013207131084853368\tw-4=0.007536669918393234\tw-data=0.010032753257577107\tw-evalu=0.012202599142678187\tw-mean=0.020527996138169267\tw-blood=0.017917005926137118\tw-independ=0.014521149735653193\tw-relat=0.00919673385551904\tw-n=0.027092620288488353\tw-1=0.053999184034103066\tw-min=0.08085406235490317\tw-treatment=0.016690197534164366\tw-0=0.03879977301831897\tw-influenc=0.016596727335037663\tw-enhanc=0.01663655057182188\tw-rate=0.02182949438993343\tw-plasma=0.07932508283444394\tw-5=0.014593343809030632\tw-associ=0.006439320899920757\tw-initi=0.015330037411275314\tw-baselin=0.014801010750239542\tw-9=0.009968039378746851\tw-01=0.026083705055538836\tw-95=0.0139195817561528\tw-interv=0.016964799578769233\tw-ci=0.016640991375001777\tw-lt=0.034480221274498554\tw-diseas=0.00814056201388614\tw-renal=0.016766657174011004\tw-progress=0.028973314201340777\tw-hypertens=0.01767920478910518\tw-followup=0.01610125888936614\tw-year=0.04468476134311139\tw-mg=0.01163494104448775\tw-treat=0.013442573094821914\tw-rang=0.01608095112727746\tw-daili=0.017446889594978462\tw-100=0.01722305153684931\tw-11=0.013432031907821078\tw-longterm=0.05337211346404126\tw-ii=0.015718808048898563\tw-aim=0.012289405958114721\tw-total=0.011767991394160557\tw-hypothesi=0.016024444431386193\tw-001=0.012523295883524159\tw-ml=0.23211942530420537\tw-pressur=0.02960860585193857\tw-arteri=0.018019973793135458\tw-nephropathi=0.054009930652496346\tsummary=w-studi,w-2,w-month,w-compar,w-method,w-6,w-level,w-result,w-increas,w-conclus,w-play,w-role,w-p,w-r,w-chang,w-dure,w-correl,w-decreas,w-determin,w-period,w-group,w-type,w-suggest,w-patient,w-valu,w-4,w-data,w-evalu,w-mean,w-blood,w-independ,w-relat,w-n,w-1,w-min,w-treatment,w-0,w-influenc,w-enhanc,w-rate,w-plasma,w-5,w-associ,w-initi,w-baselin,w-9,w-01,w-95,w-interv,w-ci,w-lt,w-diseas,w-renal,w-progress,w-hypertens,w-followup,w-year,w-mg,w-treat,w-rang,w-daili,w-100,w-11,w-longterm,w-ii,w-aim,w-total,w-hypothesi,w-001,w-ml,w-pressur,w-arteri,w-nephropathi\n",
            "\n"
          ]
        }
      ],
      "source": [
        "with open(os.path.join(data_dir, node_data_name)) as f:\n",
        "  for line in f.readlines()[:50]:\n",
        "    print(line)\n"
      ],
      "id": "2pJQMqGlutVF"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IXzrQNuEVoxp",
        "outputId": "dbe146e8-7c39-4956-981c-ddd74e0c64d8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "set()"
            ]
          },
          "execution_count": 41,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "set(node_ID.keys()) - set(data_s['ID'])"
      ],
      "id": "IXzrQNuEVoxp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d250393f",
        "outputId": "156b57d0-79d8-4cb8-df1e-58759e06e652"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[5 2 0 ... 2 2 2]\n"
          ]
        }
      ],
      "source": [
        "catagories=data_s['class'].to_numpy()\n",
        "print(catagories)"
      ],
      "id": "d250393f"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H9P4nGnFbu0a",
        "outputId": "86c2f463-e103-4004-c6d9-5de7e9f79381"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "metadata": {},
          "execution_count": 46
        }
      ],
      "source": [
        "max(catagories)"
      ],
      "id": "H9P4nGnFbu0a"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1ccc24d",
        "outputId": "62affc5d-30a4-4c0b-b14c-6a872a296feb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0, 1, 2, 3, 4, 5, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 19, 24, 25, 26, 27, 28, 31, 32, 33, 37, 38, 39, 41, 43, 45, 46, 47, 48, 49, 50, 51, 55, 56, 62, 63, 64, 65, 66, 67, 68, 70, 71, 73, 74, 75, 77, 78, 80, 82, 83, 84, 85, 86, 87, 89, 90, 91, 92, 93, 96, 97, 100, 101, 103, 104, 109, 110, 111, 112, 113, 117, 118, 119, 122, 125, 127, 128, 129, 131, 133, 134, 135, 138, 139, 141, 143, 146, 147, 148, 149, 153, 154, 155, 156, 157, 158, 161, 164, 165, 166, 167, 169, 175, 177, 180, 181, 182, 186, 188, 189, 191, 193, 194, 195, 196, 197, 200, 201, 203, 204, 207, 209, 211, 213, 215, 216, 218, 221, 222, 226, 229, 231, 232, 236, 237, 240, 241, 242, 244, 245, 246, 247, 250, 252, 254, 255, 256, 259, 260, 261, 262, 264, 265, 266, 267, 268, 269, 271, 273, 274, 275, 276, 277, 279, 280, 283, 284, 288, 289, 291, 292, 293, 295, 297, 298, 299, 300, 302, 303, 304, 305, 306, 308, 309, 310, 312, 313, 314, 315, 316, 318, 320, 322, 323, 324, 326, 329, 330, 331, 332, 333, 334, 335, 336, 339, 340, 341, 342, 343, 345, 346, 347, 350, 352, 353, 354, 355, 356, 358, 359, 360, 362, 363, 364, 366, 368, 369, 370, 373, 374, 375, 377, 378, 379, 380, 381, 385, 388, 390, 392, 393, 394, 395, 396, 397, 400, 401, 402, 403, 404, 406, 407, 408, 409, 410, 411, 412, 413, 416, 417, 418, 419, 420, 423, 424, 426, 428, 429, 434, 435, 437, 438, 442, 443, 444, 445, 447, 448, 449, 450, 451, 453, 454, 455, 456, 458, 460, 461, 462, 463, 465, 466, 467, 469, 470, 473, 476, 477, 480, 482, 483, 484, 485, 487, 488, 489, 490, 491, 492, 493, 496, 498, 500, 501, 503, 505, 506, 513, 514, 515, 516, 518, 522, 524, 525, 526, 527, 528, 530, 531, 532, 533, 534, 536, 537, 540, 541, 543, 544, 547, 549, 550, 551, 554, 556, 557, 559, 561, 562, 563, 565, 566, 571, 572, 573, 576, 577, 578, 579, 581, 582, 583, 586, 587, 588, 589, 590, 591, 592, 593, 595, 596, 597, 598, 599, 600, 601, 602, 603, 604, 608, 609, 610, 612, 613, 614, 615, 616, 617, 619, 621, 622, 625, 626, 627, 628, 629, 630, 632, 633, 634, 635, 636, 637, 638, 639, 642, 644, 647, 648, 652, 654, 655, 656, 658, 659, 660, 662, 663, 664, 665, 668, 669, 671, 674, 675, 676, 678, 679, 680, 681, 683, 684, 687, 688, 692, 694, 695, 696, 698, 699, 700, 701, 702, 704, 708, 710, 712, 713, 715, 716, 717, 718, 719, 721, 722, 723, 724, 726, 729, 730, 734, 735, 736, 744, 745, 746, 749, 752, 753, 756, 757, 758, 759, 760, 762, 764, 766, 768, 769, 771, 772, 773, 774, 775, 776, 779, 780, 781, 783, 784, 786, 787, 788, 790, 791, 792, 795, 796, 797, 798, 800, 802, 803, 804, 805, 806, 811, 813, 814, 817, 818, 819, 822, 823, 826, 828, 829, 830, 831, 832, 833, 834, 837, 838, 840, 841, 842, 843, 845, 846, 847, 848, 852, 853, 854, 855, 856, 857, 858, 860, 861, 863, 866, 868, 871, 873, 874, 875, 876, 877, 879, 880, 881, 883, 884, 885, 887, 888, 890, 891, 892, 893, 894, 896, 899, 900, 902, 903, 904, 905, 906, 907, 909, 911, 913, 914, 915, 916, 917, 919, 920, 923, 924, 928, 931, 932, 933, 934, 935, 937, 938, 939, 940, 941, 942, 943, 945, 949, 950, 952, 956, 960, 962, 964, 965, 966, 967, 969, 970, 972, 973, 975, 976, 977, 978, 979, 980, 981, 982, 985, 988, 989, 990, 994, 996, 997, 999, 1001, 1002, 1003, 1005, 1013, 1015, 1017, 1018, 1019, 1020, 1023, 1024, 1025, 1026, 1027, 1028, 1031, 1032, 1033, 1035, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1049, 1051, 1053, 1054, 1055, 1056, 1057, 1058, 1061, 1062, 1063, 1064, 1065, 1066, 1069, 1070, 1071, 1074, 1075, 1076, 1077, 1078, 1081, 1083, 1084, 1085, 1086, 1087, 1089, 1090, 1091, 1092, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1104, 1105, 1106, 1107, 1111, 1112, 1113, 1118, 1119, 1120, 1123, 1126, 1130, 1132, 1136, 1137, 1138, 1139, 1140, 1143, 1148, 1151, 1152, 1153, 1154, 1155, 1157, 1158, 1159, 1161, 1162, 1164, 1166, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1178, 1179, 1180, 1181, 1182, 1183, 1184, 1186, 1188, 1189, 1190, 1191, 1193, 1194, 1195, 1196, 1197, 1198, 1199, 1201, 1206, 1207, 1208, 1209, 1210, 1211, 1212, 1213, 1215, 1216, 1217, 1218, 1219, 1220, 1222, 1224, 1225, 1226, 1227, 1228, 1231, 1232, 1233, 1238, 1239, 1240, 1242, 1243, 1244, 1245, 1246, 1249, 1250, 1251, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1262, 1265, 1269, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1283, 1284, 1285, 1286, 1287, 1288, 1291, 1294, 1295, 1296, 1298, 1299, 1305, 1306, 1307, 1308, 1312, 1313, 1315, 1316, 1317, 1320, 1322, 1323, 1325, 1326, 1327, 1328, 1330, 1333, 1335, 1336, 1338, 1339, 1340, 1342, 1344, 1345, 1348, 1349, 1350, 1352, 1353, 1354, 1356, 1357, 1359, 1361, 1362, 1363, 1364, 1365, 1366, 1367, 1369, 1370, 1374, 1375, 1376, 1377, 1378, 1380, 1381, 1382, 1383, 1385, 1386, 1387, 1388, 1389, 1390, 1391, 1392, 1395, 1396, 1397, 1398, 1399, 1400, 1401, 1402, 1403, 1405, 1406, 1412, 1413, 1414, 1415, 1416, 1417, 1418, 1419, 1422, 1423, 1424, 1425, 1428, 1430, 1432, 1433, 1434, 1435, 1439, 1443, 1446, 1447, 1449, 1450, 1451, 1454, 1456, 1458, 1459, 1461, 1463, 1464, 1467, 1468, 1469, 1470, 1472, 1477, 1478, 1480, 1484, 1485, 1486, 1489, 1490, 1491, 1492, 1494, 1495, 1497, 1498, 1499, 1501, 1503, 1504, 1507, 1508, 1510, 1511, 1515, 1517, 1519, 1520, 1521, 1522, 1523, 1526, 1527, 1529, 1531, 1532, 1533, 1534, 1535, 1538, 1539, 1541, 1543, 1545, 1547, 1548, 1551, 1552, 1553, 1554, 1556, 1558, 1561, 1562, 1564, 1565, 1566, 1567, 1568, 1570, 1571, 1572, 1573, 1574, 1575, 1576, 1578, 1581, 1582, 1583, 1584, 1587, 1588, 1589, 1590, 1591, 1593, 1595, 1597, 1598, 1600, 1602, 1603, 1604, 1605, 1606, 1608, 1609, 1610, 1611, 1612, 1615, 1617, 1618, 1621, 1622, 1623, 1625, 1628, 1629, 1631, 1632, 1634, 1636, 1637, 1639, 1640, 1641, 1642, 1647, 1648, 1649, 1651, 1652, 1655, 1656, 1657, 1660, 1661, 1662, 1663, 1664, 1666, 1667, 1668, 1669, 1670, 1672, 1674, 1675, 1676, 1677, 1678, 1679, 1681, 1682, 1683, 1684, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1699, 1700, 1701, 1703, 1704, 1708, 1712, 1713, 1714, 1716, 1720, 1723, 1725, 1726, 1727, 1728, 1735, 1738, 1740, 1741, 1743, 1745, 1746, 1748, 1750, 1751, 1753, 1754, 1756, 1757, 1758, 1760, 1761, 1762, 1763, 1767, 1769, 1770, 1773, 1778, 1779, 1783, 1785, 1786, 1787, 1790, 1793, 1794, 1795, 1796, 1797, 1798, 1801, 1803, 1804, 1806, 1807, 1810, 1811, 1813, 1815, 1816, 1817, 1818, 1821, 1824, 1825, 1826, 1827, 1828, 1831, 1832, 1833, 1834, 1835, 1836, 1837, 1838, 1841, 1842, 1844, 1845, 1847, 1848, 1849, 1850, 1851, 1853, 1855, 1856, 1857, 1858, 1859, 1861, 1862, 1863, 1864, 1865, 1866, 1868, 1870, 1871, 1872, 1874, 1875, 1876, 1878, 1879, 1880, 1882, 1883, 1885, 1887, 1888, 1889, 1891, 1896, 1897, 1900, 1901, 1904, 1906, 1907, 1908, 1911, 1912, 1913, 1914, 1916, 1921, 1922, 1923, 1926, 1927, 1930, 1931, 1932, 1933, 1935, 1936, 1937, 1938, 1939, 1940, 1942, 1943, 1944, 1945, 1946, 1947, 1950, 1952, 1955, 1959, 1960, 1961, 1962, 1964, 1965, 1967, 1970, 1971, 1972, 1973, 1974, 1976, 1977, 1978, 1979, 1980, 1981, 1983, 1985, 1986, 1987, 1988, 1989, 1993, 1994, 1995, 1996, 1998, 2000, 2001, 2002, 2004, 2005, 2006, 2010, 2011, 2012, 2014, 2015, 2016, 2017, 2019, 2022, 2023, 2025, 2027, 2028, 2029, 2034, 2037, 2038, 2039, 2040, 2042, 2043, 2044, 2046, 2047, 2052, 2053, 2054, 2055, 2056, 2059, 2060, 2062, 2064, 2068, 2069, 2070, 2071, 2072, 2073, 2074, 2077, 2079, 2082, 2089, 2092, 2095, 2096, 2099, 2101, 2102, 2104, 2105, 2106, 2108, 2109, 2110, 2112, 2113, 2117, 2118, 2120, 2121, 2122, 2123, 2124, 2125, 2127, 2131, 2132, 2137, 2138, 2139, 2142, 2146, 2147, 2148, 2149, 2151, 2152, 2154, 2155, 2157, 2159, 2160, 2161, 2163, 2164, 2165, 2166, 2167, 2168, 2169, 2171, 2172, 2173, 2174, 2179, 2180, 2181, 2183, 2186, 2187, 2191, 2192, 2193, 2194, 2195, 2197, 2199, 2201, 2206, 2208, 2209, 2210, 2211, 2212, 2214, 2218, 2222, 2223, 2224, 2225, 2227, 2228, 2229, 2232, 2240, 2241, 2245, 2246, 2247, 2250, 2251, 2252, 2253, 2255, 2257, 2258, 2259, 2260, 2261, 2263, 2264, 2266, 2268, 2271, 2273, 2275, 2276, 2277, 2278, 2279, 2281, 2282, 2285, 2286, 2288, 2289, 2290, 2292, 2294, 2295, 2296, 2297, 2301, 2302, 2305, 2306, 2307, 2309, 2310, 2312, 2316, 2318, 2319, 2320, 2321, 2322, 2323, 2326, 2327, 2328, 2329, 2330, 2333, 2335, 2336, 2339, 2340, 2341, 2342, 2344, 2345, 2346, 2347, 2348, 2349, 2350, 2351, 2353, 2354, 2355, 2357, 2359, 2360, 2362, 2363, 2364, 2365, 2366, 2367, 2369, 2370, 2374, 2375, 2378, 2380, 2382, 2386, 2390, 2392, 2393, 2394, 2395, 2397, 2398, 2399, 2401, 2402, 2403, 2404, 2405, 2407, 2408, 2409, 2410, 2412, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2425, 2426, 2427, 2429, 2431, 2434, 2435, 2436, 2438, 2439, 2440, 2441, 2442, 2443, 2444, 2446, 2447, 2448, 2449, 2450, 2451, 2452, 2453, 2454, 2455, 2456, 2458, 2459, 2460, 2461, 2462, 2463, 2465, 2469, 2470, 2471, 2473, 2474, 2475, 2477, 2478, 2480, 2481, 2482, 2484, 2485, 2487, 2489, 2490, 2491, 2493, 2496, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2505, 2507, 2509, 2510, 2511, 2512, 2513, 2517, 2519, 2520, 2521, 2523, 2524, 2525, 2527, 2529, 2530, 2531, 2532, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2545, 2548, 2549, 2551, 2553, 2554, 2555, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2571, 2573, 2575, 2577, 2579, 2580, 2582, 2583, 2585, 2586, 2588, 2591, 2594, 2595, 2596, 2597, 2598, 2600, 2601, 2602, 2603, 2604, 2605, 2607, 2608, 2611, 2613, 2615, 2616, 2617, 2621, 2622, 2624, 2625, 2627, 2628, 2630, 2631, 2635, 2638, 2640, 2642, 2644, 2645, 2646, 2647, 2648, 2651, 2652, 2653, 2655, 2656, 2657, 2658, 2659, 2662, 2665, 2666, 2668, 2669, 2670, 2672, 2673, 2674, 2676, 2678, 2679, 2680, 2681, 2683, 2684, 2685, 2686, 2690, 2691, 2692, 2694, 2697, 2701, 2704, 2705, 2707]\n",
            "[3, 5, 6, 7, 8, 9, 10, 11, 12, 14, 15, 19, 20, 21, 24, 25, 26, 27, 28, 31, 33, 34, 35, 36, 37, 40, 41, 42, 43, 45, 46, 48, 49, 50, 51, 52, 54, 55, 57, 58, 59, 60, 62, 63, 64, 66, 67, 72, 78, 79, 80, 81, 84, 85, 87, 89, 92, 93, 94, 95, 96, 97, 98, 99, 100, 101, 105, 106, 107, 108, 109, 110, 111, 113, 114, 115, 116, 117, 118, 119, 120, 122, 124, 126, 127, 128, 129, 130, 131, 134, 135, 136, 137, 138, 139, 143, 145, 149, 150, 152, 154, 156, 158, 159, 160, 161, 163, 169, 173, 174, 176, 177, 178, 179, 180, 185, 187, 188, 190, 191, 192, 194, 196, 197, 198, 199, 201, 203, 204, 205, 206, 207, 211, 212, 213, 214, 215, 218, 220, 223, 227, 229, 231, 232, 233, 234, 239, 240, 244, 245, 246, 247, 249, 250, 252, 254, 255, 256, 258, 259, 260, 261, 263, 265, 267, 268, 269, 270, 271, 273, 274, 275, 276, 277, 278, 281, 283, 284, 286, 287, 288, 292, 293, 296, 297, 299, 300, 301, 302, 306, 308, 310, 311, 312, 314, 316, 317, 319, 321, 322, 323, 324, 327, 328, 329, 330, 335, 337, 338, 339, 340, 341, 342, 344, 345, 346, 348, 350, 351, 352, 353, 354, 355, 360, 361, 362, 363, 364, 365, 366, 368, 370, 372, 375, 376, 377, 378, 380, 386, 388, 389, 390, 391, 395, 398, 399, 400, 401, 402, 403, 405, 406, 407, 408, 411, 412, 413, 414, 417, 418, 419, 421, 422, 423, 424, 425, 427, 428, 430, 431, 432, 433, 434, 435, 436, 437, 439, 441, 442, 443, 445, 446, 447, 449, 450, 451, 452, 453, 454, 455, 456, 457, 458, 463, 465, 467, 468, 470, 471, 473, 476, 481, 483, 484, 485, 486, 488, 490, 492, 495, 496, 497, 499, 500, 503, 505, 506, 508, 509, 512, 515, 516, 518, 519, 520, 521, 522, 525, 527, 528, 532, 533, 534, 535, 538, 540, 542, 543, 544, 545, 547, 551, 552, 553, 554, 557, 558, 559, 560, 563, 564, 565, 566, 567, 568, 570, 575, 576, 579, 580, 581, 583, 584, 587, 588, 589, 591, 592, 594, 595, 596, 598, 599, 602, 603, 606, 607, 610, 613, 614, 615, 616, 618, 621, 622, 623, 624, 625, 627, 628, 630, 635, 636, 637, 641, 643, 644, 645, 646, 647, 648, 649, 652, 654, 655, 657, 658, 659, 662, 663, 664, 666, 667, 668, 669, 673, 674, 677, 679, 680, 682, 684, 685, 687, 692, 693, 694, 697, 699, 700, 701, 703, 704, 706, 707, 708, 709, 711, 712, 713, 715, 717, 718, 719, 722, 723, 724, 725, 728, 729, 730, 735, 738, 739, 741, 743, 747, 751, 754, 755, 756, 757, 758, 759, 761, 764, 765, 766, 767, 768, 769, 770, 773, 774, 775, 776, 779, 781, 783, 784, 785, 787, 788, 790, 791, 792, 794, 796, 797, 798, 799, 802, 804, 805, 806, 808, 809, 811, 813, 816, 818, 820, 821, 823, 826, 829, 830, 831, 833, 838, 841, 843, 844, 845, 846, 847, 848, 849, 850, 851, 853, 854, 855, 857, 858, 859, 862, 863, 865, 868, 869, 870, 871, 872, 873, 875, 876, 879, 881, 883, 885, 886, 889, 890, 892, 893, 894, 896, 898, 902, 903, 904, 905, 906, 908, 909, 910, 913, 914, 915, 916, 917, 919, 921, 922, 923, 924, 926, 927, 928, 929, 930, 932, 933, 934, 935, 936, 937, 939, 940, 943, 944, 946, 947, 949, 952, 953, 954, 956, 957, 958, 960, 961, 963, 964, 967, 969, 972, 974, 975, 977, 980, 982, 986, 987, 988, 990, 992, 993, 994, 995, 996, 997, 999, 1000, 1002, 1003, 1004, 1005, 1006, 1009, 1012, 1013, 1015, 1016, 1017, 1018, 1019, 1020, 1023, 1025, 1026, 1027, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1037, 1039, 1043, 1044, 1045, 1049, 1051, 1052, 1055, 1056, 1057, 1058, 1059, 1063, 1064, 1065, 1066, 1067, 1068, 1069, 1071, 1073, 1074, 1075, 1076, 1077, 1078, 1080, 1082, 1083, 1088, 1089, 1090, 1093, 1095, 1096, 1097, 1098, 1099, 1100, 1101, 1102, 1103, 1105, 1106, 1107, 1110, 1111, 1112, 1114, 1115, 1116, 1117, 1120, 1126, 1129, 1130, 1133, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1145, 1146, 1148, 1151, 1152, 1154, 1155, 1156, 1157, 1159, 1160, 1162, 1163, 1164, 1165, 1166, 1169, 1172, 1173, 1174, 1177, 1179, 1180, 1181, 1182, 1185, 1186, 1187, 1189, 1190, 1192, 1194, 1196, 1198, 1199, 1200, 1202, 1203, 1204, 1206, 1207, 1209, 1210, 1212, 1213, 1214, 1217, 1218, 1219, 1221, 1223, 1224, 1227, 1228, 1230, 1231, 1233, 1234, 1236, 1238, 1241, 1243, 1244, 1245, 1246, 1247, 1250, 1251, 1252, 1255, 1257, 1259, 1264, 1270, 1273, 1274, 1275, 1278, 1280, 1281, 1282, 1283, 1284, 1285, 1286, 1287, 1288, 1290, 1291, 1294, 1295, 1296, 1297, 1299, 1304, 1306, 1309, 1315, 1316, 1317, 1319, 1320, 1322, 1323, 1324, 1325, 1326, 1328, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1343, 1344, 1346, 1348, 1350, 1351, 1352, 1353, 1356, 1357, 1358, 1359, 1360, 1363, 1364, 1365, 1367, 1368, 1369, 1370, 1371, 1373, 1374, 1377, 1378, 1381, 1383, 1385, 1388, 1389, 1391, 1392, 1393, 1394, 1395, 1398, 1399, 1401, 1402, 1403, 1406, 1407, 1408, 1409, 1411, 1412, 1414, 1417, 1418, 1419, 1420, 1421, 1423, 1424, 1426, 1427, 1430, 1431, 1433, 1434, 1435, 1436, 1438, 1439, 1440, 1441, 1442, 1446, 1447, 1448, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1457, 1458, 1463, 1464, 1466, 1467, 1470, 1471, 1472, 1474, 1477, 1479, 1480, 1481, 1482, 1483, 1484, 1485, 1486, 1488, 1489, 1494, 1495, 1496, 1497, 1500, 1501, 1502, 1503, 1504, 1505, 1508, 1510, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1519, 1520, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1529, 1530, 1531, 1532, 1533, 1537, 1538, 1539, 1540, 1541, 1542, 1544, 1547, 1551, 1552, 1553, 1554, 1555, 1556, 1557, 1562, 1564, 1566, 1568, 1570, 1571, 1573, 1574, 1578, 1579, 1584, 1586, 1587, 1589, 1590, 1591, 1593, 1595, 1596, 1597, 1598, 1600, 1602, 1603, 1605, 1607, 1609, 1610, 1611, 1612, 1614, 1617, 1618, 1620, 1622, 1623, 1624, 1626, 1628, 1633, 1634, 1636, 1638, 1641, 1642, 1645, 1646, 1647, 1648, 1649, 1655, 1656, 1659, 1661, 1662, 1663, 1664, 1665, 1666, 1667, 1669, 1670, 1671, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1683, 1684, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1695, 1697, 1698, 1699, 1700, 1702, 1704, 1705, 1709, 1710, 1715, 1717, 1719, 1720, 1722, 1723, 1724, 1725, 1726, 1727, 1728, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1739, 1740, 1741, 1742, 1743, 1746, 1747, 1748, 1750, 1751, 1752, 1753, 1754, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1767, 1768, 1770, 1772, 1773, 1775, 1776, 1778, 1779, 1780, 1781, 1782, 1784, 1785, 1788, 1789, 1791, 1792, 1793, 1796, 1797, 1798, 1800, 1803, 1805, 1806, 1808, 1809, 1810, 1812, 1814, 1815, 1817, 1818, 1822, 1823, 1824, 1825, 1826, 1827, 1828, 1829, 1832, 1833, 1834, 1835, 1836, 1838, 1839, 1840, 1843, 1844, 1848, 1849, 1850, 1851, 1852, 1853, 1854, 1855, 1856, 1857, 1858, 1859, 1860, 1863, 1864, 1865, 1866, 1868, 1869, 1870, 1871, 1872, 1873, 1874, 1875, 1878, 1881, 1883, 1884, 1885, 1886, 1887, 1892, 1894, 1895, 1896, 1899, 1900, 1902, 1905, 1907, 1909, 1910, 1912, 1913, 1914, 1916, 1917, 1918, 1919, 1921, 1922, 1923, 1924, 1925, 1926, 1928, 1929, 1930, 1931, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1939, 1941, 1943, 1945, 1947, 1948, 1949, 1951, 1952, 1954, 1956, 1958, 1959, 1964, 1965, 1967, 1968, 1969, 1970, 1973, 1974, 1975, 1976, 1977, 1978, 1980, 1981, 1986, 1990, 1991, 1992, 1994, 1996, 2000, 2001, 2002, 2004, 2005, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2027, 2028, 2030, 2031, 2033, 2035, 2037, 2038, 2039, 2040, 2041, 2042, 2043, 2044, 2046, 2047, 2052, 2053, 2054, 2056, 2058, 2059, 2061, 2064, 2065, 2066, 2067, 2068, 2069, 2075, 2076, 2079, 2081, 2082, 2083, 2085, 2086, 2090, 2092, 2093, 2094, 2098, 2099, 2101, 2103, 2104, 2105, 2107, 2108, 2110, 2111, 2112, 2113, 2115, 2116, 2118, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2130, 2131, 2132, 2134, 2135, 2137, 2139, 2141, 2143, 2145, 2147, 2148, 2150, 2152, 2155, 2156, 2157, 2158, 2160, 2163, 2164, 2166, 2168, 2170, 2171, 2177, 2178, 2179, 2180, 2181, 2182, 2184, 2185, 2186, 2187, 2190, 2191, 2192, 2193, 2194, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2203, 2205, 2209, 2211, 2213, 2216, 2217, 2218, 2220, 2222, 2227, 2228, 2229, 2230, 2231, 2236, 2238, 2239, 2241, 2244, 2246, 2248, 2249, 2251, 2252, 2253, 2254, 2256, 2257, 2258, 2259, 2262, 2263, 2264, 2265, 2266, 2267, 2269, 2271, 2274, 2275, 2276, 2277, 2278, 2280, 2281, 2283, 2285, 2286, 2287, 2288, 2289, 2290, 2295, 2296, 2301, 2302, 2304, 2309, 2311, 2312, 2313, 2314, 2316, 2318, 2319, 2320, 2321, 2322, 2323, 2325, 2326, 2329, 2330, 2331, 2333, 2334, 2335, 2337, 2339, 2342, 2343, 2344, 2346, 2347, 2348, 2351, 2352, 2354, 2355, 2356, 2357, 2358, 2360, 2361, 2362, 2363, 2364, 2365, 2368, 2369, 2370, 2372, 2375, 2376, 2377, 2378, 2379, 2380, 2381, 2383, 2386, 2387, 2391, 2392, 2396, 2400, 2401, 2402, 2403, 2404, 2405, 2406, 2408, 2415, 2416, 2417, 2420, 2423, 2424, 2425, 2427, 2428, 2429, 2430, 2431, 2433, 2437, 2439, 2440, 2442, 2443, 2444, 2445, 2446, 2448, 2452, 2453, 2457, 2458, 2459, 2462, 2464, 2465, 2466, 2467, 2471, 2472, 2476, 2477, 2478, 2479, 2480, 2481, 2483, 2485, 2487, 2488, 2489, 2491, 2493, 2494, 2495, 2496, 2498, 2499, 2500, 2502, 2503, 2505, 2507, 2508, 2509, 2513, 2516, 2517, 2519, 2520, 2521, 2522, 2523, 2524, 2525, 2529, 2531, 2532, 2533, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2542, 2543, 2545, 2547, 2548, 2549, 2552, 2556, 2559, 2561, 2562, 2563, 2564, 2565, 2567, 2572, 2574, 2575, 2576, 2577, 2578, 2581, 2582, 2583, 2587, 2588, 2589, 2591, 2593, 2594, 2595, 2596, 2597, 2598, 2599, 2601, 2603, 2604, 2606, 2607, 2608, 2609, 2610, 2611, 2612, 2613, 2614, 2616, 2618, 2619, 2621, 2622, 2623, 2624, 2625, 2626, 2628, 2629, 2630, 2631, 2632, 2633, 2635, 2637, 2640, 2643, 2644, 2645, 2646, 2647, 2649, 2650, 2652, 2653, 2654, 2656, 2657, 2658, 2659, 2661, 2662, 2663, 2664, 2665, 2667, 2668, 2671, 2674, 2675, 2676, 2677, 2678, 2680, 2681, 2682, 2683, 2684, 2686, 2687, 2688, 2689, 2690, 2691, 2692, 2694, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2703, 2706, 2707]\n",
            "[0, 2, 5, 6, 7, 8, 9, 10, 12, 14, 15, 17, 22, 24, 26, 28, 29, 32, 33, 35, 40, 42, 43, 44, 48, 50, 51, 52, 54, 55, 56, 59, 60, 61, 63, 67, 69, 70, 71, 76, 77, 80, 82, 84, 86, 87, 88, 93, 94, 95, 96, 97, 98, 99, 102, 104, 105, 106, 107, 109, 110, 112, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125, 126, 127, 128, 130, 134, 135, 138, 139, 140, 141, 145, 147, 151, 152, 153, 155, 157, 158, 160, 161, 162, 164, 165, 166, 167, 168, 170, 172, 173, 174, 176, 179, 180, 181, 182, 185, 187, 189, 190, 191, 193, 195, 197, 198, 199, 202, 203, 204, 205, 206, 209, 210, 211, 212, 214, 215, 216, 217, 218, 221, 222, 225, 227, 232, 233, 234, 235, 236, 239, 241, 242, 243, 244, 246, 247, 255, 256, 257, 259, 260, 262, 263, 265, 267, 269, 271, 272, 273, 274, 275, 276, 277, 278, 279, 280, 282, 283, 284, 293, 294, 297, 299, 301, 302, 303, 304, 305, 306, 311, 313, 315, 318, 320, 321, 323, 324, 329, 330, 332, 335, 336, 338, 339, 340, 341, 342, 344, 345, 346, 347, 348, 349, 350, 351, 353, 355, 356, 357, 360, 361, 364, 365, 366, 367, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 381, 383, 384, 385, 386, 387, 388, 390, 392, 393, 395, 397, 401, 402, 403, 404, 405, 408, 409, 411, 412, 416, 417, 419, 421, 422, 424, 426, 428, 430, 431, 432, 434, 435, 438, 439, 441, 444, 445, 446, 448, 449, 450, 452, 453, 455, 456, 457, 458, 459, 460, 462, 465, 466, 469, 472, 473, 475, 476, 477, 479, 481, 482, 484, 485, 486, 487, 488, 489, 490, 492, 494, 495, 497, 498, 499, 501, 502, 503, 504, 505, 506, 507, 508, 509, 510, 512, 513, 514, 515, 517, 520, 521, 524, 525, 526, 528, 529, 531, 532, 536, 537, 539, 540, 541, 542, 543, 544, 545, 546, 547, 548, 549, 550, 551, 552, 553, 556, 560, 561, 562, 563, 565, 566, 567, 568, 570, 573, 574, 575, 576, 577, 579, 581, 582, 583, 584, 585, 586, 587, 589, 590, 591, 592, 596, 598, 600, 603, 604, 605, 606, 607, 608, 609, 611, 613, 615, 616, 617, 619, 621, 625, 626, 628, 630, 631, 633, 634, 635, 636, 637, 638, 640, 642, 643, 644, 647, 651, 652, 653, 654, 655, 656, 657, 658, 659, 660, 662, 663, 664, 665, 671, 673, 675, 678, 679, 680, 681, 682, 686, 687, 688, 689, 691, 692, 695, 698, 699, 701, 703, 704, 705, 707, 711, 712, 714, 715, 716, 717, 719, 720, 723, 726, 727, 729, 732, 734, 735, 736, 738, 740, 742, 743, 745, 746, 747, 750, 751, 752, 755, 756, 757, 758, 759, 760, 761, 762, 765, 767, 772, 773, 775, 776, 780, 783, 784, 785, 789, 790, 791, 792, 793, 794, 795, 796, 799, 800, 801, 802, 804, 806, 808, 809, 810, 813, 814, 815, 817, 818, 819, 820, 821, 822, 824, 826, 831, 832, 833, 836, 837, 841, 843, 844, 846, 847, 848, 849, 852, 853, 855, 857, 858, 859, 860, 862, 863, 864, 866, 868, 869, 873, 874, 875, 876, 877, 878, 879, 880, 881, 884, 887, 889, 891, 892, 894, 897, 899, 901, 903, 904, 905, 908, 909, 910, 911, 912, 913, 914, 916, 917, 918, 919, 920, 922, 924, 926, 928, 929, 932, 935, 936, 937, 938, 941, 942, 943, 944, 945, 947, 948, 949, 950, 951, 952, 956, 957, 958, 959, 960, 961, 962, 964, 965, 967, 968, 969, 971, 974, 975, 976, 977, 978, 980, 983, 984, 986, 988, 990, 994, 996, 997, 1000, 1002, 1004, 1005, 1006, 1007, 1008, 1009, 1010, 1013, 1014, 1016, 1017, 1018, 1019, 1020, 1022, 1025, 1027, 1028, 1030, 1032, 1033, 1036, 1037, 1038, 1040, 1041, 1043, 1044, 1046, 1047, 1048, 1050, 1051, 1052, 1053, 1054, 1055, 1057, 1060, 1061, 1062, 1063, 1065, 1068, 1069, 1070, 1071, 1073, 1074, 1075, 1076, 1078, 1080, 1083, 1085, 1087, 1089, 1090, 1091, 1092, 1095, 1097, 1098, 1100, 1102, 1103, 1104, 1107, 1108, 1109, 1110, 1111, 1112, 1116, 1118, 1120, 1122, 1123, 1124, 1125, 1127, 1128, 1130, 1134, 1137, 1139, 1141, 1142, 1144, 1146, 1147, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1160, 1161, 1163, 1165, 1166, 1167, 1168, 1170, 1171, 1172, 1173, 1174, 1175, 1176, 1177, 1178, 1179, 1180, 1181, 1182, 1184, 1186, 1187, 1188, 1189, 1192, 1193, 1194, 1195, 1196, 1197, 1198, 1200, 1202, 1204, 1207, 1208, 1209, 1210, 1211, 1214, 1217, 1218, 1220, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1231, 1235, 1236, 1238, 1240, 1241, 1242, 1244, 1245, 1247, 1248, 1249, 1250, 1251, 1253, 1255, 1256, 1257, 1261, 1267, 1268, 1271, 1272, 1276, 1279, 1280, 1281, 1282, 1283, 1284, 1287, 1288, 1294, 1295, 1296, 1298, 1299, 1302, 1303, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1316, 1318, 1319, 1321, 1323, 1324, 1325, 1326, 1329, 1332, 1333, 1335, 1339, 1342, 1343, 1345, 1347, 1348, 1349, 1350, 1351, 1353, 1356, 1358, 1360, 1361, 1363, 1364, 1368, 1369, 1370, 1371, 1372, 1373, 1376, 1377, 1378, 1379, 1380, 1381, 1382, 1386, 1389, 1391, 1392, 1394, 1397, 1398, 1400, 1402, 1406, 1409, 1411, 1412, 1413, 1415, 1416, 1418, 1419, 1420, 1423, 1424, 1425, 1427, 1428, 1430, 1431, 1432, 1434, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1445, 1446, 1449, 1450, 1453, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1462, 1463, 1464, 1465, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1474, 1475, 1476, 1477, 1479, 1480, 1481, 1484, 1486, 1488, 1490, 1491, 1492, 1494, 1496, 1497, 1498, 1502, 1505, 1506, 1507, 1509, 1510, 1511, 1512, 1516, 1520, 1521, 1522, 1523, 1525, 1526, 1528, 1529, 1530, 1533, 1534, 1535, 1537, 1538, 1539, 1540, 1541, 1542, 1544, 1546, 1547, 1548, 1549, 1550, 1552, 1553, 1554, 1557, 1558, 1560, 1561, 1562, 1564, 1566, 1569, 1570, 1571, 1573, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1583, 1584, 1585, 1587, 1589, 1590, 1591, 1592, 1595, 1598, 1599, 1600, 1605, 1606, 1607, 1609, 1611, 1612, 1613, 1614, 1615, 1618, 1620, 1621, 1622, 1624, 1625, 1626, 1627, 1630, 1631, 1632, 1633, 1634, 1637, 1639, 1640, 1641, 1642, 1644, 1645, 1647, 1652, 1653, 1655, 1656, 1657, 1658, 1659, 1661, 1662, 1663, 1664, 1666, 1669, 1671, 1672, 1675, 1676, 1678, 1679, 1682, 1690, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1701, 1704, 1705, 1706, 1707, 1708, 1709, 1711, 1712, 1714, 1715, 1716, 1717, 1728, 1729, 1730, 1731, 1732, 1737, 1738, 1741, 1744, 1745, 1747, 1748, 1749, 1751, 1752, 1755, 1756, 1758, 1759, 1760, 1761, 1762, 1763, 1766, 1771, 1773, 1774, 1775, 1779, 1782, 1783, 1785, 1787, 1788, 1790, 1791, 1794, 1795, 1796, 1797, 1799, 1801, 1804, 1805, 1806, 1808, 1809, 1810, 1811, 1812, 1815, 1816, 1817, 1819, 1820, 1821, 1822, 1823, 1825, 1826, 1827, 1828, 1829, 1830, 1831, 1832, 1835, 1837, 1838, 1839, 1840, 1841, 1842, 1843, 1846, 1849, 1850, 1851, 1852, 1853, 1855, 1856, 1858, 1859, 1861, 1862, 1863, 1868, 1869, 1871, 1872, 1874, 1875, 1876, 1877, 1880, 1881, 1883, 1884, 1885, 1887, 1888, 1889, 1890, 1891, 1892, 1894, 1895, 1899, 1900, 1901, 1902, 1904, 1905, 1906, 1909, 1912, 1913, 1914, 1916, 1917, 1919, 1921, 1922, 1924, 1925, 1926, 1927, 1929, 1930, 1931, 1933, 1934, 1937, 1939, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1948, 1949, 1950, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1960, 1961, 1962, 1963, 1964, 1966, 1968, 1969, 1970, 1971, 1972, 1973, 1974, 1979, 1980, 1983, 1986, 1987, 1988, 1990, 1991, 1992, 1995, 1996, 1997, 1998, 2001, 2002, 2005, 2007, 2008, 2009, 2010, 2011, 2012, 2014, 2015, 2020, 2022, 2024, 2025, 2028, 2030, 2031, 2032, 2034, 2035, 2036, 2037, 2040, 2042, 2043, 2044, 2045, 2046, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2064, 2067, 2069, 2070, 2071, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2085, 2086, 2088, 2089, 2091, 2094, 2095, 2096, 2097, 2101, 2103, 2104, 2105, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2116, 2118, 2119, 2120, 2121, 2123, 2124, 2125, 2127, 2128, 2129, 2132, 2133, 2136, 2137, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2147, 2148, 2149, 2151, 2152, 2153, 2154, 2155, 2161, 2162, 2163, 2164, 2165, 2166, 2168, 2170, 2171, 2172, 2173, 2176, 2177, 2178, 2179, 2181, 2184, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2196, 2197, 2198, 2201, 2202, 2203, 2205, 2206, 2207, 2208, 2210, 2211, 2214, 2215, 2217, 2218, 2219, 2220, 2221, 2222, 2223, 2225, 2227, 2228, 2229, 2230, 2231, 2233, 2234, 2235, 2237, 2238, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2253, 2254, 2255, 2256, 2259, 2261, 2262, 2263, 2265, 2267, 2268, 2269, 2270, 2271, 2273, 2275, 2277, 2280, 2281, 2282, 2283, 2284, 2285, 2287, 2288, 2289, 2290, 2291, 2292, 2293, 2294, 2295, 2296, 2297, 2298, 2300, 2301, 2305, 2310, 2311, 2312, 2313, 2314, 2315, 2317, 2318, 2319, 2321, 2322, 2324, 2328, 2329, 2330, 2331, 2334, 2337, 2339, 2341, 2343, 2344, 2345, 2346, 2347, 2348, 2350, 2351, 2354, 2356, 2359, 2360, 2361, 2362, 2363, 2364, 2366, 2367, 2368, 2369, 2370, 2371, 2373, 2374, 2375, 2376, 2377, 2378, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2389, 2390, 2392, 2393, 2394, 2395, 2396, 2397, 2399, 2400, 2401, 2403, 2405, 2406, 2409, 2412, 2413, 2417, 2419, 2420, 2421, 2422, 2423, 2425, 2430, 2431, 2432, 2433, 2434, 2435, 2436, 2437, 2441, 2446, 2447, 2452, 2453, 2455, 2456, 2458, 2459, 2460, 2461, 2462, 2464, 2465, 2467, 2468, 2469, 2472, 2473, 2474, 2478, 2480, 2481, 2482, 2483, 2484, 2485, 2486, 2487, 2490, 2491, 2492, 2493, 2494, 2495, 2498, 2499, 2500, 2503, 2504, 2507, 2508, 2509, 2512, 2515, 2516, 2517, 2520, 2522, 2523, 2526, 2527, 2529, 2530, 2533, 2534, 2535, 2537, 2538, 2539, 2540, 2541, 2543, 2544, 2545, 2546, 2548, 2549, 2551, 2553, 2555, 2557, 2558, 2561, 2562, 2566, 2567, 2569, 2571, 2574, 2575, 2577, 2579, 2581, 2582, 2583, 2584, 2585, 2586, 2589, 2591, 2593, 2595, 2598, 2602, 2605, 2608, 2609, 2614, 2617, 2618, 2619, 2620, 2621, 2622, 2623, 2625, 2628, 2632, 2635, 2638, 2639, 2640, 2641, 2642, 2644, 2646, 2647, 2651, 2652, 2653, 2654, 2655, 2657, 2658, 2659, 2660, 2661, 2663, 2664, 2666, 2667, 2668, 2670, 2671, 2672, 2673, 2675, 2677, 2679, 2680, 2681, 2683, 2684, 2685, 2686, 2690, 2691, 2693, 2695, 2696, 2697, 2698, 2699, 2701, 2703, 2706, 2707]\n",
            "[0, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 17, 20, 21, 23, 24, 26, 27, 29, 30, 32, 33, 34, 35, 36, 37, 39, 40, 41, 42, 43, 45, 46, 47, 48, 49, 51, 53, 55, 56, 57, 59, 61, 62, 63, 64, 65, 66, 67, 68, 70, 72, 74, 76, 77, 78, 80, 81, 83, 85, 87, 88, 89, 90, 92, 93, 95, 97, 98, 100, 102, 103, 104, 105, 107, 108, 111, 112, 113, 116, 117, 119, 120, 121, 124, 126, 128, 129, 131, 132, 133, 135, 136, 137, 139, 140, 141, 143, 144, 145, 147, 148, 149, 150, 151, 154, 158, 160, 161, 162, 163, 167, 170, 177, 178, 179, 182, 184, 185, 186, 187, 188, 190, 191, 192, 194, 197, 198, 199, 201, 202, 203, 204, 206, 210, 211, 212, 213, 215, 217, 220, 221, 223, 224, 228, 229, 230, 233, 235, 238, 239, 240, 243, 244, 245, 247, 248, 249, 250, 251, 252, 253, 254, 257, 258, 259, 261, 262, 264, 267, 268, 270, 272, 274, 276, 278, 279, 280, 281, 283, 285, 286, 287, 289, 292, 293, 294, 295, 297, 300, 301, 302, 303, 304, 305, 308, 309, 310, 313, 318, 319, 320, 321, 324, 325, 326, 327, 328, 329, 330, 331, 333, 334, 335, 337, 338, 339, 341, 342, 343, 345, 347, 349, 352, 353, 355, 357, 358, 360, 362, 365, 366, 369, 370, 371, 372, 373, 376, 377, 378, 380, 381, 382, 383, 385, 387, 388, 389, 390, 391, 393, 394, 395, 397, 398, 399, 400, 401, 403, 406, 408, 409, 410, 411, 412, 414, 415, 417, 420, 421, 422, 423, 424, 425, 428, 430, 431, 432, 434, 435, 436, 438, 439, 440, 441, 442, 444, 445, 448, 450, 451, 452, 453, 454, 455, 456, 457, 460, 461, 462, 463, 464, 465, 466, 467, 469, 470, 472, 475, 476, 478, 479, 480, 483, 488, 490, 492, 496, 497, 499, 501, 504, 505, 506, 507, 510, 511, 512, 514, 515, 516, 517, 519, 520, 521, 522, 526, 527, 529, 531, 532, 533, 534, 535, 536, 538, 540, 541, 542, 544, 545, 547, 549, 550, 551, 552, 553, 556, 557, 558, 560, 564, 565, 569, 572, 573, 574, 575, 578, 579, 581, 582, 583, 584, 586, 589, 591, 592, 594, 596, 597, 598, 599, 600, 601, 602, 603, 605, 607, 608, 610, 612, 613, 614, 615, 616, 617, 619, 621, 622, 623, 624, 625, 626, 627, 628, 629, 630, 631, 632, 633, 634, 635, 636, 637, 640, 641, 642, 645, 648, 651, 652, 654, 655, 656, 659, 660, 661, 663, 666, 667, 668, 669, 671, 672, 673, 675, 678, 679, 680, 682, 683, 685, 688, 691, 693, 695, 696, 697, 699, 701, 703, 705, 708, 710, 711, 714, 715, 718, 720, 726, 727, 728, 729, 730, 731, 733, 739, 740, 741, 742, 744, 746, 748, 750, 754, 756, 761, 762, 763, 765, 766, 767, 768, 769, 770, 771, 772, 774, 775, 777, 780, 781, 782, 783, 784, 785, 787, 788, 790, 792, 793, 794, 795, 796, 797, 800, 802, 805, 806, 807, 808, 809, 810, 812, 814, 815, 816, 819, 820, 822, 823, 826, 827, 828, 829, 830, 831, 833, 835, 837, 838, 839, 842, 843, 844, 847, 849, 852, 853, 857, 859, 860, 861, 863, 865, 866, 868, 869, 870, 872, 874, 875, 876, 877, 879, 880, 881, 883, 885, 886, 887, 889, 890, 893, 894, 896, 897, 898, 901, 903, 905, 906, 907, 908, 909, 910, 911, 912, 914, 915, 917, 918, 919, 920, 921, 922, 923, 925, 926, 928, 929, 931, 932, 935, 936, 937, 939, 944, 945, 946, 948, 949, 950, 952, 954, 955, 956, 957, 958, 959, 960, 961, 962, 963, 965, 966, 970, 972, 973, 974, 976, 977, 979, 982, 984, 986, 988, 989, 990, 991, 992, 993, 995, 996, 1001, 1003, 1005, 1006, 1007, 1009, 1011, 1012, 1013, 1014, 1015, 1016, 1017, 1019, 1021, 1022, 1023, 1025, 1026, 1027, 1028, 1030, 1031, 1033, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1048, 1050, 1051, 1055, 1057, 1058, 1060, 1062, 1063, 1064, 1066, 1069, 1071, 1074, 1075, 1078, 1079, 1083, 1084, 1085, 1086, 1088, 1093, 1094, 1095, 1096, 1097, 1098, 1100, 1101, 1106, 1107, 1109, 1110, 1113, 1114, 1115, 1116, 1117, 1118, 1119, 1120, 1123, 1124, 1125, 1129, 1131, 1134, 1135, 1136, 1137, 1138, 1139, 1140, 1141, 1142, 1143, 1144, 1145, 1146, 1147, 1148, 1150, 1151, 1152, 1154, 1156, 1157, 1158, 1159, 1160, 1162, 1163, 1164, 1168, 1169, 1174, 1175, 1176, 1177, 1178, 1180, 1181, 1184, 1187, 1188, 1189, 1190, 1191, 1192, 1193, 1196, 1197, 1198, 1200, 1201, 1202, 1203, 1204, 1205, 1206, 1208, 1209, 1212, 1213, 1217, 1219, 1220, 1221, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1233, 1234, 1235, 1237, 1238, 1239, 1241, 1242, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1260, 1264, 1265, 1266, 1268, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1283, 1285, 1287, 1289, 1290, 1292, 1293, 1294, 1298, 1303, 1305, 1306, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1315, 1317, 1319, 1320, 1322, 1327, 1328, 1329, 1330, 1331, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1344, 1345, 1346, 1348, 1349, 1350, 1351, 1353, 1356, 1359, 1360, 1363, 1364, 1365, 1367, 1369, 1370, 1373, 1375, 1376, 1380, 1381, 1383, 1384, 1385, 1386, 1387, 1391, 1392, 1393, 1394, 1395, 1399, 1400, 1401, 1404, 1405, 1406, 1411, 1412, 1413, 1414, 1415, 1418, 1420, 1421, 1422, 1425, 1427, 1429, 1430, 1432, 1434, 1435, 1438, 1439, 1441, 1443, 1445, 1448, 1449, 1453, 1455, 1456, 1457, 1460, 1461, 1464, 1465, 1466, 1468, 1471, 1472, 1474, 1475, 1478, 1483, 1486, 1487, 1488, 1490, 1491, 1492, 1493, 1494, 1495, 1496, 1497, 1498, 1499, 1502, 1503, 1504, 1506, 1509, 1510, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1519, 1520, 1522, 1525, 1527, 1528, 1529, 1531, 1532, 1533, 1534, 1535, 1537, 1538, 1541, 1543, 1545, 1546, 1547, 1548, 1551, 1554, 1555, 1557, 1559, 1560, 1561, 1562, 1563, 1564, 1566, 1567, 1568, 1569, 1570, 1571, 1572, 1573, 1575, 1576, 1578, 1579, 1580, 1581, 1584, 1586, 1589, 1590, 1591, 1592, 1599, 1601, 1602, 1604, 1607, 1611, 1612, 1613, 1615, 1616, 1617, 1619, 1620, 1625, 1628, 1629, 1631, 1632, 1633, 1634, 1635, 1636, 1638, 1639, 1640, 1642, 1643, 1645, 1646, 1648, 1654, 1655, 1656, 1659, 1662, 1663, 1665, 1668, 1669, 1670, 1672, 1673, 1674, 1676, 1677, 1679, 1680, 1683, 1684, 1685, 1686, 1688, 1689, 1692, 1694, 1695, 1703, 1704, 1705, 1706, 1707, 1708, 1710, 1711, 1712, 1716, 1719, 1724, 1726, 1729, 1730, 1733, 1734, 1735, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1743, 1744, 1746, 1747, 1748, 1751, 1752, 1754, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1766, 1768, 1769, 1770, 1774, 1775, 1777, 1781, 1782, 1785, 1787, 1788, 1791, 1792, 1793, 1794, 1795, 1797, 1798, 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1808, 1811, 1813, 1818, 1819, 1820, 1821, 1824, 1825, 1829, 1831, 1832, 1833, 1834, 1839, 1840, 1841, 1842, 1843, 1844, 1845, 1847, 1848, 1850, 1851, 1852, 1853, 1855, 1858, 1860, 1862, 1866, 1871, 1872, 1873, 1874, 1875, 1876, 1883, 1884, 1886, 1887, 1888, 1890, 1891, 1893, 1894, 1895, 1897, 1898, 1900, 1901, 1902, 1903, 1906, 1907, 1908, 1910, 1911, 1912, 1913, 1914, 1915, 1917, 1918, 1919, 1920, 1921, 1922, 1924, 1926, 1927, 1928, 1929, 1935, 1939, 1940, 1941, 1943, 1945, 1947, 1948, 1949, 1950, 1951, 1952, 1953, 1954, 1955, 1956, 1958, 1959, 1961, 1962, 1964, 1968, 1969, 1970, 1971, 1973, 1975, 1976, 1979, 1982, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1994, 1997, 1999, 2000, 2002, 2003, 2006, 2009, 2011, 2013, 2014, 2016, 2017, 2018, 2020, 2021, 2023, 2027, 2029, 2030, 2031, 2032, 2034, 2035, 2036, 2039, 2040, 2041, 2042, 2043, 2044, 2045, 2046, 2047, 2049, 2050, 2051, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2065, 2067, 2068, 2069, 2070, 2071, 2072, 2073, 2075, 2076, 2077, 2078, 2080, 2082, 2083, 2087, 2088, 2090, 2091, 2092, 2094, 2095, 2097, 2098, 2100, 2102, 2104, 2106, 2107, 2108, 2109, 2110, 2111, 2112, 2113, 2114, 2115, 2116, 2118, 2119, 2123, 2124, 2125, 2126, 2127, 2128, 2129, 2131, 2132, 2133, 2134, 2135, 2136, 2137, 2138, 2142, 2143, 2144, 2145, 2146, 2148, 2150, 2152, 2153, 2154, 2155, 2156, 2158, 2160, 2164, 2165, 2166, 2168, 2169, 2175, 2177, 2178, 2179, 2180, 2182, 2183, 2184, 2185, 2188, 2189, 2190, 2191, 2192, 2193, 2194, 2196, 2199, 2200, 2201, 2203, 2206, 2209, 2210, 2211, 2212, 2213, 2214, 2217, 2220, 2223, 2224, 2228, 2229, 2231, 2235, 2237, 2238, 2239, 2240, 2241, 2244, 2246, 2247, 2249, 2250, 2251, 2252, 2253, 2254, 2257, 2259, 2260, 2264, 2268, 2269, 2270, 2273, 2274, 2275, 2276, 2281, 2282, 2283, 2284, 2285, 2286, 2287, 2288, 2289, 2292, 2293, 2294, 2295, 2297, 2298, 2299, 2300, 2302, 2303, 2308, 2309, 2310, 2311, 2312, 2313, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2336, 2339, 2340, 2341, 2342, 2346, 2347, 2348, 2350, 2351, 2352, 2353, 2355, 2360, 2361, 2362, 2363, 2364, 2365, 2367, 2369, 2370, 2371, 2372, 2373, 2374, 2375, 2376, 2379, 2380, 2381, 2383, 2384, 2385, 2387, 2388, 2389, 2390, 2393, 2395, 2397, 2398, 2399, 2402, 2404, 2405, 2409, 2410, 2411, 2412, 2414, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2424, 2426, 2428, 2429, 2430, 2431, 2433, 2434, 2435, 2436, 2440, 2442, 2443, 2444, 2445, 2448, 2449, 2450, 2453, 2454, 2456, 2458, 2461, 2462, 2463, 2465, 2466, 2467, 2468, 2469, 2472, 2476, 2478, 2479, 2481, 2482, 2485, 2486, 2487, 2489, 2491, 2492, 2496, 2497, 2499, 2500, 2501, 2502, 2503, 2505, 2506, 2508, 2510, 2511, 2512, 2513, 2515, 2517, 2519, 2520, 2522, 2523, 2524, 2529, 2531, 2532, 2533, 2534, 2535, 2538, 2545, 2548, 2550, 2552, 2554, 2556, 2557, 2559, 2560, 2562, 2563, 2565, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2576, 2577, 2578, 2579, 2583, 2584, 2585, 2586, 2587, 2589, 2593, 2594, 2595, 2597, 2598, 2599, 2601, 2605, 2606, 2607, 2608, 2609, 2610, 2612, 2613, 2615, 2617, 2618, 2619, 2621, 2623, 2624, 2625, 2626, 2627, 2629, 2630, 2631, 2632, 2633, 2635, 2636, 2639, 2640, 2641, 2643, 2644, 2646, 2648, 2649, 2650, 2651, 2652, 2653, 2654, 2655, 2656, 2657, 2658, 2662, 2664, 2665, 2666, 2668, 2669, 2670, 2671, 2672, 2675, 2676, 2680, 2681, 2684, 2685, 2686, 2687, 2690, 2692, 2694, 2695, 2697, 2699, 2701, 2703, 2704, 2705, 2706, 2707]\n",
            "[0, 2, 3, 5, 7, 8, 9, 10, 13, 16, 17, 18, 20, 21, 22, 23, 24, 25, 27, 28, 29, 30, 32, 34, 35, 37, 39, 40, 41, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 57, 58, 60, 61, 62, 63, 65, 68, 69, 70, 71, 72, 73, 75, 77, 78, 80, 85, 87, 88, 89, 92, 95, 96, 97, 98, 100, 101, 103, 107, 109, 111, 112, 113, 114, 117, 118, 123, 125, 126, 128, 129, 130, 131, 132, 134, 136, 137, 140, 141, 143, 145, 147, 148, 149, 151, 152, 153, 154, 155, 157, 160, 161, 162, 164, 165, 166, 167, 170, 173, 174, 175, 176, 177, 180, 182, 183, 185, 186, 187, 188, 189, 190, 191, 193, 195, 197, 198, 199, 200, 201, 204, 208, 209, 211, 214, 215, 216, 217, 221, 223, 228, 230, 231, 232, 233, 236, 237, 238, 240, 242, 244, 246, 247, 249, 250, 253, 255, 257, 258, 259, 261, 262, 263, 264, 265, 266, 267, 268, 269, 270, 271, 272, 273, 275, 277, 279, 280, 282, 283, 284, 285, 287, 290, 291, 292, 294, 295, 297, 299, 300, 302, 304, 305, 307, 308, 310, 311, 312, 313, 316, 317, 322, 323, 324, 328, 333, 335, 336, 340, 341, 343, 344, 346, 348, 351, 353, 354, 355, 357, 358, 359, 361, 364, 367, 369, 370, 371, 373, 374, 376, 380, 381, 382, 385, 386, 388, 390, 391, 393, 394, 396, 398, 401, 402, 403, 406, 407, 409, 410, 411, 412, 413, 414, 415, 416, 419, 421, 422, 424, 425, 426, 427, 428, 429, 430, 432, 433, 434, 438, 440, 442, 446, 447, 448, 449, 452, 453, 454, 455, 460, 462, 463, 464, 466, 468, 469, 470, 471, 472, 474, 475, 482, 483, 484, 485, 486, 489, 490, 492, 495, 498, 500, 501, 504, 506, 508, 509, 510, 512, 514, 515, 518, 519, 520, 521, 523, 524, 527, 528, 530, 531, 533, 534, 535, 536, 537, 538, 539, 540, 541, 542, 543, 544, 545, 546, 547, 549, 550, 552, 555, 556, 558, 559, 560, 561, 562, 563, 564, 565, 566, 567, 568, 570, 571, 572, 575, 576, 577, 578, 580, 581, 582, 583, 584, 585, 586, 587, 588, 589, 591, 592, 593, 594, 595, 596, 598, 601, 603, 605, 607, 609, 610, 613, 616, 618, 619, 621, 622, 623, 624, 625, 630, 632, 633, 634, 635, 636, 637, 638, 639, 640, 643, 645, 646, 648, 650, 652, 655, 656, 659, 660, 662, 664, 665, 667, 669, 672, 673, 678, 680, 681, 683, 685, 688, 689, 690, 691, 692, 693, 694, 696, 698, 700, 701, 702, 703, 704, 705, 706, 707, 708, 709, 710, 711, 712, 713, 715, 716, 717, 718, 720, 721, 722, 723, 725, 726, 728, 729, 731, 733, 736, 737, 738, 740, 742, 746, 748, 749, 752, 753, 754, 755, 756, 757, 758, 759, 761, 763, 765, 766, 767, 768, 769, 770, 771, 773, 776, 778, 780, 781, 782, 784, 786, 787, 788, 789, 790, 791, 796, 799, 800, 801, 802, 803, 805, 807, 808, 813, 814, 815, 817, 819, 821, 823, 824, 827, 830, 831, 832, 833, 838, 839, 840, 842, 844, 845, 846, 847, 848, 854, 855, 856, 857, 860, 863, 864, 865, 868, 871, 872, 873, 874, 876, 878, 879, 880, 881, 885, 887, 890, 891, 892, 893, 894, 895, 896, 897, 898, 900, 903, 905, 906, 907, 908, 910, 911, 914, 917, 922, 923, 924, 925, 927, 928, 929, 930, 932, 933, 936, 937, 939, 940, 941, 943, 946, 947, 949, 950, 951, 953, 954, 955, 956, 957, 958, 959, 960, 961, 962, 964, 965, 966, 967, 968, 969, 970, 971, 972, 973, 975, 976, 978, 979, 980, 981, 983, 984, 985, 986, 987, 988, 989, 991, 992, 994, 995, 996, 997, 1000, 1003, 1004, 1005, 1008, 1009, 1011, 1012, 1013, 1014, 1015, 1016, 1018, 1022, 1023, 1024, 1026, 1027, 1028, 1030, 1031, 1033, 1034, 1035, 1036, 1037, 1038, 1039, 1040, 1042, 1043, 1045, 1047, 1048, 1051, 1052, 1053, 1054, 1057, 1059, 1061, 1063, 1064, 1066, 1067, 1068, 1069, 1071, 1072, 1073, 1075, 1076, 1077, 1078, 1079, 1080, 1081, 1082, 1083, 1084, 1085, 1087, 1088, 1089, 1090, 1091, 1094, 1095, 1098, 1099, 1100, 1101, 1103, 1106, 1111, 1112, 1113, 1114, 1116, 1117, 1118, 1121, 1123, 1124, 1125, 1126, 1127, 1128, 1129, 1130, 1131, 1132, 1136, 1137, 1138, 1140, 1142, 1144, 1146, 1147, 1148, 1149, 1150, 1153, 1154, 1155, 1156, 1158, 1159, 1160, 1161, 1165, 1167, 1168, 1169, 1170, 1171, 1172, 1173, 1176, 1177, 1180, 1181, 1182, 1183, 1185, 1186, 1189, 1191, 1192, 1193, 1196, 1197, 1198, 1199, 1201, 1204, 1206, 1207, 1210, 1211, 1214, 1216, 1217, 1219, 1220, 1221, 1222, 1225, 1226, 1227, 1228, 1232, 1235, 1236, 1239, 1242, 1244, 1245, 1246, 1247, 1249, 1251, 1252, 1254, 1255, 1256, 1257, 1259, 1260, 1265, 1267, 1269, 1270, 1272, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1283, 1284, 1285, 1286, 1287, 1288, 1289, 1290, 1293, 1294, 1295, 1297, 1298, 1299, 1300, 1301, 1303, 1304, 1306, 1308, 1309, 1310, 1311, 1313, 1314, 1315, 1316, 1317, 1318, 1319, 1320, 1323, 1325, 1329, 1334, 1335, 1337, 1339, 1340, 1341, 1343, 1345, 1346, 1348, 1350, 1354, 1356, 1358, 1368, 1371, 1373, 1374, 1376, 1377, 1378, 1379, 1380, 1381, 1383, 1384, 1386, 1388, 1392, 1393, 1394, 1395, 1396, 1397, 1401, 1402, 1404, 1406, 1407, 1408, 1411, 1413, 1414, 1415, 1416, 1419, 1422, 1423, 1424, 1426, 1427, 1428, 1429, 1430, 1434, 1435, 1436, 1438, 1440, 1441, 1442, 1444, 1445, 1447, 1449, 1456, 1460, 1461, 1462, 1463, 1464, 1465, 1467, 1468, 1469, 1470, 1471, 1472, 1473, 1475, 1478, 1479, 1480, 1482, 1483, 1484, 1486, 1489, 1490, 1491, 1492, 1494, 1496, 1497, 1498, 1499, 1500, 1502, 1503, 1505, 1506, 1507, 1508, 1510, 1513, 1514, 1515, 1516, 1517, 1520, 1522, 1523, 1524, 1525, 1527, 1528, 1529, 1531, 1532, 1533, 1535, 1536, 1537, 1538, 1539, 1541, 1542, 1543, 1546, 1547, 1551, 1553, 1555, 1556, 1557, 1558, 1559, 1560, 1563, 1564, 1566, 1567, 1568, 1570, 1573, 1574, 1575, 1576, 1577, 1581, 1582, 1583, 1586, 1589, 1590, 1591, 1592, 1593, 1594, 1595, 1596, 1597, 1598, 1601, 1602, 1605, 1606, 1607, 1609, 1611, 1612, 1613, 1615, 1617, 1618, 1623, 1627, 1628, 1630, 1632, 1633, 1634, 1635, 1636, 1637, 1641, 1642, 1643, 1644, 1645, 1648, 1649, 1651, 1652, 1653, 1655, 1656, 1657, 1658, 1662, 1663, 1664, 1666, 1667, 1668, 1669, 1670, 1673, 1674, 1675, 1676, 1678, 1679, 1681, 1684, 1685, 1688, 1689, 1690, 1693, 1694, 1695, 1696, 1697, 1698, 1700, 1701, 1702, 1703, 1709, 1710, 1712, 1713, 1715, 1717, 1720, 1722, 1723, 1724, 1725, 1727, 1729, 1731, 1732, 1734, 1735, 1736, 1737, 1739, 1740, 1741, 1742, 1743, 1744, 1747, 1748, 1749, 1756, 1759, 1760, 1761, 1762, 1763, 1765, 1766, 1767, 1768, 1770, 1771, 1772, 1773, 1774, 1775, 1778, 1779, 1780, 1781, 1782, 1785, 1787, 1788, 1789, 1790, 1791, 1793, 1795, 1797, 1798, 1799, 1800, 1801, 1804, 1805, 1806, 1807, 1811, 1812, 1814, 1815, 1816, 1817, 1818, 1819, 1820, 1824, 1825, 1826, 1827, 1828, 1830, 1831, 1834, 1836, 1839, 1841, 1842, 1843, 1849, 1850, 1853, 1854, 1855, 1856, 1857, 1860, 1861, 1864, 1868, 1869, 1870, 1871, 1877, 1878, 1879, 1880, 1881, 1882, 1886, 1888, 1889, 1890, 1892, 1893, 1897, 1899, 1900, 1901, 1903, 1905, 1906, 1907, 1909, 1910, 1911, 1912, 1914, 1915, 1916, 1917, 1918, 1919, 1921, 1922, 1923, 1924, 1926, 1928, 1930, 1931, 1932, 1933, 1935, 1936, 1938, 1939, 1940, 1941, 1942, 1943, 1944, 1945, 1946, 1947, 1950, 1952, 1953, 1955, 1956, 1958, 1960, 1961, 1964, 1965, 1966, 1967, 1969, 1970, 1971, 1972, 1974, 1975, 1976, 1978, 1979, 1980, 1983, 1984, 1986, 1989, 1992, 1994, 1995, 1996, 2003, 2004, 2005, 2009, 2010, 2012, 2013, 2015, 2018, 2019, 2021, 2022, 2024, 2025, 2027, 2028, 2029, 2030, 2031, 2032, 2033, 2035, 2037, 2039, 2041, 2043, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2055, 2056, 2057, 2062, 2063, 2065, 2066, 2067, 2069, 2070, 2071, 2072, 2075, 2076, 2078, 2080, 2082, 2083, 2084, 2085, 2086, 2087, 2092, 2093, 2094, 2097, 2098, 2099, 2101, 2102, 2103, 2105, 2107, 2112, 2113, 2115, 2116, 2118, 2119, 2121, 2122, 2123, 2124, 2125, 2126, 2127, 2129, 2130, 2131, 2133, 2134, 2135, 2138, 2139, 2140, 2141, 2142, 2143, 2144, 2145, 2147, 2148, 2153, 2154, 2155, 2156, 2158, 2159, 2160, 2162, 2163, 2164, 2165, 2167, 2168, 2171, 2172, 2174, 2175, 2176, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2190, 2192, 2194, 2195, 2197, 2199, 2200, 2201, 2204, 2205, 2206, 2207, 2208, 2209, 2211, 2213, 2215, 2218, 2220, 2221, 2223, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2235, 2236, 2237, 2241, 2243, 2244, 2247, 2248, 2250, 2253, 2254, 2255, 2256, 2257, 2258, 2262, 2263, 2264, 2265, 2269, 2270, 2273, 2274, 2278, 2279, 2280, 2281, 2282, 2283, 2285, 2286, 2287, 2288, 2289, 2290, 2291, 2293, 2294, 2296, 2297, 2300, 2301, 2302, 2304, 2305, 2306, 2310, 2311, 2312, 2313, 2315, 2317, 2318, 2319, 2320, 2321, 2324, 2327, 2328, 2330, 2331, 2332, 2334, 2339, 2341, 2342, 2343, 2344, 2346, 2347, 2348, 2349, 2351, 2354, 2355, 2356, 2358, 2359, 2360, 2362, 2364, 2366, 2367, 2370, 2372, 2374, 2376, 2377, 2378, 2381, 2382, 2383, 2384, 2385, 2386, 2389, 2391, 2392, 2394, 2395, 2399, 2401, 2403, 2405, 2410, 2411, 2412, 2414, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2423, 2425, 2426, 2427, 2428, 2429, 2430, 2431, 2433, 2434, 2435, 2436, 2438, 2441, 2443, 2444, 2445, 2446, 2447, 2448, 2449, 2454, 2455, 2456, 2458, 2459, 2465, 2468, 2469, 2470, 2471, 2472, 2473, 2475, 2476, 2477, 2479, 2480, 2481, 2483, 2484, 2485, 2488, 2489, 2490, 2492, 2494, 2495, 2496, 2497, 2498, 2500, 2502, 2503, 2504, 2507, 2508, 2509, 2510, 2511, 2512, 2513, 2514, 2516, 2518, 2519, 2520, 2522, 2524, 2526, 2528, 2529, 2530, 2532, 2534, 2535, 2536, 2537, 2538, 2539, 2541, 2545, 2546, 2547, 2549, 2550, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2561, 2562, 2563, 2564, 2565, 2566, 2567, 2568, 2572, 2574, 2575, 2577, 2579, 2580, 2581, 2582, 2587, 2588, 2589, 2593, 2594, 2595, 2598, 2600, 2602, 2603, 2604, 2605, 2608, 2609, 2614, 2615, 2619, 2623, 2625, 2626, 2628, 2629, 2631, 2632, 2634, 2637, 2638, 2639, 2643, 2645, 2646, 2648, 2649, 2652, 2655, 2657, 2661, 2662, 2665, 2666, 2667, 2668, 2669, 2670, 2673, 2674, 2677, 2682, 2683, 2685, 2686, 2688, 2689, 2690, 2692, 2693, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2703, 2704, 2707]\n",
            "[1, 2, 3, 4, 5, 6, 7, 8, 9, 12, 15, 16, 17, 18, 19, 21, 22, 23, 24, 25, 29, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 43, 44, 45, 46, 48, 49, 51, 52, 54, 57, 58, 59, 60, 61, 62, 63, 65, 66, 69, 71, 72, 74, 75, 76, 77, 78, 79, 88, 89, 90, 91, 94, 95, 96, 97, 98, 99, 102, 103, 104, 107, 112, 113, 115, 116, 118, 119, 122, 123, 124, 126, 127, 128, 133, 134, 137, 138, 139, 140, 142, 143, 145, 147, 149, 150, 152, 153, 154, 157, 159, 160, 161, 162, 165, 167, 168, 170, 171, 173, 175, 176, 177, 179, 181, 183, 184, 185, 187, 188, 189, 190, 196, 197, 199, 201, 202, 205, 207, 209, 210, 212, 213, 214, 215, 216, 217, 219, 224, 225, 226, 228, 230, 231, 233, 235, 238, 239, 242, 243, 244, 246, 247, 248, 249, 250, 253, 254, 255, 256, 258, 259, 263, 264, 265, 269, 271, 272, 273, 275, 277, 278, 279, 281, 285, 287, 288, 289, 294, 296, 297, 298, 299, 300, 301, 303, 304, 305, 307, 308, 309, 310, 312, 314, 315, 316, 317, 318, 319, 321, 322, 323, 326, 327, 328, 329, 332, 336, 337, 338, 339, 340, 341, 342, 343, 346, 348, 349, 351, 353, 355, 356, 357, 358, 361, 364, 365, 366, 369, 370, 371, 373, 374, 375, 377, 380, 381, 382, 384, 385, 386, 388, 389, 391, 394, 395, 397, 399, 400, 402, 403, 404, 408, 409, 416, 417, 418, 419, 420, 423, 424, 425, 426, 428, 429, 435, 437, 441, 442, 443, 444, 447, 449, 450, 451, 452, 453, 455, 456, 458, 461, 462, 463, 465, 466, 467, 470, 471, 472, 473, 475, 476, 481, 482, 484, 487, 489, 491, 494, 495, 498, 500, 501, 503, 505, 507, 511, 512, 514, 515, 516, 517, 518, 521, 522, 523, 524, 525, 527, 529, 530, 531, 532, 533, 534, 535, 536, 539, 540, 541, 543, 544, 545, 551, 553, 554, 555, 556, 557, 558, 559, 560, 561, 564, 566, 569, 571, 572, 574, 575, 576, 577, 578, 579, 580, 582, 586, 587, 588, 589, 590, 591, 593, 594, 596, 597, 598, 599, 600, 601, 602, 603, 604, 605, 606, 609, 611, 612, 613, 615, 617, 618, 621, 622, 623, 624, 625, 626, 629, 630, 631, 632, 633, 635, 636, 638, 639, 645, 646, 647, 649, 650, 652, 654, 656, 657, 661, 662, 663, 664, 665, 667, 668, 669, 670, 671, 672, 673, 674, 677, 678, 680, 681, 682, 683, 685, 687, 688, 689, 690, 692, 694, 695, 696, 697, 698, 700, 701, 703, 704, 708, 709, 710, 711, 714, 715, 716, 717, 718, 719, 720, 722, 723, 724, 725, 726, 728, 730, 731, 732, 735, 736, 737, 738, 741, 745, 746, 748, 753, 754, 755, 758, 759, 761, 762, 764, 765, 766, 767, 770, 771, 773, 774, 775, 776, 777, 781, 783, 784, 786, 788, 789, 790, 791, 795, 799, 800, 801, 805, 806, 809, 813, 814, 816, 817, 818, 819, 820, 823, 824, 825, 826, 828, 829, 830, 832, 833, 834, 835, 836, 838, 840, 841, 842, 844, 847, 852, 855, 856, 857, 859, 860, 861, 862, 863, 864, 867, 870, 871, 873, 874, 878, 879, 882, 883, 884, 887, 888, 889, 890, 891, 892, 895, 896, 900, 901, 902, 903, 905, 906, 907, 909, 910, 912, 913, 914, 915, 916, 917, 919, 920, 922, 923, 924, 925, 926, 927, 929, 931, 933, 934, 937, 939, 940, 941, 943, 944, 945, 947, 951, 953, 954, 955, 957, 959, 961, 962, 963, 964, 966, 967, 968, 969, 970, 971, 972, 973, 974, 975, 977, 981, 983, 984, 985, 988, 989, 990, 992, 993, 995, 996, 1000, 1001, 1003, 1005, 1007, 1008, 1009, 1010, 1012, 1013, 1016, 1018, 1020, 1021, 1024, 1025, 1026, 1027, 1028, 1031, 1032, 1033, 1034, 1035, 1036, 1038, 1040, 1041, 1043, 1044, 1045, 1047, 1051, 1052, 1053, 1054, 1056, 1059, 1060, 1061, 1062, 1065, 1067, 1068, 1071, 1072, 1075, 1077, 1080, 1082, 1086, 1088, 1089, 1090, 1092, 1095, 1096, 1099, 1100, 1101, 1103, 1104, 1105, 1106, 1108, 1109, 1113, 1116, 1117, 1121, 1122, 1123, 1124, 1126, 1127, 1128, 1129, 1131, 1132, 1134, 1135, 1138, 1139, 1142, 1143, 1145, 1146, 1148, 1150, 1151, 1152, 1153, 1157, 1160, 1162, 1163, 1166, 1169, 1172, 1173, 1174, 1175, 1176, 1177, 1179, 1183, 1184, 1187, 1188, 1189, 1190, 1191, 1199, 1202, 1204, 1205, 1206, 1208, 1209, 1210, 1213, 1215, 1219, 1222, 1224, 1225, 1227, 1228, 1230, 1232, 1233, 1234, 1235, 1237, 1239, 1240, 1243, 1244, 1246, 1247, 1250, 1252, 1256, 1258, 1259, 1261, 1266, 1267, 1268, 1269, 1271, 1272, 1273, 1274, 1275, 1276, 1278, 1279, 1280, 1282, 1283, 1285, 1286, 1287, 1289, 1290, 1291, 1294, 1295, 1297, 1298, 1299, 1301, 1303, 1304, 1308, 1309, 1310, 1311, 1312, 1313, 1314, 1316, 1317, 1318, 1322, 1323, 1324, 1325, 1326, 1327, 1328, 1332, 1333, 1334, 1335, 1336, 1337, 1338, 1339, 1340, 1341, 1342, 1344, 1347, 1348, 1350, 1351, 1353, 1354, 1357, 1358, 1359, 1362, 1364, 1366, 1367, 1370, 1371, 1373, 1374, 1375, 1376, 1377, 1380, 1382, 1383, 1384, 1385, 1386, 1388, 1389, 1390, 1392, 1393, 1396, 1400, 1401, 1403, 1405, 1406, 1407, 1409, 1413, 1414, 1416, 1417, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1428, 1430, 1432, 1434, 1435, 1438, 1439, 1440, 1441, 1444, 1445, 1446, 1447, 1448, 1450, 1451, 1452, 1454, 1455, 1456, 1457, 1458, 1459, 1460, 1461, 1464, 1465, 1468, 1470, 1471, 1472, 1473, 1474, 1475, 1478, 1479, 1480, 1481, 1482, 1483, 1485, 1488, 1490, 1492, 1493, 1494, 1495, 1497, 1498, 1501, 1503, 1504, 1505, 1507, 1508, 1509, 1510, 1511, 1512, 1513, 1514, 1517, 1519, 1520, 1522, 1523, 1525, 1526, 1527, 1528, 1531, 1532, 1533, 1534, 1537, 1538, 1539, 1544, 1545, 1546, 1547, 1549, 1551, 1553, 1556, 1557, 1558, 1560, 1562, 1563, 1565, 1567, 1570, 1571, 1572, 1573, 1575, 1576, 1578, 1579, 1580, 1581, 1583, 1584, 1587, 1589, 1591, 1592, 1593, 1595, 1598, 1599, 1600, 1601, 1602, 1603, 1605, 1608, 1609, 1613, 1614, 1617, 1620, 1623, 1625, 1626, 1627, 1628, 1630, 1631, 1632, 1633, 1634, 1639, 1642, 1643, 1645, 1647, 1649, 1650, 1651, 1652, 1653, 1655, 1657, 1658, 1659, 1660, 1663, 1665, 1666, 1667, 1669, 1671, 1673, 1677, 1678, 1680, 1681, 1682, 1683, 1684, 1686, 1687, 1688, 1689, 1690, 1691, 1692, 1693, 1694, 1695, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1705, 1706, 1709, 1711, 1712, 1713, 1717, 1718, 1719, 1720, 1723, 1725, 1726, 1727, 1728, 1730, 1731, 1732, 1733, 1736, 1737, 1738, 1739, 1740, 1742, 1744, 1747, 1749, 1750, 1751, 1753, 1755, 1756, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1766, 1767, 1768, 1770, 1772, 1773, 1774, 1775, 1776, 1778, 1779, 1780, 1781, 1782, 1783, 1784, 1785, 1786, 1787, 1788, 1789, 1794, 1795, 1797, 1798, 1800, 1802, 1804, 1806, 1807, 1809, 1810, 1811, 1812, 1813, 1814, 1815, 1817, 1819, 1820, 1821, 1824, 1826, 1828, 1829, 1831, 1832, 1834, 1835, 1837, 1838, 1839, 1841, 1842, 1844, 1846, 1848, 1850, 1851, 1852, 1853, 1854, 1857, 1858, 1859, 1863, 1866, 1867, 1868, 1869, 1871, 1872, 1873, 1875, 1876, 1877, 1880, 1881, 1883, 1884, 1885, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1897, 1898, 1900, 1901, 1903, 1904, 1905, 1906, 1908, 1909, 1910, 1911, 1912, 1913, 1914, 1915, 1916, 1917, 1918, 1919, 1923, 1924, 1925, 1926, 1927, 1928, 1929, 1930, 1931, 1932, 1935, 1938, 1940, 1941, 1942, 1943, 1944, 1945, 1948, 1949, 1955, 1957, 1958, 1960, 1961, 1962, 1963, 1966, 1968, 1969, 1970, 1971, 1974, 1977, 1978, 1979, 1982, 1983, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1992, 1993, 1994, 1996, 1998, 1999, 2000, 2001, 2002, 2004, 2005, 2007, 2008, 2012, 2014, 2015, 2017, 2018, 2020, 2024, 2027, 2028, 2030, 2032, 2034, 2035, 2036, 2037, 2038, 2040, 2042, 2044, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2053, 2054, 2055, 2056, 2057, 2058, 2059, 2060, 2061, 2062, 2063, 2065, 2066, 2067, 2069, 2071, 2072, 2075, 2076, 2077, 2082, 2083, 2085, 2087, 2089, 2091, 2092, 2094, 2095, 2100, 2101, 2102, 2103, 2104, 2105, 2106, 2108, 2109, 2113, 2116, 2117, 2118, 2120, 2122, 2123, 2124, 2126, 2127, 2128, 2129, 2130, 2131, 2132, 2133, 2137, 2138, 2142, 2144, 2147, 2148, 2152, 2154, 2157, 2158, 2160, 2161, 2162, 2164, 2165, 2166, 2167, 2168, 2169, 2172, 2175, 2176, 2178, 2180, 2182, 2183, 2186, 2187, 2188, 2189, 2192, 2193, 2194, 2195, 2196, 2199, 2201, 2202, 2204, 2205, 2206, 2209, 2211, 2212, 2214, 2215, 2216, 2217, 2218, 2220, 2221, 2225, 2226, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2235, 2237, 2238, 2239, 2243, 2246, 2247, 2248, 2249, 2250, 2251, 2252, 2253, 2257, 2258, 2261, 2262, 2265, 2267, 2268, 2269, 2270, 2271, 2273, 2274, 2277, 2279, 2280, 2284, 2287, 2288, 2289, 2292, 2294, 2295, 2296, 2297, 2298, 2299, 2300, 2303, 2305, 2306, 2307, 2308, 2309, 2310, 2312, 2314, 2318, 2319, 2320, 2325, 2326, 2327, 2328, 2330, 2331, 2332, 2335, 2336, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2347, 2348, 2349, 2350, 2351, 2353, 2354, 2355, 2356, 2357, 2358, 2361, 2363, 2364, 2365, 2366, 2368, 2371, 2372, 2373, 2375, 2376, 2377, 2379, 2380, 2381, 2382, 2383, 2384, 2385, 2386, 2387, 2388, 2390, 2394, 2395, 2396, 2397, 2398, 2399, 2401, 2403, 2404, 2405, 2408, 2410, 2411, 2413, 2415, 2416, 2417, 2418, 2419, 2420, 2421, 2422, 2424, 2425, 2426, 2428, 2431, 2432, 2433, 2434, 2436, 2439, 2442, 2443, 2445, 2446, 2447, 2448, 2449, 2450, 2452, 2453, 2454, 2455, 2457, 2459, 2460, 2462, 2464, 2466, 2467, 2469, 2470, 2472, 2473, 2474, 2476, 2478, 2479, 2480, 2481, 2483, 2484, 2487, 2491, 2492, 2493, 2494, 2495, 2496, 2497, 2500, 2501, 2503, 2504, 2506, 2508, 2510, 2513, 2518, 2519, 2523, 2524, 2525, 2526, 2528, 2529, 2530, 2534, 2535, 2536, 2537, 2538, 2539, 2540, 2541, 2545, 2549, 2551, 2552, 2553, 2555, 2559, 2560, 2561, 2564, 2565, 2566, 2567, 2569, 2570, 2571, 2573, 2574, 2577, 2578, 2579, 2582, 2583, 2584, 2585, 2586, 2587, 2589, 2590, 2592, 2593, 2594, 2596, 2597, 2601, 2602, 2605, 2606, 2609, 2611, 2612, 2614, 2615, 2616, 2617, 2618, 2619, 2620, 2621, 2623, 2624, 2626, 2628, 2629, 2630, 2631, 2632, 2633, 2634, 2637, 2638, 2639, 2640, 2641, 2642, 2646, 2648, 2649, 2652, 2655, 2656, 2657, 2658, 2660, 2663, 2665, 2668, 2669, 2670, 2672, 2674, 2675, 2676, 2677, 2678, 2679, 2680, 2681, 2682, 2683, 2684, 2685, 2686, 2688, 2692, 2693, 2695, 2696, 2697, 2698, 2699, 2700, 2701, 2702, 2704, 2705, 2706, 2707]\n",
            "[0, 1, 3, 5, 6, 7, 9, 10, 11, 12, 14, 16, 18, 19, 20, 22, 23, 24, 25, 28, 29, 31, 35, 36, 37, 39, 41, 42, 43, 44, 46, 47, 48, 50, 51, 52, 54, 55, 56, 59, 62, 63, 64, 65, 66, 67, 69, 70, 71, 72, 73, 75, 76, 77, 81, 84, 86, 89, 93, 95, 99, 100, 104, 105, 106, 107, 108, 110, 111, 112, 113, 114, 115, 116, 117, 119, 120, 121, 123, 125, 127, 128, 129, 130, 131, 132, 134, 136, 137, 138, 139, 141, 144, 145, 146, 147, 149, 152, 153, 154, 156, 158, 159, 161, 162, 163, 164, 165, 166, 167, 171, 172, 175, 176, 177, 179, 180, 181, 182, 183, 189, 190, 191, 193, 194, 195, 196, 197, 198, 199, 200, 203, 204, 205, 206, 207, 208, 210, 211, 212, 215, 217, 218, 222, 223, 224, 225, 226, 228, 230, 231, 233, 234, 235, 236, 237, 239, 241, 243, 244, 245, 246, 247, 248, 249, 250, 251, 255, 259, 261, 262, 263, 264, 265, 267, 268, 269, 270, 272, 273, 275, 280, 281, 282, 283, 284, 286, 290, 291, 294, 295, 296, 297, 300, 304, 305, 308, 309, 310, 311, 312, 313, 319, 320, 321, 322, 324, 331, 332, 333, 334, 335, 336, 337, 339, 340, 341, 343, 344, 345, 347, 349, 352, 353, 354, 355, 358, 359, 361, 362, 367, 370, 372, 373, 374, 375, 376, 379, 380, 381, 382, 383, 384, 385, 386, 387, 388, 389, 391, 392, 393, 394, 397, 398, 399, 400, 402, 403, 405, 407, 409, 410, 411, 413, 414, 415, 416, 417, 419, 420, 422, 423, 424, 425, 427, 428, 430, 434, 435, 437, 442, 443, 444, 445, 446, 449, 450, 451, 453, 454, 455, 456, 457, 459, 460, 461, 463, 464, 465, 467, 468, 469, 471, 472, 473, 474, 475, 476, 477, 482, 484, 485, 487, 488, 489, 490, 491, 492, 493, 494, 495, 497, 498, 499, 501, 504, 506, 508, 509, 510, 511, 512, 513, 514, 515, 517, 518, 520, 522, 523, 527, 529, 530, 531, 533, 534, 535, 537, 538, 539, 542, 546, 547, 548, 550, 551, 552, 553, 554, 555, 556, 557, 561, 562, 563, 564, 566, 569, 570, 573, 575, 578, 579, 580, 581, 583, 584, 585, 586, 587, 588, 589, 590, 591, 593, 595, 596, 599, 600, 602, 603, 606, 608, 609, 610, 612, 613, 615, 617, 619, 620, 621, 622, 624, 625, 626, 629, 630, 631, 633, 635, 637, 638, 641, 642, 643, 645, 646, 648, 649, 650, 651, 652, 653, 654, 656, 657, 660, 661, 664, 666, 668, 669, 671, 672, 673, 678, 679, 680, 681, 684, 685, 686, 687, 688, 689, 690, 691, 693, 695, 696, 699, 700, 702, 705, 706, 707, 708, 709, 710, 711, 712, 717, 718, 719, 721, 722, 723, 724, 726, 727, 729, 731, 733, 734, 736, 737, 739, 742, 743, 744, 745, 746, 747, 749, 751, 752, 753, 754, 755, 757, 758, 760, 762, 764, 765, 766, 769, 770, 772, 773, 774, 775, 778, 779, 782, 783, 785, 786, 788, 789, 790, 793, 795, 796, 797, 799, 800, 803, 804, 807, 808, 809, 812, 813, 814, 815, 817, 818, 820, 822, 825, 827, 828, 829, 832, 833, 834, 835, 838, 840, 843, 844, 846, 847, 848, 849, 850, 851, 852, 853, 854, 855, 856, 857, 859, 862, 864, 868, 869, 870, 871, 872, 874, 878, 879, 882, 883, 884, 885, 888, 889, 890, 893, 895, 897, 898, 899, 903, 905, 906, 907, 908, 909, 910, 911, 913, 914, 915, 916, 919, 920, 922, 923, 925, 926, 927, 928, 929, 930, 932, 933, 935, 936, 937, 939, 940, 941, 942, 943, 947, 948, 949, 950, 951, 952, 953, 954, 955, 957, 958, 959, 961, 962, 964, 965, 968, 969, 970, 973, 975, 977, 978, 981, 984, 987, 989, 999, 1000, 1001, 1002, 1003, 1004, 1006, 1008, 1011, 1012, 1013, 1015, 1016, 1018, 1020, 1021, 1022, 1023, 1024, 1025, 1026, 1029, 1030, 1032, 1034, 1038, 1040, 1041, 1044, 1045, 1047, 1048, 1049, 1050, 1054, 1055, 1056, 1057, 1058, 1059, 1060, 1061, 1063, 1064, 1065, 1067, 1068, 1069, 1070, 1072, 1074, 1075, 1079, 1080, 1081, 1083, 1084, 1085, 1086, 1090, 1091, 1092, 1093, 1094, 1095, 1099, 1100, 1103, 1106, 1107, 1109, 1111, 1112, 1113, 1114, 1115, 1117, 1119, 1121, 1122, 1123, 1124, 1125, 1126, 1127, 1130, 1131, 1133, 1137, 1138, 1140, 1141, 1142, 1143, 1144, 1145, 1147, 1149, 1153, 1154, 1155, 1156, 1158, 1160, 1162, 1163, 1164, 1166, 1167, 1169, 1171, 1174, 1176, 1178, 1179, 1180, 1181, 1183, 1184, 1185, 1186, 1188, 1189, 1193, 1194, 1196, 1199, 1200, 1201, 1204, 1209, 1210, 1211, 1212, 1213, 1214, 1217, 1219, 1220, 1222, 1224, 1225, 1227, 1230, 1233, 1235, 1236, 1237, 1239, 1241, 1242, 1243, 1244, 1245, 1246, 1248, 1249, 1250, 1251, 1252, 1255, 1256, 1257, 1260, 1263, 1264, 1265, 1266, 1269, 1272, 1276, 1281, 1282, 1283, 1285, 1288, 1289, 1295, 1296, 1297, 1298, 1300, 1303, 1304, 1306, 1307, 1310, 1312, 1313, 1314, 1316, 1317, 1319, 1320, 1321, 1324, 1325, 1328, 1330, 1331, 1332, 1333, 1334, 1336, 1337, 1338, 1341, 1342, 1343, 1344, 1345, 1349, 1351, 1352, 1353, 1355, 1357, 1358, 1359, 1360, 1361, 1364, 1366, 1367, 1368, 1369, 1370, 1373, 1376, 1377, 1379, 1380, 1381, 1383, 1384, 1385, 1387, 1388, 1391, 1394, 1395, 1396, 1398, 1401, 1402, 1404, 1405, 1408, 1413, 1416, 1417, 1418, 1422, 1423, 1424, 1427, 1428, 1429, 1430, 1431, 1432, 1433, 1435, 1436, 1437, 1438, 1439, 1440, 1441, 1442, 1443, 1445, 1447, 1449, 1450, 1451, 1452, 1453, 1454, 1455, 1456, 1458, 1460, 1461, 1462, 1463, 1465, 1466, 1467, 1469, 1470, 1471, 1472, 1473, 1475, 1478, 1479, 1485, 1486, 1487, 1488, 1489, 1491, 1492, 1493, 1494, 1495, 1497, 1499, 1500, 1502, 1503, 1506, 1507, 1510, 1511, 1513, 1514, 1515, 1517, 1518, 1521, 1522, 1524, 1525, 1526, 1527, 1529, 1530, 1531, 1532, 1533, 1534, 1535, 1538, 1539, 1540, 1544, 1545, 1546, 1547, 1549, 1552, 1553, 1554, 1555, 1556, 1558, 1560, 1561, 1562, 1563, 1564, 1565, 1567, 1568, 1571, 1573, 1574, 1575, 1576, 1577, 1578, 1579, 1580, 1581, 1582, 1583, 1585, 1586, 1588, 1589, 1590, 1591, 1594, 1597, 1599, 1600, 1602, 1603, 1606, 1608, 1610, 1611, 1613, 1614, 1615, 1616, 1617, 1620, 1621, 1622, 1623, 1624, 1625, 1626, 1628, 1629, 1630, 1633, 1638, 1640, 1641, 1642, 1646, 1648, 1650, 1652, 1653, 1655, 1658, 1663, 1665, 1666, 1672, 1673, 1674, 1677, 1678, 1680, 1681, 1683, 1684, 1685, 1687, 1693, 1694, 1695, 1696, 1697, 1698, 1699, 1700, 1701, 1702, 1703, 1704, 1705, 1706, 1707, 1709, 1710, 1711, 1712, 1713, 1715, 1716, 1717, 1718, 1719, 1721, 1722, 1723, 1724, 1725, 1727, 1731, 1733, 1735, 1737, 1740, 1742, 1743, 1745, 1746, 1747, 1748, 1749, 1750, 1752, 1753, 1755, 1756, 1757, 1758, 1760, 1762, 1763, 1765, 1766, 1768, 1770, 1771, 1774, 1775, 1776, 1777, 1778, 1779, 1782, 1783, 1785, 1786, 1789, 1790, 1791, 1792, 1793, 1794, 1797, 1801, 1804, 1805, 1806, 1809, 1811, 1813, 1815, 1816, 1817, 1820, 1822, 1823, 1824, 1825, 1826, 1827, 1829, 1830, 1834, 1835, 1837, 1838, 1839, 1841, 1842, 1844, 1845, 1846, 1847, 1853, 1854, 1855, 1857, 1858, 1861, 1862, 1863, 1864, 1868, 1869, 1870, 1871, 1872, 1874, 1875, 1877, 1878, 1880, 1882, 1885, 1886, 1888, 1889, 1890, 1892, 1893, 1894, 1895, 1896, 1897, 1898, 1899, 1900, 1901, 1902, 1904, 1905, 1908, 1910, 1913, 1914, 1916, 1917, 1918, 1919, 1920, 1921, 1923, 1926, 1927, 1928, 1929, 1932, 1933, 1934, 1935, 1936, 1937, 1938, 1940, 1942, 1943, 1944, 1945, 1947, 1948, 1950, 1952, 1953, 1954, 1955, 1956, 1957, 1958, 1959, 1963, 1964, 1968, 1970, 1973, 1974, 1977, 1978, 1984, 1985, 1986, 1987, 1988, 1989, 1990, 1991, 1992, 1993, 1995, 1996, 1997, 2000, 2002, 2004, 2011, 2012, 2013, 2014, 2016, 2020, 2022, 2023, 2025, 2026, 2028, 2030, 2035, 2037, 2038, 2039, 2040, 2041, 2043, 2044, 2045, 2047, 2048, 2049, 2056, 2058, 2059, 2060, 2061, 2064, 2065, 2066, 2071, 2072, 2074, 2075, 2076, 2077, 2079, 2082, 2083, 2084, 2085, 2086, 2088, 2090, 2091, 2092, 2093, 2095, 2096, 2097, 2098, 2100, 2101, 2102, 2103, 2104, 2107, 2108, 2109, 2111, 2112, 2113, 2114, 2115, 2117, 2119, 2120, 2121, 2122, 2124, 2125, 2126, 2127, 2129, 2130, 2131, 2132, 2133, 2134, 2135, 2140, 2144, 2145, 2146, 2147, 2150, 2151, 2153, 2154, 2155, 2158, 2159, 2162, 2163, 2166, 2167, 2168, 2172, 2173, 2174, 2175, 2176, 2177, 2178, 2180, 2181, 2188, 2189, 2190, 2191, 2192, 2196, 2197, 2198, 2202, 2203, 2204, 2205, 2207, 2211, 2212, 2214, 2215, 2216, 2217, 2218, 2219, 2220, 2227, 2229, 2232, 2233, 2234, 2236, 2237, 2238, 2240, 2241, 2243, 2244, 2245, 2246, 2247, 2248, 2249, 2250, 2251, 2253, 2254, 2258, 2259, 2261, 2262, 2263, 2264, 2265, 2268, 2270, 2271, 2274, 2275, 2276, 2277, 2278, 2280, 2282, 2283, 2285, 2286, 2288, 2290, 2292, 2293, 2294, 2295, 2297, 2299, 2300, 2301, 2305, 2308, 2309, 2312, 2313, 2314, 2316, 2317, 2318, 2319, 2320, 2321, 2322, 2324, 2325, 2326, 2328, 2329, 2330, 2332, 2333, 2336, 2337, 2339, 2340, 2341, 2342, 2343, 2344, 2345, 2353, 2354, 2357, 2358, 2359, 2360, 2361, 2362, 2364, 2365, 2366, 2367, 2369, 2370, 2372, 2373, 2374, 2375, 2376, 2377, 2379, 2380, 2382, 2383, 2384, 2388, 2389, 2391, 2392, 2394, 2396, 2399, 2400, 2401, 2404, 2407, 2408, 2409, 2411, 2416, 2418, 2419, 2420, 2421, 2422, 2423, 2426, 2427, 2428, 2429, 2430, 2432, 2434, 2435, 2437, 2439, 2440, 2442, 2444, 2446, 2447, 2448, 2449, 2450, 2453, 2454, 2455, 2456, 2458, 2459, 2462, 2465, 2466, 2467, 2468, 2473, 2474, 2476, 2477, 2478, 2481, 2482, 2484, 2485, 2486, 2487, 2489, 2490, 2492, 2493, 2494, 2495, 2496, 2497, 2500, 2501, 2502, 2503, 2504, 2505, 2508, 2509, 2510, 2511, 2512, 2513, 2515, 2516, 2518, 2520, 2521, 2523, 2524, 2525, 2526, 2527, 2528, 2530, 2531, 2533, 2534, 2536, 2538, 2539, 2543, 2545, 2546, 2547, 2548, 2550, 2551, 2552, 2553, 2554, 2556, 2557, 2562, 2564, 2565, 2566, 2567, 2568, 2569, 2573, 2575, 2577, 2578, 2579, 2580, 2582, 2583, 2585, 2586, 2587, 2589, 2591, 2593, 2594, 2595, 2599, 2600, 2601, 2602, 2603, 2605, 2606, 2609, 2610, 2611, 2612, 2613, 2615, 2616, 2617, 2618, 2620, 2621, 2622, 2623, 2627, 2628, 2629, 2631, 2633, 2635, 2636, 2637, 2638, 2639, 2640, 2642, 2643, 2644, 2645, 2648, 2651, 2653, 2654, 2655, 2656, 2657, 2658, 2659, 2660, 2661, 2662, 2665, 2666, 2669, 2671, 2672, 2674, 2675, 2676, 2679, 2681, 2686, 2688, 2690, 2692, 2695, 2696, 2698, 2699, 2700, 2701, 2703, 2705, 2706, 2707]\n",
            "[2, 3, 4, 6, 7, 9, 10, 11, 12, 14, 15, 16, 17, 20, 21, 22, 23, 25, 26, 30, 31, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 44, 45, 46, 47, 48, 49, 50, 51, 52, 55, 56, 60, 61, 63, 64, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 79, 81, 82, 83, 84, 85, 86, 88, 89, 91, 92, 93, 95, 96, 97, 98, 101, 102, 103, 105, 108, 109, 111, 112, 113, 114, 117, 118, 119, 120, 121, 123, 125, 126, 127, 129, 130, 131, 133, 134, 135, 136, 137, 139, 140, 141, 142, 144, 146, 148, 152, 153, 154, 155, 157, 158, 161, 162, 163, 164, 166, 169, 171, 174, 175, 176, 177, 178, 180, 181, 183, 185, 186, 187, 190, 191, 192, 194, 195, 196, 198, 199, 200, 201, 202, 204, 205, 206, 207, 208, 210, 211, 212, 215, 217, 219, 220, 221, 222, 223, 224, 226, 228, 229, 230, 232, 235, 239, 240, 242, 243, 246, 247, 248, 249, 252, 254, 255, 257, 258, 259, 263, 265, 266, 267, 268, 269, 270, 273, 276, 277, 280, 281, 282, 284, 286, 287, 288, 292, 293, 294, 295, 296, 302, 303, 305, 307, 308, 309, 310, 315, 317, 318, 323, 324, 326, 327, 329, 331, 332, 333, 334, 337, 339, 340, 341, 342, 343, 345, 346, 347, 348, 349, 350, 351, 354, 355, 356, 357, 362, 363, 364, 367, 368, 372, 373, 374, 377, 379, 380, 381, 382, 383, 386, 387, 388, 389, 390, 391, 393, 394, 396, 397, 398, 399, 401, 402, 403, 405, 406, 407, 409, 411, 413, 416, 420, 422, 423, 424, 425, 426, 428, 429, 430, 432, 434, 436, 437, 438, 442, 443, 444, 445, 446, 448, 449, 450, 451, 454, 455, 456, 457, 458, 459, 461, 463, 464, 465, 467, 470, 471, 472, 473, 474, 475, 477, 478, 479, 483, 484, 488, 489, 490, 491, 496, 497, 498, 500, 502, 503, 504, 506, 508, 509, 511, 513, 514, 516, 517, 520, 521, 523, 525, 526, 527, 529, 530, 531, 532, 533, 536, 538, 539, 541, 542, 543, 544, 545, 547, 550, 551, 554, 555, 557, 558, 565, 566, 567, 568, 569, 570, 571, 573, 574, 575, 576, 578, 579, 580, 582, 583, 585, 586, 587, 588, 589, 590, 591, 592, 594, 595, 596, 597, 600, 602, 603, 604, 605, 606, 607, 608, 609, 611, 613, 614, 616, 617, 618, 619, 621, 622, 624, 625, 626, 628, 629, 633, 635, 636, 638, 639, 640, 641, 642, 645, 649, 650, 651, 652, 655, 657, 658, 661, 665, 666, 669, 671, 672, 673, 674, 675, 676, 679, 680, 681, 682, 683, 684, 687, 688, 689, 690, 693, 694, 695, 697, 698, 699, 700, 702, 703, 707, 709, 710, 712, 716, 717, 719, 721, 722, 723, 724, 726, 728, 729, 731, 733, 734, 735, 737, 740, 741, 743, 744, 745, 746, 747, 749, 751, 752, 754, 755, 756, 758, 760, 761, 762, 763, 765, 768, 769, 770, 771, 773, 776, 777, 778, 779, 780, 781, 782, 783, 784, 785, 786, 788, 789, 790, 791, 793, 794, 795, 798, 801, 802, 803, 804, 805, 806, 807, 809, 810, 811, 812, 813, 814, 815, 816, 817, 818, 819, 822, 823, 827, 828, 829, 831, 832, 833, 835, 839, 840, 842, 844, 846, 847, 848, 849, 853, 854, 855, 857, 858, 860, 864, 868, 870, 871, 872, 873, 874, 875, 876, 877, 879, 880, 881, 882, 885, 887, 888, 890, 892, 893, 896, 897, 900, 901, 903, 904, 906, 907, 908, 909, 910, 912, 914, 915, 916, 917, 918, 920, 922, 924, 925, 926, 927, 928, 931, 933, 934, 936, 938, 939, 941, 942, 943, 944, 946, 948, 949, 950, 951, 952, 954, 955, 957, 958, 959, 960, 962, 964, 968, 971, 972, 973, 975, 976, 977, 978, 979, 984, 985, 988, 989, 991, 992, 993, 994, 995, 996, 997, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1010, 1011, 1014, 1018, 1019, 1022, 1023, 1024, 1027, 1030, 1033, 1034, 1035, 1037, 1038, 1039, 1040, 1041, 1042, 1043, 1044, 1046, 1047, 1048, 1049, 1051, 1054, 1055, 1056, 1058, 1059, 1060, 1063, 1064, 1065, 1066, 1068, 1070, 1071, 1073, 1074, 1076, 1078, 1079, 1080, 1083, 1085, 1086, 1091, 1093, 1095, 1096, 1099, 1100, 1101, 1103, 1104, 1105, 1106, 1108, 1112, 1114, 1115, 1116, 1117, 1118, 1119, 1121, 1122, 1123, 1124, 1125, 1126, 1128, 1129, 1130, 1131, 1132, 1134, 1137, 1139, 1140, 1141, 1142, 1143, 1148, 1149, 1150, 1151, 1152, 1153, 1154, 1155, 1156, 1158, 1160, 1161, 1162, 1163, 1165, 1170, 1171, 1173, 1174, 1175, 1176, 1177, 1181, 1182, 1184, 1186, 1187, 1188, 1189, 1190, 1193, 1194, 1196, 1197, 1199, 1200, 1201, 1202, 1203, 1204, 1206, 1211, 1212, 1213, 1214, 1215, 1219, 1221, 1224, 1226, 1227, 1229, 1230, 1232, 1236, 1237, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1246, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1255, 1257, 1258, 1259, 1262, 1263, 1266, 1268, 1271, 1273, 1274, 1275, 1276, 1277, 1281, 1282, 1284, 1285, 1286, 1287, 1288, 1290, 1294, 1295, 1296, 1298, 1299, 1302, 1304, 1305, 1306, 1307, 1308, 1309, 1310, 1311, 1316, 1317, 1318, 1321, 1322, 1325, 1327, 1328, 1329, 1330, 1335, 1337, 1340, 1342, 1343, 1346, 1349, 1350, 1351, 1353, 1355, 1356, 1359, 1361, 1364, 1366, 1369, 1370, 1371, 1372, 1373, 1375, 1376, 1380, 1381, 1382, 1387, 1388, 1390, 1392, 1393, 1399, 1400, 1405, 1406, 1409, 1411, 1412, 1415, 1418, 1419, 1420, 1421, 1422, 1423, 1424, 1425, 1427, 1428, 1429, 1430, 1434, 1435, 1437, 1439, 1440, 1441, 1442, 1443, 1445, 1446, 1451, 1453, 1454, 1456, 1459, 1460, 1461, 1462, 1463, 1464, 1466, 1470, 1471, 1472, 1473, 1475, 1476, 1477, 1478, 1480, 1481, 1482, 1483, 1484, 1485, 1487, 1488, 1490, 1492, 1493, 1495, 1497, 1501, 1502, 1503, 1505, 1507, 1508, 1510, 1511, 1513, 1514, 1515, 1517, 1521, 1522, 1525, 1526, 1527, 1528, 1530, 1531, 1533, 1534, 1535, 1536, 1537, 1542, 1543, 1544, 1546, 1547, 1550, 1551, 1553, 1554, 1555, 1556, 1557, 1560, 1561, 1562, 1563, 1565, 1567, 1568, 1569, 1570, 1573, 1575, 1576, 1579, 1580, 1581, 1582, 1587, 1588, 1589, 1590, 1591, 1592, 1594, 1595, 1596, 1597, 1598, 1599, 1600, 1601, 1602, 1603, 1606, 1607, 1609, 1611, 1613, 1614, 1616, 1617, 1620, 1621, 1622, 1624, 1625, 1626, 1627, 1628, 1630, 1633, 1634, 1635, 1636, 1637, 1638, 1643, 1645, 1646, 1648, 1650, 1652, 1656, 1658, 1660, 1663, 1664, 1666, 1667, 1671, 1672, 1673, 1676, 1677, 1678, 1680, 1682, 1683, 1685, 1687, 1688, 1689, 1690, 1691, 1693, 1694, 1695, 1697, 1698, 1699, 1702, 1704, 1705, 1708, 1710, 1711, 1713, 1714, 1715, 1717, 1719, 1721, 1722, 1724, 1726, 1727, 1728, 1730, 1731, 1732, 1733, 1734, 1735, 1736, 1737, 1738, 1741, 1742, 1743, 1744, 1745, 1748, 1750, 1751, 1752, 1753, 1754, 1755, 1757, 1758, 1759, 1763, 1764, 1765, 1766, 1767, 1768, 1769, 1770, 1772, 1773, 1774, 1775, 1777, 1779, 1781, 1782, 1785, 1786, 1787, 1788, 1789, 1790, 1791, 1793, 1794, 1801, 1803, 1804, 1808, 1810, 1811, 1812, 1813, 1814, 1817, 1818, 1819, 1820, 1821, 1822, 1824, 1825, 1826, 1827, 1829, 1830, 1833, 1836, 1838, 1839, 1842, 1843, 1844, 1847, 1848, 1849, 1851, 1852, 1854, 1855, 1856, 1858, 1860, 1861, 1864, 1865, 1868, 1870, 1871, 1872, 1874, 1880, 1881, 1883, 1885, 1886, 1890, 1891, 1892, 1894, 1896, 1897, 1898, 1900, 1902, 1903, 1905, 1908, 1910, 1911, 1913, 1915, 1917, 1919, 1920, 1921, 1922, 1923, 1924, 1926, 1927, 1928, 1929, 1930, 1933, 1934, 1936, 1937, 1938, 1939, 1940, 1941, 1943, 1945, 1946, 1948, 1950, 1951, 1952, 1954, 1955, 1956, 1957, 1962, 1964, 1966, 1968, 1970, 1971, 1974, 1975, 1977, 1978, 1979, 1980, 1982, 1985, 1986, 1987, 1988, 1990, 1992, 1994, 1995, 1996, 1999, 2000, 2001, 2003, 2006, 2007, 2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2019, 2020, 2021, 2022, 2023, 2024, 2025, 2026, 2031, 2032, 2034, 2035, 2036, 2037, 2039, 2041, 2042, 2043, 2046, 2048, 2051, 2053, 2054, 2055, 2056, 2057, 2060, 2061, 2067, 2068, 2069, 2070, 2073, 2074, 2076, 2078, 2079, 2080, 2081, 2084, 2085, 2087, 2089, 2090, 2091, 2092, 2093, 2094, 2096, 2097, 2098, 2099, 2101, 2102, 2103, 2104, 2105, 2108, 2109, 2110, 2112, 2113, 2114, 2115, 2116, 2117, 2120, 2121, 2123, 2124, 2125, 2126, 2127, 2129, 2130, 2133, 2134, 2136, 2138, 2142, 2144, 2146, 2147, 2148, 2150, 2153, 2157, 2160, 2162, 2163, 2164, 2165, 2167, 2168, 2171, 2174, 2178, 2180, 2181, 2182, 2185, 2188, 2189, 2190, 2192, 2194, 2196, 2197, 2198, 2199, 2200, 2201, 2202, 2205, 2206, 2207, 2208, 2210, 2211, 2213, 2215, 2216, 2219, 2221, 2223, 2224, 2227, 2228, 2230, 2231, 2233, 2234, 2235, 2237, 2238, 2241, 2243, 2244, 2245, 2248, 2249, 2251, 2252, 2258, 2261, 2263, 2264, 2265, 2267, 2269, 2271, 2272, 2273, 2278, 2280, 2281, 2282, 2283, 2284, 2286, 2287, 2291, 2292, 2293, 2295, 2298, 2299, 2300, 2302, 2303, 2304, 2306, 2307, 2308, 2309, 2311, 2312, 2313, 2314, 2316, 2317, 2318, 2320, 2321, 2322, 2324, 2327, 2328, 2329, 2330, 2331, 2332, 2333, 2334, 2335, 2336, 2337, 2340, 2347, 2348, 2349, 2351, 2352, 2356, 2357, 2358, 2359, 2361, 2362, 2365, 2367, 2368, 2369, 2370, 2378, 2381, 2382, 2383, 2386, 2389, 2390, 2391, 2392, 2393, 2394, 2395, 2396, 2397, 2398, 2401, 2403, 2404, 2405, 2408, 2410, 2411, 2413, 2414, 2415, 2416, 2418, 2420, 2421, 2422, 2423, 2424, 2425, 2426, 2427, 2429, 2430, 2432, 2433, 2434, 2435, 2436, 2437, 2438, 2439, 2440, 2445, 2447, 2448, 2451, 2453, 2454, 2455, 2457, 2458, 2462, 2466, 2467, 2469, 2470, 2473, 2474, 2475, 2476, 2478, 2483, 2484, 2485, 2486, 2487, 2488, 2490, 2491, 2492, 2494, 2496, 2498, 2499, 2501, 2503, 2504, 2505, 2506, 2507, 2509, 2511, 2513, 2514, 2516, 2517, 2518, 2519, 2522, 2523, 2524, 2526, 2527, 2529, 2532, 2535, 2536, 2537, 2538, 2539, 2540, 2542, 2543, 2544, 2545, 2548, 2551, 2552, 2553, 2555, 2558, 2560, 2562, 2565, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2573, 2577, 2578, 2580, 2582, 2583, 2584, 2585, 2587, 2588, 2589, 2590, 2591, 2592, 2593, 2594, 2596, 2597, 2601, 2604, 2606, 2607, 2608, 2609, 2610, 2614, 2615, 2616, 2617, 2619, 2620, 2624, 2625, 2628, 2629, 2635, 2636, 2637, 2638, 2639, 2640, 2641, 2643, 2645, 2646, 2648, 2650, 2651, 2652, 2654, 2657, 2658, 2659, 2661, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2676, 2677, 2678, 2682, 2683, 2684, 2685, 2686, 2689, 2690, 2692, 2693, 2696, 2697, 2698, 2700, 2701, 2702, 2703, 2704, 2705, 2706]\n",
            "[0, 1, 4, 5, 6, 8, 11, 12, 14, 16, 19, 20, 21, 23, 25, 27, 30, 33, 34, 35, 36, 37, 40, 42, 43, 44, 45, 47, 48, 50, 51, 52, 53, 56, 57, 58, 59, 61, 62, 63, 64, 65, 67, 68, 70, 71, 72, 74, 75, 76, 77, 79, 81, 82, 84, 87, 88, 93, 96, 97, 98, 99, 100, 101, 102, 106, 108, 110, 111, 112, 114, 118, 119, 121, 122, 123, 124, 129, 130, 131, 132, 133, 136, 137, 142, 143, 144, 145, 146, 147, 149, 150, 151, 152, 153, 154, 155, 156, 157, 158, 161, 162, 164, 165, 168, 169, 171, 173, 174, 177, 178, 179, 181, 182, 185, 186, 187, 190, 191, 194, 195, 197, 198, 199, 202, 203, 204, 206, 209, 212, 219, 220, 221, 223, 226, 227, 228, 229, 230, 235, 237, 238, 239, 240, 241, 242, 245, 246, 247, 249, 252, 253, 254, 255, 257, 261, 265, 266, 267, 270, 271, 272, 273, 274, 275, 276, 279, 280, 281, 284, 285, 286, 289, 294, 295, 296, 297, 298, 301, 303, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 316, 317, 319, 320, 323, 324, 325, 326, 327, 328, 329, 332, 333, 335, 337, 338, 339, 340, 342, 343, 347, 348, 350, 351, 352, 355, 356, 359, 360, 363, 365, 366, 368, 369, 370, 372, 373, 374, 375, 377, 379, 381, 384, 386, 388, 389, 390, 391, 392, 395, 399, 400, 401, 402, 403, 404, 408, 409, 412, 418, 420, 423, 425, 426, 427, 428, 430, 431, 432, 434, 435, 438, 439, 440, 442, 443, 444, 446, 448, 449, 451, 452, 454, 455, 456, 457, 461, 462, 463, 464, 465, 466, 470, 471, 473, 474, 475, 476, 479, 480, 483, 484, 485, 487, 488, 489, 490, 495, 496, 497, 499, 500, 501, 502, 503, 504, 505, 506, 507, 508, 509, 511, 512, 514, 515, 517, 518, 519, 521, 522, 523, 526, 530, 532, 533, 534, 537, 538, 541, 542, 543, 546, 548, 549, 550, 551, 552, 553, 554, 555, 556, 559, 561, 562, 563, 565, 566, 567, 568, 569, 570, 572, 574, 575, 576, 577, 578, 579, 580, 582, 584, 586, 587, 588, 589, 596, 597, 598, 600, 601, 603, 604, 605, 607, 608, 610, 612, 614, 615, 616, 617, 619, 621, 622, 623, 624, 625, 626, 629, 632, 633, 636, 638, 639, 640, 644, 645, 651, 653, 654, 655, 656, 660, 662, 665, 667, 668, 669, 671, 673, 675, 676, 679, 682, 683, 684, 685, 688, 689, 690, 692, 693, 694, 696, 699, 700, 704, 705, 706, 708, 710, 711, 712, 713, 714, 715, 716, 717, 718, 720, 722, 724, 726, 727, 728, 729, 731, 732, 734, 736, 740, 741, 743, 744, 745, 747, 748, 749, 750, 751, 752, 754, 756, 757, 758, 760, 761, 762, 763, 766, 767, 768, 769, 770, 771, 772, 774, 775, 776, 781, 783, 784, 785, 786, 787, 793, 794, 795, 797, 799, 801, 802, 803, 805, 807, 808, 809, 810, 811, 812, 813, 814, 816, 817, 819, 820, 822, 823, 825, 826, 827, 828, 830, 832, 833, 836, 837, 838, 839, 840, 841, 842, 843, 845, 847, 849, 850, 851, 852, 855, 856, 859, 860, 862, 863, 866, 867, 868, 869, 870, 871, 873, 876, 877, 878, 880, 881, 883, 884, 888, 889, 890, 893, 896, 898, 899, 901, 903, 905, 906, 908, 910, 912, 915, 917, 918, 919, 921, 924, 925, 926, 928, 929, 930, 931, 932, 933, 934, 937, 938, 943, 945, 946, 947, 948, 949, 950, 951, 953, 954, 955, 956, 957, 960, 961, 962, 963, 965, 967, 969, 970, 972, 974, 975, 976, 977, 978, 980, 981, 983, 984, 986, 987, 989, 990, 992, 993, 994, 995, 999, 1000, 1003, 1005, 1006, 1008, 1010, 1013, 1016, 1018, 1019, 1021, 1022, 1023, 1025, 1026, 1027, 1028, 1029, 1030, 1031, 1032, 1035, 1038, 1040, 1041, 1042, 1043, 1044, 1045, 1046, 1047, 1049, 1051, 1054, 1055, 1056, 1059, 1060, 1061, 1062, 1065, 1068, 1069, 1071, 1072, 1074, 1075, 1076, 1077, 1079, 1080, 1083, 1084, 1090, 1092, 1093, 1096, 1103, 1104, 1105, 1107, 1109, 1110, 1114, 1115, 1120, 1122, 1123, 1124, 1125, 1126, 1128, 1129, 1130, 1131, 1132, 1133, 1135, 1136, 1138, 1139, 1140, 1141, 1143, 1144, 1145, 1148, 1150, 1151, 1153, 1154, 1156, 1157, 1158, 1160, 1162, 1163, 1164, 1165, 1168, 1169, 1175, 1176, 1179, 1180, 1183, 1184, 1185, 1187, 1188, 1189, 1191, 1193, 1194, 1195, 1196, 1199, 1200, 1202, 1204, 1207, 1209, 1210, 1211, 1212, 1213, 1214, 1215, 1216, 1217, 1220, 1223, 1224, 1225, 1226, 1227, 1228, 1229, 1230, 1234, 1240, 1241, 1243, 1247, 1248, 1249, 1250, 1251, 1252, 1253, 1254, 1256, 1259, 1260, 1263, 1264, 1265, 1266, 1267, 1268, 1269, 1271, 1272, 1273, 1274, 1275, 1276, 1277, 1278, 1279, 1280, 1281, 1282, 1284, 1286, 1287, 1288, 1290, 1291, 1293, 1294, 1296, 1297, 1298, 1300, 1301, 1302, 1303, 1307, 1310, 1312, 1313, 1315, 1317, 1320, 1321, 1322, 1324, 1328, 1329, 1330, 1331, 1333, 1335, 1336, 1338, 1339, 1340, 1342, 1344, 1347, 1348, 1350, 1353, 1356, 1357, 1359, 1361, 1363, 1365, 1366, 1367, 1368, 1371, 1372, 1374, 1375, 1376, 1378, 1379, 1380, 1381, 1382, 1383, 1384, 1386, 1387, 1388, 1389, 1390, 1391, 1395, 1398, 1399, 1401, 1403, 1404, 1405, 1409, 1410, 1412, 1414, 1415, 1416, 1418, 1419, 1421, 1423, 1424, 1425, 1426, 1427, 1428, 1429, 1431, 1432, 1433, 1435, 1437, 1439, 1440, 1442, 1443, 1444, 1445, 1446, 1448, 1450, 1458, 1459, 1461, 1462, 1463, 1464, 1465, 1467, 1469, 1471, 1472, 1473, 1474, 1476, 1477, 1479, 1481, 1483, 1486, 1490, 1491, 1493, 1494, 1495, 1496, 1498, 1499, 1501, 1503, 1504, 1505, 1506, 1511, 1512, 1513, 1514, 1515, 1516, 1517, 1518, 1520, 1523, 1524, 1525, 1526, 1527, 1528, 1530, 1533, 1537, 1538, 1540, 1542, 1543, 1545, 1546, 1547, 1548, 1552, 1553, 1555, 1556, 1558, 1559, 1560, 1562, 1563, 1564, 1565, 1567, 1570, 1571, 1573, 1575, 1576, 1577, 1578, 1579, 1582, 1583, 1584, 1586, 1587, 1589, 1590, 1591, 1593, 1594, 1596, 1598, 1599, 1600, 1601, 1603, 1605, 1606, 1607, 1608, 1609, 1611, 1612, 1613, 1615, 1616, 1617, 1618, 1619, 1621, 1622, 1623, 1624, 1625, 1626, 1627, 1629, 1630, 1631, 1632, 1633, 1634, 1636, 1637, 1638, 1641, 1643, 1647, 1648, 1649, 1651, 1653, 1654, 1656, 1660, 1661, 1663, 1665, 1669, 1672, 1673, 1674, 1675, 1676, 1677, 1678, 1679, 1680, 1681, 1682, 1685, 1687, 1689, 1691, 1692, 1693, 1694, 1695, 1696, 1697, 1698, 1700, 1701, 1702, 1703, 1704, 1706, 1709, 1710, 1711, 1712, 1713, 1714, 1716, 1717, 1718, 1719, 1722, 1723, 1725, 1726, 1727, 1728, 1729, 1730, 1732, 1733, 1734, 1736, 1737, 1738, 1739, 1740, 1741, 1742, 1744, 1745, 1746, 1747, 1749, 1751, 1752, 1753, 1755, 1756, 1757, 1758, 1759, 1761, 1762, 1768, 1770, 1772, 1773, 1774, 1776, 1777, 1778, 1779, 1781, 1782, 1783, 1784, 1786, 1787, 1789, 1791, 1793, 1799, 1800, 1802, 1803, 1804, 1806, 1807, 1808, 1809, 1812, 1814, 1815, 1817, 1819, 1820, 1821, 1822, 1823, 1826, 1828, 1829, 1830, 1831, 1832, 1833, 1834, 1836, 1837, 1838, 1839, 1841, 1842, 1843, 1845, 1849, 1850, 1851, 1852, 1854, 1855, 1856, 1857, 1858, 1859, 1861, 1862, 1863, 1864, 1865, 1866, 1868, 1869, 1870, 1871, 1873, 1874, 1875, 1876, 1877, 1879, 1881, 1882, 1883, 1885, 1886, 1888, 1889, 1890, 1891, 1892, 1896, 1898, 1899, 1900, 1902, 1909, 1910, 1912, 1913, 1916, 1917, 1918, 1919, 1920, 1921, 1922, 1923, 1925, 1926, 1927, 1928, 1930, 1931, 1932, 1933, 1935, 1937, 1939, 1942, 1943, 1945, 1946, 1948, 1949, 1951, 1953, 1954, 1956, 1957, 1958, 1959, 1960, 1962, 1965, 1966, 1967, 1969, 1970, 1971, 1973, 1975, 1977, 1979, 1980, 1982, 1983, 1985, 1993, 1994, 1995, 1996, 1999, 2002, 2004, 2006, 2008, 2009, 2012, 2014, 2015, 2016, 2018, 2022, 2024, 2025, 2026, 2027, 2030, 2032, 2033, 2035, 2036, 2039, 2040, 2041, 2042, 2043, 2045, 2050, 2051, 2053, 2054, 2055, 2056, 2057, 2058, 2060, 2063, 2064, 2065, 2066, 2069, 2072, 2073, 2074, 2075, 2076, 2077, 2078, 2079, 2080, 2082, 2083, 2085, 2086, 2087, 2088, 2089, 2091, 2093, 2094, 2097, 2102, 2103, 2108, 2109, 2111, 2112, 2113, 2114, 2116, 2119, 2122, 2123, 2124, 2125, 2126, 2129, 2132, 2134, 2136, 2137, 2138, 2139, 2140, 2141, 2142, 2143, 2147, 2148, 2149, 2151, 2152, 2153, 2154, 2156, 2158, 2159, 2160, 2161, 2162, 2164, 2166, 2167, 2168, 2170, 2171, 2172, 2173, 2174, 2176, 2178, 2179, 2180, 2181, 2182, 2183, 2184, 2185, 2187, 2188, 2189, 2190, 2191, 2192, 2196, 2201, 2202, 2204, 2205, 2206, 2207, 2208, 2210, 2214, 2215, 2218, 2220, 2224, 2228, 2229, 2230, 2231, 2233, 2234, 2235, 2236, 2237, 2239, 2242, 2246, 2247, 2248, 2249, 2251, 2252, 2253, 2254, 2255, 2258, 2259, 2261, 2262, 2264, 2267, 2268, 2269, 2271, 2273, 2274, 2276, 2277, 2278, 2279, 2280, 2281, 2283, 2284, 2285, 2291, 2292, 2294, 2296, 2297, 2298, 2299, 2301, 2302, 2304, 2305, 2306, 2307, 2308, 2309, 2310, 2312, 2313, 2315, 2316, 2317, 2318, 2319, 2320, 2322, 2324, 2325, 2326, 2329, 2331, 2333, 2335, 2337, 2338, 2339, 2340, 2341, 2342, 2344, 2345, 2346, 2351, 2352, 2353, 2354, 2355, 2357, 2358, 2360, 2363, 2366, 2368, 2370, 2372, 2374, 2375, 2376, 2377, 2380, 2381, 2384, 2385, 2387, 2391, 2393, 2395, 2396, 2397, 2398, 2401, 2402, 2403, 2404, 2405, 2406, 2407, 2408, 2410, 2416, 2417, 2419, 2420, 2423, 2427, 2428, 2429, 2430, 2432, 2434, 2435, 2436, 2438, 2440, 2442, 2443, 2445, 2446, 2448, 2450, 2453, 2458, 2459, 2461, 2462, 2464, 2465, 2466, 2467, 2469, 2470, 2473, 2474, 2475, 2476, 2477, 2479, 2480, 2481, 2482, 2483, 2485, 2486, 2487, 2488, 2489, 2491, 2492, 2493, 2494, 2495, 2496, 2498, 2499, 2501, 2504, 2505, 2506, 2507, 2509, 2510, 2511, 2514, 2515, 2516, 2517, 2518, 2519, 2520, 2524, 2525, 2527, 2528, 2530, 2531, 2532, 2534, 2535, 2538, 2540, 2543, 2544, 2545, 2547, 2548, 2549, 2550, 2551, 2554, 2555, 2556, 2558, 2559, 2560, 2561, 2562, 2563, 2564, 2565, 2566, 2568, 2569, 2570, 2571, 2572, 2573, 2574, 2575, 2577, 2578, 2581, 2582, 2583, 2584, 2585, 2589, 2592, 2593, 2594, 2595, 2597, 2600, 2601, 2602, 2603, 2605, 2606, 2607, 2608, 2609, 2610, 2612, 2613, 2614, 2616, 2618, 2619, 2620, 2621, 2622, 2623, 2624, 2626, 2630, 2631, 2633, 2634, 2640, 2641, 2642, 2643, 2644, 2648, 2649, 2650, 2651, 2653, 2654, 2655, 2657, 2658, 2659, 2660, 2663, 2664, 2665, 2666, 2667, 2668, 2669, 2670, 2671, 2672, 2673, 2674, 2676, 2678, 2679, 2681, 2683, 2684, 2685, 2686, 2689, 2690, 2691, 2693, 2694, 2695, 2696, 2698, 2700, 2702, 2704, 2705, 2706]\n",
            "[1, 2, 3, 4, 5, 6, 9, 10, 11, 12, 13, 14, 17, 18, 19, 21, 22, 24, 26, 28, 29, 32, 33, 34, 35, 38, 39, 41, 44, 45, 46, 48, 49, 50, 52, 54, 55, 57, 58, 59, 60, 61, 62, 65, 69, 71, 72, 73, 74, 75, 77, 78, 84, 87, 88, 91, 94, 95, 96, 97, 99, 103, 104, 108, 110, 111, 112, 114, 116, 117, 119, 121, 122, 124, 125, 126, 127, 128, 130, 131, 132, 133, 139, 140, 142, 144, 145, 147, 150, 151, 153, 154, 155, 158, 160, 161, 162, 163, 166, 167, 168, 169, 172, 176, 177, 178, 179, 180, 182, 183, 184, 185, 186, 187, 188, 194, 195, 196, 200, 201, 202, 203, 204, 205, 208, 210, 211, 212, 213, 214, 215, 216, 219, 220, 221, 222, 223, 224, 225, 227, 229, 230, 231, 232, 235, 237, 241, 242, 243, 244, 246, 248, 249, 251, 253, 255, 256, 257, 258, 259, 261, 263, 264, 265, 266, 267, 269, 273, 276, 278, 280, 282, 285, 287, 288, 289, 291, 294, 295, 296, 297, 298, 300, 301, 302, 305, 306, 307, 308, 309, 310, 311, 312, 313, 314, 315, 316, 318, 325, 326, 327, 328, 329, 331, 332, 333, 334, 335, 336, 337, 338, 339, 340, 341, 342, 347, 348, 349, 350, 351, 355, 356, 360, 361, 362, 363, 364, 365, 366, 368, 369, 370, 371, 372, 373, 374, 376, 377, 378, 379, 380, 381, 382, 383, 385, 386, 387, 389, 390, 391, 392, 395, 397, 400, 401, 402, 403, 404, 406, 407, 410, 412, 415, 416, 417, 418, 422, 423, 424, 428, 429, 430, 431, 432, 433, 434, 436, 437, 438, 439, 440, 441, 442, 443, 445, 446, 449, 451, 452, 453, 454, 455, 456, 457, 458, 459, 460, 461, 462, 463, 464, 466, 469, 470, 474, 475, 477, 478, 480, 481, 483, 484, 485, 486, 488, 492, 493, 494, 496, 497, 498, 501, 503, 504, 506, 508, 509, 510, 512, 513, 515, 516, 517, 519, 521, 523, 524, 528, 530, 531, 532, 533, 534, 535, 536, 537, 538, 539, 540, 542, 544, 545, 546, 547, 548, 549, 550, 552, 553, 554, 557, 559, 560, 561, 562, 563, 564, 565, 568, 569, 570, 573, 574, 576, 578, 580, 581, 584, 585, 586, 589, 591, 592, 593, 594, 595, 596, 597, 598, 600, 603, 604, 605, 608, 613, 615, 616, 618, 619, 620, 621, 623, 624, 625, 626, 629, 630, 631, 633, 634, 635, 636, 637, 638, 639, 640, 641, 642, 644, 648, 649, 650, 655, 656, 658, 662, 663, 665, 666, 667, 668, 669, 670, 672, 673, 674, 675, 677, 678, 679, 680, 681, 684, 686, 687, 688, 691, 692, 694, 696, 700, 701, 702, 703, 706, 708, 709, 710, 712, 714, 715, 718, 719, 720, 721, 723, 724, 725, 726, 728, 729, 732, 733, 734, 737, 739, 740, 743, 744, 745, 746, 747, 749, 750, 752, 753, 756, 758, 759, 762, 763, 764, 766, 767, 770, 771, 772, 773, 777, 778, 779, 780, 781, 784, 785, 786, 787, 788, 789, 790, 791, 793, 794, 796, 797, 798, 799, 800, 801, 804, 805, 808, 809, 810, 816, 819, 822, 827, 828, 829, 830, 831, 833, 835, 839, 840, 841, 842, 846, 847, 849, 850, 851, 854, 856, 859, 860, 862, 863, 864, 866, 867, 869, 871, 872, 873, 874, 876, 879, 880, 881, 887, 889, 890, 891, 892, 893, 894, 895, 896, 897, 898, 900, 901, 902, 903, 908, 909, 910, 912, 913, 914, 915, 918, 919, 922, 923, 924, 925, 928, 938, 939, 941, 942, 945, 946, 947, 948, 949, 950, 951, 955, 956, 957, 958, 960, 961, 962, 967, 968, 969, 971, 972, 974, 975, 977, 979, 980, 982, 983, 984, 985, 986, 987, 988, 991, 993, 994, 995, 996, 998, 999, 1000, 1001, 1002, 1003, 1004, 1005, 1007, 1008, 1011, 1013, 1014, 1015, 1018, 1019, 1021, 1023, 1025, 1027, 1028, 1029, 1030, 1031, 1032, 1033, 1034, 1035, 1037, 1038, 1040, 1042, 1043, 1045, 1047, 1048, 1049, 1050, 1051, 1052, 1054, 1058, 1060, 1061, 1063, 1064, 1071, 1072, 1073, 1076, 1081, 1082, 1084, 1085, 1087, 1088, 1089, 1091, 1094, 1095, 1097, 1098, 1099, 1101, 1102, 1103, 1104, 1106, 1107, 1108, 1109, 1110, 1113, 1114, 1116, 1117, 1118, 1119, 1120, 1124, 1125, 1126, 1127, 1130, 1131, 1133, 1135, 1139, 1140, 1141, 1145, 1146, 1148, 1149, 1150, 1152, 1153, 1154, 1156, 1158, 1161, 1164, 1165, 1168, 1169, 1170, 1171, 1173, 1174, 1175, 1177, 1178, 1180, 1182, 1183, 1184, 1186, 1187, 1189, 1195, 1196, 1197, 1200, 1201, 1202, 1203, 1205, 1206, 1209, 1210, 1212, 1213, 1214, 1215, 1217, 1218, 1219, 1222, 1223, 1224, 1226, 1227, 1228, 1229, 1231, 1233, 1234, 1235, 1236, 1237, 1238, 1239, 1240, 1241, 1242, 1243, 1244, 1245, 1250, 1252, 1253, 1254, 1255, 1256, 1257, 1258, 1259, 1260, 1261, 1263, 1266, 1267, 1268, 1270, 1272, 1273, 1274, 1275, 1278, 1281, 1285, 1286, 1290, 1291, 1292, 1294, 1296, 1297, 1299, 1301, 1303, 1304, 1305, 1307, 1311, 1312, 1313, 1314, 1315, 1316, 1317, 1318, 1325, 1326, 1328, 1333, 1335, 1338, 1339, 1340, 1341, 1343, 1344, 1346, 1348, 1352, 1353, 1355, 1356, 1357, 1358, 1359, 1360, 1361, 1362, 1364, 1365, 1367, 1369, 1370, 1372, 1374, 1375, 1377, 1380, 1382, 1383, 1384, 1385, 1386, 1387, 1388, 1389, 1391, 1392, 1394, 1395, 1398, 1400, 1405, 1407, 1408, 1409, 1411, 1413, 1414, 1420, 1424, 1425, 1426, 1428, 1429, 1430, 1432, 1433, 1434, 1435, 1436, 1438, 1439, 1440, 1441, 1442, 1443, 1444, 1445, 1446, 1449, 1450, 1452, 1453, 1454, 1456, 1457, 1458, 1459, 1460, 1461, 1463, 1466, 1469, 1470, 1471, 1473, 1474, 1476, 1477, 1479, 1481, 1482, 1483, 1484, 1487, 1488, 1489, 1490, 1491, 1493, 1494, 1495, 1496, 1499, 1500, 1502, 1504, 1506, 1507, 1509, 1511, 1512, 1513, 1514, 1516, 1519, 1520, 1522, 1523, 1524, 1525, 1526, 1527, 1528, 1531, 1532, 1534, 1536, 1538, 1540, 1541, 1542, 1543, 1544, 1545, 1546, 1547, 1549, 1550, 1552, 1554, 1555, 1556, 1558, 1559, 1560, 1561, 1562, 1563, 1564, 1565, 1566, 1567, 1568, 1570, 1571, 1572, 1573, 1577, 1582, 1583, 1584, 1585, 1587, 1588, 1589, 1591, 1593, 1594, 1595, 1596, 1598, 1599, 1600, 1602, 1603, 1604, 1606, 1608, 1609, 1611, 1612, 1613, 1617, 1618, 1619, 1621, 1623, 1624, 1625, 1626, 1628, 1630, 1633, 1635, 1637, 1638, 1640, 1643, 1644, 1645, 1647, 1648, 1649, 1652, 1653, 1655, 1657, 1659, 1660, 1665, 1666, 1668, 1669, 1670, 1672, 1673, 1674, 1675, 1676, 1677, 1679, 1682, 1690, 1692, 1693, 1696, 1697, 1698, 1699, 1701, 1703, 1704, 1706, 1708, 1709, 1710, 1711, 1712, 1714, 1715, 1716, 1719, 1720, 1721, 1724, 1728, 1729, 1730, 1731, 1735, 1737, 1738, 1739, 1740, 1743, 1745, 1746, 1748, 1749, 1750, 1751, 1754, 1755, 1756, 1757, 1758, 1759, 1760, 1761, 1762, 1763, 1764, 1765, 1767, 1769, 1770, 1771, 1772, 1773, 1774, 1776, 1777, 1778, 1780, 1781, 1782, 1786, 1788, 1792, 1793, 1794, 1795, 1796, 1797, 1798, 1799, 1800, 1801, 1804, 1805, 1806, 1807, 1808, 1809, 1811, 1813, 1816, 1817, 1826, 1827, 1830, 1831, 1832, 1836, 1837, 1838, 1839, 1840, 1841, 1842, 1844, 1850, 1851, 1852, 1853, 1855, 1856, 1859, 1860, 1861, 1862, 1863, 1864, 1865, 1867, 1868, 1869, 1870, 1871, 1872, 1873, 1875, 1877, 1879, 1882, 1883, 1884, 1886, 1888, 1889, 1890, 1891, 1892, 1893, 1894, 1895, 1896, 1899, 1901, 1902, 1909, 1910, 1911, 1912, 1913, 1914, 1917, 1918, 1920, 1921, 1922, 1923, 1924, 1925, 1926, 1929, 1931, 1932, 1933, 1935, 1941, 1944, 1945, 1947, 1948, 1949, 1950, 1952, 1953, 1954, 1956, 1957, 1958, 1959, 1960, 1962, 1964, 1968, 1970, 1971, 1973, 1974, 1975, 1977, 1978, 1983, 1985, 1986, 1989, 1990, 1993, 1995, 1996, 1997, 1999, 2000, 2003, 2004, 2005, 2007, 2008, 2010, 2011, 2013, 2014, 2015, 2016, 2017, 2018, 2020, 2021, 2022, 2024, 2026, 2027, 2028, 2029, 2030, 2031, 2035, 2036, 2037, 2038, 2040, 2043, 2045, 2046, 2047, 2048, 2049, 2050, 2051, 2052, 2055, 2056, 2057, 2058, 2061, 2062, 2064, 2066, 2067, 2068, 2069, 2072, 2075, 2076, 2077, 2079, 2080, 2081, 2082, 2084, 2087, 2089, 2090, 2091, 2092, 2093, 2095, 2096, 2097, 2098, 2099, 2100, 2101, 2103, 2104, 2105, 2106, 2108, 2112, 2115, 2116, 2117, 2119, 2121, 2122, 2123, 2124, 2125, 2130, 2132, 2135, 2137, 2138, 2139, 2141, 2143, 2145, 2146, 2147, 2148, 2151, 2154, 2155, 2156, 2157, 2159, 2162, 2163, 2165, 2166, 2167, 2169, 2170, 2171, 2172, 2175, 2178, 2179, 2180, 2182, 2183, 2184, 2185, 2186, 2187, 2188, 2189, 2191, 2192, 2193, 2194, 2198, 2199, 2200, 2202, 2203, 2206, 2207, 2208, 2209, 2210, 2211, 2212, 2214, 2215, 2217, 2218, 2219, 2220, 2221, 2222, 2225, 2226, 2227, 2228, 2229, 2230, 2231, 2232, 2233, 2234, 2236, 2237, 2238, 2241, 2242, 2243, 2244, 2248, 2249, 2251, 2252, 2255, 2258, 2260, 2264, 2265, 2266, 2267, 2268, 2270, 2272, 2273, 2275, 2276, 2277, 2278, 2279, 2280, 2281, 2282, 2284, 2291, 2293, 2296, 2298, 2300, 2302, 2303, 2304, 2308, 2309, 2310, 2312, 2313, 2314, 2315, 2316, 2317, 2318, 2319, 2320, 2321, 2323, 2324, 2325, 2326, 2327, 2330, 2331, 2332, 2335, 2337, 2339, 2340, 2342, 2343, 2344, 2345, 2346, 2347, 2348, 2350, 2351, 2352, 2353, 2357, 2358, 2361, 2362, 2365, 2367, 2370, 2371, 2373, 2374, 2376, 2378, 2379, 2380, 2381, 2383, 2385, 2387, 2389, 2390, 2393, 2394, 2395, 2396, 2397, 2399, 2400, 2402, 2403, 2404, 2405, 2409, 2410, 2412, 2413, 2414, 2416, 2417, 2419, 2420, 2422, 2424, 2426, 2428, 2429, 2430, 2432, 2434, 2435, 2436, 2437, 2438, 2440, 2442, 2444, 2445, 2446, 2448, 2449, 2451, 2452, 2453, 2456, 2458, 2459, 2461, 2462, 2463, 2465, 2466, 2467, 2468, 2469, 2470, 2471, 2472, 2473, 2474, 2475, 2476, 2479, 2481, 2483, 2485, 2488, 2489, 2490, 2491, 2493, 2494, 2496, 2498, 2499, 2500, 2501, 2502, 2503, 2504, 2506, 2510, 2512, 2513, 2515, 2517, 2518, 2519, 2520, 2523, 2524, 2525, 2526, 2528, 2530, 2531, 2533, 2536, 2538, 2539, 2540, 2542, 2544, 2546, 2550, 2551, 2552, 2553, 2554, 2555, 2556, 2557, 2558, 2559, 2560, 2561, 2563, 2566, 2567, 2568, 2569, 2570, 2571, 2572, 2574, 2575, 2577, 2578, 2579, 2582, 2585, 2587, 2589, 2591, 2595, 2598, 2600, 2603, 2604, 2606, 2610, 2611, 2613, 2614, 2616, 2617, 2622, 2623, 2625, 2626, 2627, 2628, 2630, 2632, 2633, 2634, 2635, 2636, 2637, 2638, 2639, 2640, 2643, 2644, 2645, 2648, 2650, 2651, 2652, 2653, 2654, 2656, 2658, 2664, 2666, 2667, 2668, 2669, 2670, 2671, 2674, 2675, 2676, 2677, 2678, 2679, 2682, 2683, 2684, 2685, 2688, 2690, 2691, 2692, 2693, 2695, 2697, 2699, 2700, 2701, 2705, 2706]\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "train_splits = []\n",
        "test_splits = []\n",
        "for _ in range(10):\n",
        "  p=random.sample(range(len(catagories)), 1000)\n",
        "  #print(p)\n",
        "  q=[]\n",
        "  for i in range(n):\n",
        "      if i not in p:\n",
        "          q.append(i)\n",
        "  print(q)\n",
        "  train_splits.append(q)\n",
        "  test_splits.append(p)"
      ],
      "id": "c1ccc24d"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN5Dtz7oi8Wo",
        "outputId": "73ba1a21-e0da-4efd-aadc-667f448395c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[array([5, 2, 0, ..., 2, 7, 2]), array([7, 7, 7, ..., 7, 2, 2]), array([5, 7, 0, ..., 7, 2, 2]), array([5, 7, 0, ..., 2, 2, 2]), array([5, 7, 0, ..., 7, 7, 2]), array([7, 2, 0, ..., 2, 2, 2]), array([5, 2, 7, ..., 2, 2, 2]), array([7, 7, 0, ..., 2, 2, 7]), array([5, 2, 7, ..., 2, 2, 7]), array([7, 2, 0, ..., 2, 2, 7])]\n"
          ]
        }
      ],
      "source": [
        "class_60 = []\n",
        "if final_test:\n",
        "  class_60=[np.array(catagories) for _ in range(10)]\n",
        "  for i in range(10):\n",
        "    for d in test_splits[i]:\n",
        "      class_60[i][d]= max(catagories) + 1\n",
        "else:\n",
        "  class_60 = np.array(catagories)\n",
        "  for d in test_splits[0]:\n",
        "    class_60[d] = max(catagories) + 1\n",
        "print(class_60)"
      ],
      "id": "oN5Dtz7oi8Wo"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7a69f92"
      },
      "outputs": [],
      "source": [
        "from pandas.core.arrays.arrow import array\n",
        "from collections import Counter\n",
        "def create_feature_vectors(arr, max_cls):\n",
        "  return [[] for _ in range(n)]\n",
        "  Node_class= range(max_cls)\n",
        "  counts_out = [np.array([0 for cls in Node_class]) for _ in range(n)]\n",
        "  counts_in = [np.array([0 for cls in Node_class]) for _ in range(n)]\n",
        "  counts_2_out = [np.array([0 for cls in Node_class]) for _ in range(n)]\n",
        "  counts_2_in = [np.array([ 0 for cls in Node_class]) for _ in range(n)]\n",
        "  list_out = [set() for _ in range(n)]\n",
        "  list_in = [set() for _ in range(n)]\n",
        "  for i in range(n):\n",
        "    for j in range(n):\n",
        "      # print(j)\n",
        "      if arr[i][j]==1 and i!=j:\n",
        "        counts_out[i][arr[j][j]] += 1\n",
        "        list_out[i].add(j)\n",
        "      if arr[j][i]==1 and i!=j:\n",
        "        counts_in[i][arr[j][j]] += 1\n",
        "        list_in[i].add(j)\n",
        "  # print(list_in)\n",
        "  print(Counter([arr[i][i] for i in list_out[0]]))\n",
        "  # print(counts_in)\n",
        "  for i in range(n):\n",
        "    for j in list_out[i]:\n",
        "      counts_2_out[i] = np.add(counts_2_out[i], counts_out[j])\n",
        "    for j in list_in[i]:\n",
        "      counts_2_in[i] = np.add(counts_2_in[i], counts_in[j])\n",
        "    \n",
        "  F_vec = [[val for pair in zip(counts_out[i], counts_in[i])\n",
        "                for val in pair] + [val for pair in zip(counts_2_out[i], \n",
        "                                      counts_2_in[i]) for val in pair]\n",
        "           for i in range(n)]\n",
        "  return F_vec\n",
        "\n",
        "import csv\n",
        "def get_feature_vectors(feat_path):\n",
        "  with open(feat_path) as f:\n",
        "    F_vec = list(csv.reader(open(feat_path)))\n",
        "    F_vec = [[int(j) for j in F_vec[i][1:-2]] for i in range(1, len(F_vec))]\n",
        "    \n",
        "    return F_vec"
      ],
      "id": "e7a69f92"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fgkVsi8WZHJO",
        "outputId": "5ca48db1-51d5-42bb-83b7-52c66e530b83"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[array([1, 1, 3, ..., 1, 1, 2]),\n",
              " array([1, 1, 1, ..., 1, 1, 2]),\n",
              " array([1, 1, 1, ..., 1, 1, 2]),\n",
              " array([1, 1, 1, ..., 1, 1, 2]),\n",
              " array([1, 1, 1, ..., 1, 1, 2]),\n",
              " array([3, 1, 1, ..., 1, 1, 2]),\n",
              " array([1, 1, 1, ..., 1, 1, 2]),\n",
              " array([1, 1, 1, ..., 1, 1, 2]),\n",
              " array([1, 1, 1, ..., 1, 1, 2]),\n",
              " array([1, 1, 1, ..., 1, 1, 2])]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_60"
      ],
      "id": "fgkVsi8WZHJO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2ff12a40"
      },
      "outputs": [],
      "source": [
        "adj_mats = []\n",
        "adj = A.copy()\n",
        "# if final_test:\n",
        "#   for j in range(5):\n",
        "#     for i in range(n):\n",
        "#         adj[i][i]=class_60[j][i]\n",
        "#     adj_mats.append(adj.copy())\n",
        "\n",
        "if not final_test:\n",
        "  for i in range(n):\n",
        "    adj[i][i]=class_60[i]"
      ],
      "id": "2ff12a40"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6JWvYVy7Oddo",
        "outputId": "b4babc07-34c9-45e7-e16d-13277b3c98cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]]\n"
          ]
        }
      ],
      "source": [
        "print(adj)"
      ],
      "id": "6JWvYVy7Oddo"
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaling = StandardScaler()\n",
        "\n",
        "Scaled_data = data_s.drop('class', axis=1)\n",
        "\n",
        "scaling.fit(Scaled_data)\n",
        "Scaled_data = scaling.transform(Scaled_data)\n",
        "\n",
        "m = 50\n",
        "principal = PCA(n_components=m)\n",
        "principal.fit(Scaled_data)\n",
        "x = principal.transform(Scaled_data)\n",
        "\n",
        "print(x.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8eQItr4mfX93",
        "outputId": "68a56cc3-ff2b-4713-85de-074ca29e3354"
      },
      "id": "8eQItr4mfX93",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2708, 50)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Scaled_data = x"
      ],
      "metadata": {
        "id": "gq2HI024fdke"
      },
      "id": "gq2HI024fdke",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VW4pwGdqv-24"
      },
      "outputs": [],
      "source": [
        "def train_model(F_vec, p, q, earlystop1, earlystop2, lr):\n",
        "  # x = np.array(F_vec)\n",
        "  # k = len(F_vec[0])\n",
        "  # data=pd.DataFrame({\"a_0\":x[:,0]})\n",
        "  # data.insert(loc=1, column=\"b_0\", value=x[:,1])\n",
        "  # for i in range(1,k-1,2):\n",
        "  #     data.insert(loc=i+1, column=\"a_{}\".format(int((i+1)/2)), value=x[:,i+1])\n",
        "  #     data.insert(loc=i+2, column=\"b_{}\".format(int((i+1)/2)), value=x[:,i+2])\n",
        "  # print(k)\n",
        "  # data.insert(loc=k,column='Class',value=data_s['class'].to_numpy())\n",
        "  # print(data.head(30))\n",
        "\n",
        "  x = np.array(F_vec)\n",
        "  k = int((max(catagories)+1)*4)\n",
        "  data = pd.DataFrame(x)\n",
        "  # data=pd.DataFrame({\"a_0\":x[:,0]})\n",
        "  # data.insert(loc=1, column=\"b_0\", value=x[:,1])\n",
        "  # for i in range(1,k-1,2):\n",
        "  #     # print(i)\n",
        "  #     data.insert(loc=i+1, column=\"a_{}\".format(int((i+1)/2)), value=x[:,i+1])\n",
        "  #     data.insert(loc=i+2, column=\"b_{}\".format(int((i+1)/2)), value=x[:,i+2])\n",
        "  # for i in range(len(F_vec[0])):\n",
        "  #   data.insert(loc=k+i, column=f'S_{i}', value=x[:,k+i])\n",
        "  data.insert(loc=len(F_vec[0]),column='Class',value=data_s['class'].to_numpy())\n",
        "\n",
        "  # feature=[\"a_0\",\"b_0\"]\n",
        "  # for i in range(1,k-1,2):\n",
        "  #     feature.append(\"a_{}\".format(int((i+1)/2)))\n",
        "  #     feature.append(\"b_{}\".format(int((i+1)/2)))\n",
        "\n",
        "  X=data.drop('Class', axis=1) # Features\n",
        "  y=data['Class']  # Labels\n",
        "  print(p)\n",
        "  X_train=X.iloc[q]\n",
        "  X_test=X.iloc[p]\n",
        "  y_train=y.iloc[q]\n",
        "  y_test=y.iloc[p]\n",
        "\n",
        "  checkpoint_filepath='./tmp/checkpoint'\n",
        "  # for train, test in kfold.split(X_train, y_train):\n",
        "  model = tf.keras.Sequential([\n",
        "      tf.keras.layers.Input(shape=(len(X_train.iloc[0]),)),\n",
        "      tf.keras.layers.Dense(90),\n",
        "      tf.keras.layers.Dropout(.2),\n",
        "      tf.keras.layers.Dense(90),\n",
        "      tf.keras.layers.Dropout(.2),\n",
        "      tf.keras.layers.Dense(max(y_train)+1, activation='softmax')\n",
        "  ])\n",
        "\n",
        "  loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "  model.compile(optimizer=tf.keras.optimizers.Adam(lr), loss=loss_fn, \n",
        "                metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "  print('------------------------------------------------------------------------')\n",
        "  history = model.fit(np.array(X_train), np.array(y_train),\n",
        "                      batch_size = 16,\n",
        "                      epochs=100,\n",
        "                      use_multiprocessing=True,\n",
        "                      workers=4,\n",
        "                      validation_data=(np.array(X_test), np.array(y_test)),\n",
        "                      callbacks=[earlystop1, earlystop2,\n",
        "                                  tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                                  save_best_only=True,\n",
        "                                                                  save_weights_only=True,\n",
        "                                                                  monitor='val_sparse_categorical_accuracy',\n",
        "                                                                  mode='max')])\n",
        "  model.load_weights(checkpoint_filepath)\n",
        "  scores = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
        "  y_pred = [np.argmax(i) for i in model.predict(X_test)]\n",
        "\n",
        "  return model, scores, y_pred"
      ],
      "id": "VW4pwGdqv-24"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XOyN9Vyph5pb",
        "outputId": "7d5621d8-0f7a-4248-fb44-66cdcfbe7a67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7197 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5617 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7423 - sparse_categorical_accuracy: 0.7301 - val_loss: 0.5582 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7454 - sparse_categorical_accuracy: 0.7231 - val_loss: 0.5597 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7197 - sparse_categorical_accuracy: 0.7406 - val_loss: 0.5516 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6887 - sparse_categorical_accuracy: 0.7541 - val_loss: 0.5490 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6899 - sparse_categorical_accuracy: 0.7453 - val_loss: 0.5429 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6977 - sparse_categorical_accuracy: 0.7553 - val_loss: 0.5435 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7064 - sparse_categorical_accuracy: 0.7418 - val_loss: 0.5344 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6711 - sparse_categorical_accuracy: 0.7494 - val_loss: 0.5303 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6666 - sparse_categorical_accuracy: 0.7529 - val_loss: 0.5376 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6755 - sparse_categorical_accuracy: 0.7564 - val_loss: 0.5275 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6760 - sparse_categorical_accuracy: 0.7570 - val_loss: 0.5206 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6690 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.5223 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6477 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.5168 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6427 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.5151 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6502 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.5142 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6352 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.5067 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6500 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.5025 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6210 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.5035 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6223 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.5070 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6248 - sparse_categorical_accuracy: 0.7728 - val_loss: 0.4972 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6239 - sparse_categorical_accuracy: 0.7758 - val_loss: 0.4963 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6150 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.4941 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6233 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.4939 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5944 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4864 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6084 - sparse_categorical_accuracy: 0.7804 - val_loss: 0.4895 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6048 - sparse_categorical_accuracy: 0.7804 - val_loss: 0.4840 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5981 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.4846 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5945 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4767 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5723 - sparse_categorical_accuracy: 0.7910 - val_loss: 0.4781 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5967 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.4777 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5964 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.4719 - val_sparse_categorical_accuracy: 0.8440 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5838 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.4742 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5729 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4673 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5788 - sparse_categorical_accuracy: 0.7904 - val_loss: 0.4666 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5920 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4673 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5678 - sparse_categorical_accuracy: 0.7992 - val_loss: 0.4641 - val_sparse_categorical_accuracy: 0.8460 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 5, test #0, 2nd iteration: loss of 0.46411049365997314; sparse_categorical_accuracy of 84.60000157356262%\n",
            "appending basis + selective feature vector data\n",
            "[192, 2155, 1264, 363, 1554, 234, 387, 26, 742, 274, 2208, 1933, 106, 2568, 976, 980, 240, 2337, 2661, 513, 398, 434, 1081, 660, 1063, 2653, 585, 2200, 2509, 2266, 109, 1262, 1115, 757, 2554, 1991, 1087, 1050, 570, 1118, 132, 266, 782, 2543, 1635, 2286, 1588, 2019, 493, 1433, 1594, 1307, 653, 2227, 1618, 1015, 2013, 2112, 1442, 1019, 644, 949, 1822, 2572, 2224, 1079, 158, 1997, 843, 2666, 1391, 1130, 885, 810, 1049, 2558, 1610, 1674, 894, 2533, 2223, 1426, 2512, 85, 1934, 2644, 1550, 1552, 2659, 1543, 1466, 1615, 241, 1346, 2548, 729, 1277, 1242, 1387, 1185, 50, 2276, 812, 1293, 2254, 2174, 1253, 1808, 422, 2119, 1559, 1231, 2360, 2490, 2489, 1487, 261, 706, 2184, 1521, 1078, 876, 359, 1023, 2435, 676, 548, 1192, 1597, 1540, 740, 2153, 1956, 1114, 1830, 1320, 752, 2622, 1489, 2068, 1607, 872, 1284, 2219, 280, 220, 2393, 174, 1516, 528, 1084, 1777, 2662, 693, 1833, 2352, 960, 2198, 2441, 1946, 193, 414, 2149, 2608, 804, 2073, 2156, 637, 1006, 1302, 1672, 180, 1069, 2603, 2369, 2502, 1203, 2185, 1265, 2329, 2409, 28, 2468, 2514, 1161, 1536, 893, 1477, 849, 538, 1735, 2575, 1211, 1823, 1048, 1895, 655, 1757, 1102, 2456, 991, 1201, 1057, 1748, 1661, 191, 2088, 1397, 1624, 2689, 282, 2084, 1664, 2240, 1500, 509, 946, 2334, 1670, 686, 286, 1849, 2402, 438, 1238, 474, 1535, 880, 793, 1171, 1524, 1167, 2171, 1133, 1431, 2302, 1002, 268, 1721, 2074, 1055, 311, 83, 2173, 2029, 1886, 379, 2520, 1170, 1679, 2321, 221, 354, 1696, 2532, 1619, 148, 932, 2107, 81, 1368, 911, 2517, 490, 1582, 1604, 2264, 448, 796, 237, 111, 1220, 1365, 2461, 1198, 904, 262, 2255, 563, 182, 1662, 2581, 942, 1803, 1708, 769, 2111, 2430, 567, 2242, 510, 2281, 1125, 178, 607, 186, 1168, 2313, 550, 2338, 2650, 2429, 1577, 803, 496, 1648, 815, 616, 1561, 2694, 778, 1860, 1791, 480, 1349, 1825, 1363, 614, 194, 1907, 2259, 743, 1288, 1542, 1411, 2139, 581, 1251, 2407, 1107, 1315, 1569, 642, 313, 1141, 432, 27, 1254, 1306, 958, 1896, 1486, 747, 1120, 546, 2562, 2078, 2033, 295, 415, 666, 772, 151, 1402, 1257, 347, 1378, 260, 2125, 1453, 1369, 935, 1827, 1729, 1585, 620, 100, 1899, 866, 734, 1226, 1596, 325, 1178, 2143, 1564, 1022, 877, 1902, 2135, 2599, 1076, 1093, 1707, 130, 2114, 1476, 648, 1980, 1847, 1398, 1149, 756, 2181, 2179, 520, 2140, 1144, 413, 807, 851, 1668, 1404, 768, 1752, 2333, 2505, 1415, 2414, 410, 2687, 1223, 2482, 1541, 1217, 549, 2406, 1410, 619, 659, 2301, 251, 1449, 172, 1156, 2291, 1004, 2086, 1921, 2516, 1769, 1042, 658, 497, 2282, 464, 707, 20, 1207, 2576, 2362, 2550, 335, 334, 350, 1976, 1147, 1878, 105, 702, 2293, 1967, 229, 1836, 641, 1586, 643, 1566, 198, 1548, 1840, 1469, 2437, 2096, 1704, 479, 1950, 2610, 87, 1499, 1714, 2604, 208, 2304, 1305, 1612, 2488, 1937, 2595, 2009, 1248, 324, 367, 2389, 2391, 1861, 822, 1743, 2600, 1724, 2011, 204, 827, 930, 125, 396, 469, 1818, 1429, 468, 1518, 164, 14, 610, 1529, 169, 2121, 936, 2006, 1229, 352, 978, 1629, 1939, 1506, 875, 1158, 1193, 1381, 684, 608, 2324, 506, 457, 2141, 739, 378, 1070, 2477, 750, 853, 331, 1995, 1241, 2263, 2026, 780, 2244, 42, 1637, 1216, 2427, 1734, 1412, 2098, 2110, 84, 1331, 2647, 1136, 2556, 1330, 141, 306, 792, 245, 1137, 2093, 488, 392, 460, 445, 845, 405, 2190, 2588, 2475, 898, 2580, 80, 2444, 839, 211, 1110, 2064, 1805, 477, 1801, 1792, 2531, 2003, 1260, 705, 1816, 1164, 110, 1952, 763, 1640, 1862, 552, 1638, 965, 291, 2097, 508, 1030, 1502, 2203, 2290, 1437, 411, 1646, 368, 459, 1920, 485, 537, 1296, 155, 640, 1972, 2465, 2115, 881, 236, 436, 2163, 2245, 2643, 2527, 811, 994, 2544, 1394, 431, 2370, 206, 270, 454, 744, 733, 2440, 427, 592, 276, 30, 1959, 2316, 433, 918, 1270, 203, 135, 390, 1214, 779, 195, 2272, 1953, 2563, 1292, 1255, 1710, 952, 1606, 93, 10, 1329, 2645, 1644, 1616, 1037, 1484, 2090, 2045, 372, 2079, 2392, 73, 486, 1964, 1181, 2463, 1676, 2177, 595, 1263, 86, 492, 886, 2703, 1845, 31, 2213, 749, 2522, 121, 2591, 1463, 1864, 1622, 117, 2150, 2412, 0, 2671, 2022, 2636, 1218, 713, 2367, 1197, 2651, 1793, 865, 1140, 504, 218, 2315, 1356, 292, 2016, 727, 2134, 712, 2283, 1355, 222, 1029, 2099, 808, 136, 200, 267, 831, 293, 166, 2625, 1066, 2145, 440, 1194, 2498, 156, 1641, 2322, 821, 2557, 131, 2511, 1186, 1236, 2151, 2031, 1011, 2499, 1621, 1654, 1372, 679, 2236, 1715, 928, 2521, 982, 1195, 257, 1085, 1741, 478, 1462, 2359, 2691, 751, 2690, 232, 1790, 92, 2159, 1111, 1515, 2507, 908, 320, 868, 899, 114, 2471, 1855, 439, 2635, 1112, 2311, 2146, 628, 675, 2547, 1046, 1154, 1922, 1530, 1722, 1221, 1590, 1954, 938, 1981, 144, 1568, 802, 850, 1611, 1196, 2278, 2613, 542, 2627, 794, 53, 1870, 412, 360, 163, 1091, 1965, 1408, 2136, 2317, 1345, 526, 1882, 1656, 1936, 1555, 987, 956, 2222, 401, 999, 47, 1249, 2081, 2170, 2374, 691, 407, 1685, 869, 854, 55, 583, 68, 2458, 223, 1074, 302, 998, 979, 1097, 1399, 1200, 2515, 787, 2378, 1395, 2323, 568, 1879, 1745, 1064, 362, 565, 1379, 393, 2607, 1746, 837, 547, 2210, 129, 846, 13, 2673, 2598, 997, 2542, 101, 1073, 483, 848, 2041, 2207, 858, 383, 797, 56, 760, 1574, 1865, 2486, 1361, 2438, 1856, 446, 2260, 430, 421, 2451, 921, 1300, 290, 70, 1017, 2485, 1094, 67, 986, 584, 406, 1443, 2423, 1119, 1799, 1343, 1058, 2654, 1467, 1754, 2039, 785, 1975, 2400, 2241, 950, 499, 1319, 1843, 1039, 2191, 1496, 2025, 948, 502, 2275, 562, 573, 1874, 333, 1771, 798, 1083, 2664, 2197, 146, 1281, 1675, 1155, 699, 897, 283, 519, 1436, 651, 108, 2080, 330, 1636, 1212, 2346, 64, 1716, 1321, 376, 1098, 2021, 1973, 1796, 344, 2010, 1165, 1182, 721, 2023, 1951, 345, 120, 1352, 2070, 227, 1887, 252, 11, 1014, 634, 2256, 2667, 627, 2043, 1180, 2546, 1427, 1159, 1947, 2285, 1491, 1245, 284, 82, 1360]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 10.5264 - sparse_categorical_accuracy: 0.1475 - val_loss: 4.1077 - val_sparse_categorical_accuracy: 0.2580 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.9260 - sparse_categorical_accuracy: 0.1874 - val_loss: 2.9821 - val_sparse_categorical_accuracy: 0.2540 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.1992 - sparse_categorical_accuracy: 0.1786 - val_loss: 2.9979 - val_sparse_categorical_accuracy: 0.2830 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.7261 - sparse_categorical_accuracy: 0.1897 - val_loss: 2.5667 - val_sparse_categorical_accuracy: 0.2820 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.3563 - sparse_categorical_accuracy: 0.1961 - val_loss: 2.3039 - val_sparse_categorical_accuracy: 0.2890 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.8445 - sparse_categorical_accuracy: 0.1926 - val_loss: 2.4827 - val_sparse_categorical_accuracy: 0.2920 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.5409 - sparse_categorical_accuracy: 0.2078 - val_loss: 2.2011 - val_sparse_categorical_accuracy: 0.2990 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.1507 - sparse_categorical_accuracy: 0.2037 - val_loss: 2.0335 - val_sparse_categorical_accuracy: 0.3100 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.6234 - sparse_categorical_accuracy: 0.2283 - val_loss: 1.7484 - val_sparse_categorical_accuracy: 0.3440 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.3769 - sparse_categorical_accuracy: 0.2359 - val_loss: 1.7612 - val_sparse_categorical_accuracy: 0.3310 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.1835 - sparse_categorical_accuracy: 0.2576 - val_loss: 1.6688 - val_sparse_categorical_accuracy: 0.3610 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.6379 - sparse_categorical_accuracy: 0.2758 - val_loss: 1.4590 - val_sparse_categorical_accuracy: 0.4310 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.4986 - sparse_categorical_accuracy: 0.3004 - val_loss: 1.5147 - val_sparse_categorical_accuracy: 0.3990 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.2202 - sparse_categorical_accuracy: 0.2998 - val_loss: 1.3538 - val_sparse_categorical_accuracy: 0.5090 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1213 - sparse_categorical_accuracy: 0.2916 - val_loss: 1.3627 - val_sparse_categorical_accuracy: 0.4130 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8969 - sparse_categorical_accuracy: 0.3033 - val_loss: 1.2986 - val_sparse_categorical_accuracy: 0.4690 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.7286 - sparse_categorical_accuracy: 0.3173 - val_loss: 1.1806 - val_sparse_categorical_accuracy: 0.5380 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5579 - sparse_categorical_accuracy: 0.3636 - val_loss: 1.1435 - val_sparse_categorical_accuracy: 0.5750 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3904 - sparse_categorical_accuracy: 0.3525 - val_loss: 1.1462 - val_sparse_categorical_accuracy: 0.5390 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2210 - sparse_categorical_accuracy: 0.3747 - val_loss: 1.0101 - val_sparse_categorical_accuracy: 0.6260 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1616 - sparse_categorical_accuracy: 0.3882 - val_loss: 1.0089 - val_sparse_categorical_accuracy: 0.6300 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1052 - sparse_categorical_accuracy: 0.3940 - val_loss: 0.9979 - val_sparse_categorical_accuracy: 0.6280 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8933 - sparse_categorical_accuracy: 0.4309 - val_loss: 0.9498 - val_sparse_categorical_accuracy: 0.6620 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9163 - sparse_categorical_accuracy: 0.4344 - val_loss: 0.9076 - val_sparse_categorical_accuracy: 0.6810 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8162 - sparse_categorical_accuracy: 0.4368 - val_loss: 0.9004 - val_sparse_categorical_accuracy: 0.6780 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7152 - sparse_categorical_accuracy: 0.4473 - val_loss: 0.8901 - val_sparse_categorical_accuracy: 0.6780 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6310 - sparse_categorical_accuracy: 0.4754 - val_loss: 0.8701 - val_sparse_categorical_accuracy: 0.6890 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5431 - sparse_categorical_accuracy: 0.4778 - val_loss: 0.8412 - val_sparse_categorical_accuracy: 0.7190 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5223 - sparse_categorical_accuracy: 0.4889 - val_loss: 0.8245 - val_sparse_categorical_accuracy: 0.7120 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.3922 - sparse_categorical_accuracy: 0.5246 - val_loss: 0.7984 - val_sparse_categorical_accuracy: 0.7330 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3807 - sparse_categorical_accuracy: 0.5187 - val_loss: 0.7990 - val_sparse_categorical_accuracy: 0.7290 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3166 - sparse_categorical_accuracy: 0.5381 - val_loss: 0.7817 - val_sparse_categorical_accuracy: 0.7230 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3116 - sparse_categorical_accuracy: 0.5515 - val_loss: 0.7615 - val_sparse_categorical_accuracy: 0.7460 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2410 - sparse_categorical_accuracy: 0.5492 - val_loss: 0.7465 - val_sparse_categorical_accuracy: 0.7330 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1770 - sparse_categorical_accuracy: 0.5960 - val_loss: 0.7403 - val_sparse_categorical_accuracy: 0.7440 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1896 - sparse_categorical_accuracy: 0.5902 - val_loss: 0.7245 - val_sparse_categorical_accuracy: 0.7620 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1550 - sparse_categorical_accuracy: 0.5697 - val_loss: 0.7248 - val_sparse_categorical_accuracy: 0.7630 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1337 - sparse_categorical_accuracy: 0.5872 - val_loss: 0.7075 - val_sparse_categorical_accuracy: 0.7560 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1066 - sparse_categorical_accuracy: 0.6101 - val_loss: 0.6937 - val_sparse_categorical_accuracy: 0.7680 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0512 - sparse_categorical_accuracy: 0.6224 - val_loss: 0.6883 - val_sparse_categorical_accuracy: 0.7720 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0304 - sparse_categorical_accuracy: 0.6270 - val_loss: 0.6734 - val_sparse_categorical_accuracy: 0.7770 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0133 - sparse_categorical_accuracy: 0.6370 - val_loss: 0.6682 - val_sparse_categorical_accuracy: 0.7770 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0282 - sparse_categorical_accuracy: 0.6189 - val_loss: 0.6619 - val_sparse_categorical_accuracy: 0.7840 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9430 - sparse_categorical_accuracy: 0.6458 - val_loss: 0.6465 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9523 - sparse_categorical_accuracy: 0.6639 - val_loss: 0.6461 - val_sparse_categorical_accuracy: 0.7830 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9000 - sparse_categorical_accuracy: 0.6704 - val_loss: 0.6365 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9094 - sparse_categorical_accuracy: 0.6598 - val_loss: 0.6275 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.8676 - sparse_categorical_accuracy: 0.6868 - val_loss: 0.6258 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8385 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.6166 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8660 - sparse_categorical_accuracy: 0.6762 - val_loss: 0.6112 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8357 - sparse_categorical_accuracy: 0.6979 - val_loss: 0.6061 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8023 - sparse_categorical_accuracy: 0.7166 - val_loss: 0.6049 - val_sparse_categorical_accuracy: 0.7920 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7856 - sparse_categorical_accuracy: 0.7096 - val_loss: 0.5973 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7891 - sparse_categorical_accuracy: 0.7078 - val_loss: 0.5942 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7679 - sparse_categorical_accuracy: 0.7166 - val_loss: 0.5913 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7860 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.5832 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7602 - sparse_categorical_accuracy: 0.7365 - val_loss: 0.5799 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7653 - sparse_categorical_accuracy: 0.7225 - val_loss: 0.5719 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.7674 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.5656 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7163 - sparse_categorical_accuracy: 0.7383 - val_loss: 0.5661 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7643 - sparse_categorical_accuracy: 0.7248 - val_loss: 0.5636 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7314 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5548 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7261 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5513 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7166 - sparse_categorical_accuracy: 0.7254 - val_loss: 0.5468 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7030 - sparse_categorical_accuracy: 0.7500 - val_loss: 0.5483 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7087 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.5462 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6853 - sparse_categorical_accuracy: 0.7400 - val_loss: 0.5416 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6931 - sparse_categorical_accuracy: 0.7453 - val_loss: 0.5331 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6811 - sparse_categorical_accuracy: 0.7594 - val_loss: 0.5335 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6679 - sparse_categorical_accuracy: 0.7559 - val_loss: 0.5301 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6616 - sparse_categorical_accuracy: 0.7535 - val_loss: 0.5285 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6666 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.5209 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6514 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.5212 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6523 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5133 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6240 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.5124 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6516 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.5096 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6367 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.5063 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6373 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.5054 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6324 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.5021 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6409 - sparse_categorical_accuracy: 0.7758 - val_loss: 0.5002 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6351 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.4965 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6190 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.4995 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6373 - sparse_categorical_accuracy: 0.7506 - val_loss: 0.4918 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6138 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.4904 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6216 - sparse_categorical_accuracy: 0.7693 - val_loss: 0.4923 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6327 - sparse_categorical_accuracy: 0.7705 - val_loss: 0.4869 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5978 - sparse_categorical_accuracy: 0.7787 - val_loss: 0.4915 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6068 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4865 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6016 - sparse_categorical_accuracy: 0.7822 - val_loss: 0.4803 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5993 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.4854 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6027 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.4764 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5954 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.4740 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5996 - sparse_categorical_accuracy: 0.7804 - val_loss: 0.4745 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5712 - sparse_categorical_accuracy: 0.7892 - val_loss: 0.4708 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5840 - sparse_categorical_accuracy: 0.7892 - val_loss: 0.4693 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5846 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.4672 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5598 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4660 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5702 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4623 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5578 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.4625 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5725 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.4615 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 5, test #1, 2nd iteration: loss of 0.46147027611732483; sparse_categorical_accuracy of 84.10000205039978%\n",
            "appending basis + selective feature vector data\n",
            "[192, 2155, 1264, 363, 1554, 234, 387, 26, 742, 274, 2208, 1933, 106, 2568, 976, 980, 240, 2337, 2661, 513, 398, 434, 1081, 660, 1063, 2653, 585, 2200, 2509, 2266, 109, 1262, 1115, 757, 2554, 1991, 1087, 1050, 570, 1118, 132, 266, 782, 2543, 1635, 2286, 1588, 2019, 493, 1433, 1594, 1307, 653, 2227, 1618, 1015, 2013, 2112, 1442, 1019, 644, 949, 1822, 2572, 2224, 1079, 158, 1997, 843, 2666, 1391, 1130, 885, 810, 1049, 2558, 1610, 1674, 894, 2533, 2223, 1426, 2512, 85, 1934, 2644, 1550, 1552, 2659, 1543, 1466, 1615, 241, 1346, 2548, 729, 1277, 1242, 1387, 1185, 50, 2276, 812, 1293, 2254, 2174, 1253, 1808, 422, 2119, 1559, 1231, 2360, 2490, 2489, 1487, 261, 706, 2184, 1521, 1078, 876, 359, 1023, 2435, 676, 548, 1192, 1597, 1540, 740, 2153, 1956, 1114, 1830, 1320, 752, 2622, 1489, 2068, 1607, 872, 1284, 2219, 280, 220, 2393, 174, 1516, 528, 1084, 1777, 2662, 693, 1833, 2352, 960, 2198, 2441, 1946, 193, 414, 2149, 2608, 804, 2073, 2156, 637, 1006, 1302, 1672, 180, 1069, 2603, 2369, 2502, 1203, 2185, 1265, 2329, 2409, 28, 2468, 2514, 1161, 1536, 893, 1477, 849, 538, 1735, 2575, 1211, 1823, 1048, 1895, 655, 1757, 1102, 2456, 991, 1201, 1057, 1748, 1661, 191, 2088, 1397, 1624, 2689, 282, 2084, 1664, 2240, 1500, 509, 946, 2334, 1670, 686, 286, 1849, 2402, 438, 1238, 474, 1535, 880, 793, 1171, 1524, 1167, 2171, 1133, 1431, 2302, 1002, 268, 1721, 2074, 1055, 311, 83, 2173, 2029, 1886, 379, 2520, 1170, 1679, 2321, 221, 354, 1696, 2532, 1619, 148, 932, 2107, 81, 1368, 911, 2517, 490, 1582, 1604, 2264, 448, 796, 237, 111, 1220, 1365, 2461, 1198, 904, 262, 2255, 563, 182, 1662, 2581, 942, 1803, 1708, 769, 2111, 2430, 567, 2242, 510, 2281, 1125, 178, 607, 186, 1168, 2313, 550, 2338, 2650, 2429, 1577, 803, 496, 1648, 815, 616, 1561, 2694, 778, 1860, 1791, 480, 1349, 1825, 1363, 614, 194, 1907, 2259, 743, 1288, 1542, 1411, 2139, 581, 1251, 2407, 1107, 1315, 1569, 642, 313, 1141, 432, 27, 1254, 1306, 958, 1896, 1486, 747, 1120, 546, 2562, 2078, 2033, 295, 415, 666, 772, 151, 1402, 1257, 347, 1378, 260, 2125, 1453, 1369, 935, 1827, 1729, 1585, 620, 100, 1899, 866, 734, 1226, 1596, 325, 1178, 2143, 1564, 1022, 877, 1902, 2135, 2599, 1076, 1093, 1707, 130, 2114, 1476, 648, 1980, 1847, 1398, 1149, 756, 2181, 2179, 520, 2140, 1144, 413, 807, 851, 1668, 1404, 768, 1752, 2333, 2505, 1415, 2414, 410, 2687, 1223, 2482, 1541, 1217, 549, 2406, 1410, 619, 659, 2301, 251, 1449, 172, 1156, 2291, 1004, 2086, 1921, 2516, 1769, 1042, 658, 497, 2282, 464, 707, 20, 1207, 2576, 2362, 2550, 335, 334, 350, 1976, 1147, 1878, 105, 702, 2293, 1967, 229, 1836, 641, 1586, 643, 1566, 198, 1548, 1840, 1469, 2437, 2096, 1704, 479, 1950, 2610, 87, 1499, 1714, 2604, 208, 2304, 1305, 1612, 2488, 1937, 2595, 2009, 1248, 324, 367, 2389, 2391, 1861, 822, 1743, 2600, 1724, 2011, 204, 827, 930, 125, 396, 469, 1818, 1429, 468, 1518, 164, 14, 610, 1529, 169, 2121, 936, 2006, 1229, 352, 978, 1629, 1939, 1506, 875, 1158, 1193, 1381, 684, 608, 2324, 506, 457, 2141, 739, 378, 1070, 2477, 750, 853, 331, 1995, 1241, 2263, 2026, 780, 2244, 42, 1637, 1216, 2427, 1734, 1412, 2098, 2110, 84, 1331, 2647, 1136, 2556, 1330, 141, 306, 792, 245, 1137, 2093, 488, 392, 460, 445, 845, 405, 2190, 2588, 2475, 898, 2580, 80, 2444, 839, 211, 1110, 2064, 1805, 477, 1801, 1792, 2531, 2003, 1260, 705, 1816, 1164, 110, 1952, 763, 1640, 1862, 552, 1638, 965, 291, 2097, 508, 1030, 1502, 2203, 2290, 1437, 411, 1646, 368, 459, 1920, 485, 537, 1296, 155, 640, 1972, 2465, 2115, 881, 236, 436, 2163, 2245, 2643, 2527, 811, 994, 2544, 1394, 431, 2370, 206, 270, 454, 744, 733, 2440, 427, 592, 276, 30, 1959, 2316, 433, 918, 1270, 203, 135, 390, 1214, 779, 195, 2272, 1953, 2563, 1292, 1255, 1710, 952, 1606, 93, 10, 1329, 2645, 1644, 1616, 1037, 1484, 2090, 2045, 372, 2079, 2392, 73, 486, 1964, 1181, 2463, 1676, 2177, 595, 1263, 86, 492, 886, 2703, 1845, 31, 2213, 749, 2522, 121, 2591, 1463, 1864, 1622, 117, 2150, 2412, 0, 2671, 2022, 2636, 1218, 713, 2367, 1197, 2651, 1793, 865, 1140, 504, 218, 2315, 1356, 292, 2016, 727, 2134, 712, 2283, 1355, 222, 1029, 2099, 808, 136, 200, 267, 831, 293, 166, 2625, 1066, 2145, 440, 1194, 2498, 156, 1641, 2322, 821, 2557, 131, 2511, 1186, 1236, 2151, 2031, 1011, 2499, 1621, 1654, 1372, 679, 2236, 1715, 928, 2521, 982, 1195, 257, 1085, 1741, 478, 1462, 2359, 2691, 751, 2690, 232, 1790, 92, 2159, 1111, 1515, 2507, 908, 320, 868, 899, 114, 2471, 1855, 439, 2635, 1112, 2311, 2146, 628, 675, 2547, 1046, 1154, 1922, 1530, 1722, 1221, 1590, 1954, 938, 1981, 144, 1568, 802, 850, 1611, 1196, 2278, 2613, 542, 2627, 794, 53, 1870, 412, 360, 163, 1091, 1965, 1408, 2136, 2317, 1345, 526, 1882, 1656, 1936, 1555, 987, 956, 2222, 401, 999, 47, 1249, 2081, 2170, 2374, 691, 407, 1685, 869, 854, 55, 583, 68, 2458, 223, 1074, 302, 998, 979, 1097, 1399, 1200, 2515, 787, 2378, 1395, 2323, 568, 1879, 1745, 1064, 362, 565, 1379, 393, 2607, 1746, 837, 547, 2210, 129, 846, 13, 2673, 2598, 997, 2542, 101, 1073, 483, 848, 2041, 2207, 858, 383, 797, 56, 760, 1574, 1865, 2486, 1361, 2438, 1856, 446, 2260, 430, 421, 2451, 921, 1300, 290, 70, 1017, 2485, 1094, 67, 986, 584, 406, 1443, 2423, 1119, 1799, 1343, 1058, 2654, 1467, 1754, 2039, 785, 1975, 2400, 2241, 950, 499, 1319, 1843, 1039, 2191, 1496, 2025, 948, 502, 2275, 562, 573, 1874, 333, 1771, 798, 1083, 2664, 2197, 146, 1281, 1675, 1155, 699, 897, 283, 519, 1436, 651, 108, 2080, 330, 1636, 1212, 2346, 64, 1716, 1321, 376, 1098, 2021, 1973, 1796, 344, 2010, 1165, 1182, 721, 2023, 1951, 345, 120, 1352, 2070, 227, 1887, 252, 11, 1014, 634, 2256, 2667, 627, 2043, 1180, 2546, 1427, 1159, 1947, 2285, 1491, 1245, 284, 82, 1360]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 10.1579 - sparse_categorical_accuracy: 0.1879 - val_loss: 3.2577 - val_sparse_categorical_accuracy: 0.2820 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.9978 - sparse_categorical_accuracy: 0.1932 - val_loss: 2.2191 - val_sparse_categorical_accuracy: 0.2870 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.2843 - sparse_categorical_accuracy: 0.2037 - val_loss: 2.1810 - val_sparse_categorical_accuracy: 0.3010 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.9818 - sparse_categorical_accuracy: 0.2119 - val_loss: 2.1702 - val_sparse_categorical_accuracy: 0.3020 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.3967 - sparse_categorical_accuracy: 0.2500 - val_loss: 1.8473 - val_sparse_categorical_accuracy: 0.3400 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.2240 - sparse_categorical_accuracy: 0.2295 - val_loss: 1.8017 - val_sparse_categorical_accuracy: 0.3460 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.6302 - sparse_categorical_accuracy: 0.2529 - val_loss: 1.5518 - val_sparse_categorical_accuracy: 0.3700 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.4484 - sparse_categorical_accuracy: 0.2430 - val_loss: 1.5325 - val_sparse_categorical_accuracy: 0.3990 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.1121 - sparse_categorical_accuracy: 0.2699 - val_loss: 1.3240 - val_sparse_categorical_accuracy: 0.4530 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.7786 - sparse_categorical_accuracy: 0.2693 - val_loss: 1.2467 - val_sparse_categorical_accuracy: 0.4820 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.6776 - sparse_categorical_accuracy: 0.2740 - val_loss: 1.3358 - val_sparse_categorical_accuracy: 0.4360 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.3823 - sparse_categorical_accuracy: 0.3009 - val_loss: 1.1114 - val_sparse_categorical_accuracy: 0.5680 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1629 - sparse_categorical_accuracy: 0.3185 - val_loss: 1.1396 - val_sparse_categorical_accuracy: 0.5520 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.0004 - sparse_categorical_accuracy: 0.3132 - val_loss: 1.0716 - val_sparse_categorical_accuracy: 0.5620 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8041 - sparse_categorical_accuracy: 0.3331 - val_loss: 1.0923 - val_sparse_categorical_accuracy: 0.5550 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5622 - sparse_categorical_accuracy: 0.3671 - val_loss: 0.9918 - val_sparse_categorical_accuracy: 0.6040 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4068 - sparse_categorical_accuracy: 0.3765 - val_loss: 0.9410 - val_sparse_categorical_accuracy: 0.6450 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2014 - sparse_categorical_accuracy: 0.3782 - val_loss: 0.9170 - val_sparse_categorical_accuracy: 0.6570 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0979 - sparse_categorical_accuracy: 0.3981 - val_loss: 0.8785 - val_sparse_categorical_accuracy: 0.6840 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8594 - sparse_categorical_accuracy: 0.4297 - val_loss: 0.8870 - val_sparse_categorical_accuracy: 0.6820 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9418 - sparse_categorical_accuracy: 0.4368 - val_loss: 0.8566 - val_sparse_categorical_accuracy: 0.6980 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8210 - sparse_categorical_accuracy: 0.4450 - val_loss: 0.8269 - val_sparse_categorical_accuracy: 0.7110 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.7310 - sparse_categorical_accuracy: 0.4766 - val_loss: 0.8051 - val_sparse_categorical_accuracy: 0.7270 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5897 - sparse_categorical_accuracy: 0.4783 - val_loss: 0.8130 - val_sparse_categorical_accuracy: 0.7110 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5615 - sparse_categorical_accuracy: 0.5053 - val_loss: 0.7846 - val_sparse_categorical_accuracy: 0.7220 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.4713 - sparse_categorical_accuracy: 0.5100 - val_loss: 0.7769 - val_sparse_categorical_accuracy: 0.7310 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4204 - sparse_categorical_accuracy: 0.5146 - val_loss: 0.7667 - val_sparse_categorical_accuracy: 0.7250 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3973 - sparse_categorical_accuracy: 0.5422 - val_loss: 0.7572 - val_sparse_categorical_accuracy: 0.7360 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3547 - sparse_categorical_accuracy: 0.5240 - val_loss: 0.7273 - val_sparse_categorical_accuracy: 0.7570 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2722 - sparse_categorical_accuracy: 0.5492 - val_loss: 0.7209 - val_sparse_categorical_accuracy: 0.7570 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2742 - sparse_categorical_accuracy: 0.5638 - val_loss: 0.7131 - val_sparse_categorical_accuracy: 0.7690 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1665 - sparse_categorical_accuracy: 0.5878 - val_loss: 0.7018 - val_sparse_categorical_accuracy: 0.7590 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2030 - sparse_categorical_accuracy: 0.5907 - val_loss: 0.6842 - val_sparse_categorical_accuracy: 0.7680 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1663 - sparse_categorical_accuracy: 0.5779 - val_loss: 0.6733 - val_sparse_categorical_accuracy: 0.7890 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1087 - sparse_categorical_accuracy: 0.5943 - val_loss: 0.6616 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.0977 - sparse_categorical_accuracy: 0.5960 - val_loss: 0.6642 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0620 - sparse_categorical_accuracy: 0.6036 - val_loss: 0.6572 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0472 - sparse_categorical_accuracy: 0.6165 - val_loss: 0.6466 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0141 - sparse_categorical_accuracy: 0.6382 - val_loss: 0.6402 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9878 - sparse_categorical_accuracy: 0.6440 - val_loss: 0.6292 - val_sparse_categorical_accuracy: 0.7930 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9583 - sparse_categorical_accuracy: 0.6364 - val_loss: 0.6296 - val_sparse_categorical_accuracy: 0.7950 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9287 - sparse_categorical_accuracy: 0.6587 - val_loss: 0.6214 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9147 - sparse_categorical_accuracy: 0.6499 - val_loss: 0.6162 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9045 - sparse_categorical_accuracy: 0.6610 - val_loss: 0.6055 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8772 - sparse_categorical_accuracy: 0.6786 - val_loss: 0.6004 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8506 - sparse_categorical_accuracy: 0.6879 - val_loss: 0.5886 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8432 - sparse_categorical_accuracy: 0.6868 - val_loss: 0.5926 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8241 - sparse_categorical_accuracy: 0.7014 - val_loss: 0.5860 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8244 - sparse_categorical_accuracy: 0.6926 - val_loss: 0.5745 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8101 - sparse_categorical_accuracy: 0.7061 - val_loss: 0.5668 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8025 - sparse_categorical_accuracy: 0.6985 - val_loss: 0.5719 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7731 - sparse_categorical_accuracy: 0.7108 - val_loss: 0.5642 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7779 - sparse_categorical_accuracy: 0.7090 - val_loss: 0.5618 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7912 - sparse_categorical_accuracy: 0.7032 - val_loss: 0.5525 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7637 - sparse_categorical_accuracy: 0.7184 - val_loss: 0.5504 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7614 - sparse_categorical_accuracy: 0.7266 - val_loss: 0.5497 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7326 - sparse_categorical_accuracy: 0.7184 - val_loss: 0.5433 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7407 - sparse_categorical_accuracy: 0.7231 - val_loss: 0.5346 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7385 - sparse_categorical_accuracy: 0.7196 - val_loss: 0.5350 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7046 - sparse_categorical_accuracy: 0.7319 - val_loss: 0.5303 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7092 - sparse_categorical_accuracy: 0.7395 - val_loss: 0.5326 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7062 - sparse_categorical_accuracy: 0.7441 - val_loss: 0.5182 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7044 - sparse_categorical_accuracy: 0.7348 - val_loss: 0.5168 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6795 - sparse_categorical_accuracy: 0.7465 - val_loss: 0.5166 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6493 - sparse_categorical_accuracy: 0.7506 - val_loss: 0.5091 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6450 - sparse_categorical_accuracy: 0.7635 - val_loss: 0.5141 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6431 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.5057 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6532 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5039 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6629 - sparse_categorical_accuracy: 0.7547 - val_loss: 0.5014 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6695 - sparse_categorical_accuracy: 0.7488 - val_loss: 0.4985 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6415 - sparse_categorical_accuracy: 0.7594 - val_loss: 0.4901 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6261 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.4906 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6361 - sparse_categorical_accuracy: 0.7559 - val_loss: 0.4914 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6305 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.4898 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6252 - sparse_categorical_accuracy: 0.7600 - val_loss: 0.4811 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6109 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.4834 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6244 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.4785 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6157 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.4762 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5985 - sparse_categorical_accuracy: 0.7787 - val_loss: 0.4737 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5977 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.4717 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6044 - sparse_categorical_accuracy: 0.7769 - val_loss: 0.4729 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5968 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4766 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5923 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.4680 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5928 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.4677 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5833 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4607 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6012 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4613 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5878 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.4591 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5760 - sparse_categorical_accuracy: 0.7881 - val_loss: 0.4631 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5640 - sparse_categorical_accuracy: 0.7951 - val_loss: 0.4586 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5790 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4527 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5735 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.4497 - val_sparse_categorical_accuracy: 0.8450 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5792 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4509 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5719 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.4469 - val_sparse_categorical_accuracy: 0.8440 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5561 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4487 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5758 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.4471 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5606 - sparse_categorical_accuracy: 0.8009 - val_loss: 0.4444 - val_sparse_categorical_accuracy: 0.8480 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5706 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4441 - val_sparse_categorical_accuracy: 0.8450 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5638 - sparse_categorical_accuracy: 0.7851 - val_loss: 0.4441 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5587 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.4455 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5567 - sparse_categorical_accuracy: 0.7922 - val_loss: 0.4398 - val_sparse_categorical_accuracy: 0.8500 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 5, test #2, 2nd iteration: loss of 0.43978583812713623; sparse_categorical_accuracy of 85.00000238418579%\n",
            "appending basis + selective feature vector data\n",
            "[192, 2155, 1264, 363, 1554, 234, 387, 26, 742, 274, 2208, 1933, 106, 2568, 976, 980, 240, 2337, 2661, 513, 398, 434, 1081, 660, 1063, 2653, 585, 2200, 2509, 2266, 109, 1262, 1115, 757, 2554, 1991, 1087, 1050, 570, 1118, 132, 266, 782, 2543, 1635, 2286, 1588, 2019, 493, 1433, 1594, 1307, 653, 2227, 1618, 1015, 2013, 2112, 1442, 1019, 644, 949, 1822, 2572, 2224, 1079, 158, 1997, 843, 2666, 1391, 1130, 885, 810, 1049, 2558, 1610, 1674, 894, 2533, 2223, 1426, 2512, 85, 1934, 2644, 1550, 1552, 2659, 1543, 1466, 1615, 241, 1346, 2548, 729, 1277, 1242, 1387, 1185, 50, 2276, 812, 1293, 2254, 2174, 1253, 1808, 422, 2119, 1559, 1231, 2360, 2490, 2489, 1487, 261, 706, 2184, 1521, 1078, 876, 359, 1023, 2435, 676, 548, 1192, 1597, 1540, 740, 2153, 1956, 1114, 1830, 1320, 752, 2622, 1489, 2068, 1607, 872, 1284, 2219, 280, 220, 2393, 174, 1516, 528, 1084, 1777, 2662, 693, 1833, 2352, 960, 2198, 2441, 1946, 193, 414, 2149, 2608, 804, 2073, 2156, 637, 1006, 1302, 1672, 180, 1069, 2603, 2369, 2502, 1203, 2185, 1265, 2329, 2409, 28, 2468, 2514, 1161, 1536, 893, 1477, 849, 538, 1735, 2575, 1211, 1823, 1048, 1895, 655, 1757, 1102, 2456, 991, 1201, 1057, 1748, 1661, 191, 2088, 1397, 1624, 2689, 282, 2084, 1664, 2240, 1500, 509, 946, 2334, 1670, 686, 286, 1849, 2402, 438, 1238, 474, 1535, 880, 793, 1171, 1524, 1167, 2171, 1133, 1431, 2302, 1002, 268, 1721, 2074, 1055, 311, 83, 2173, 2029, 1886, 379, 2520, 1170, 1679, 2321, 221, 354, 1696, 2532, 1619, 148, 932, 2107, 81, 1368, 911, 2517, 490, 1582, 1604, 2264, 448, 796, 237, 111, 1220, 1365, 2461, 1198, 904, 262, 2255, 563, 182, 1662, 2581, 942, 1803, 1708, 769, 2111, 2430, 567, 2242, 510, 2281, 1125, 178, 607, 186, 1168, 2313, 550, 2338, 2650, 2429, 1577, 803, 496, 1648, 815, 616, 1561, 2694, 778, 1860, 1791, 480, 1349, 1825, 1363, 614, 194, 1907, 2259, 743, 1288, 1542, 1411, 2139, 581, 1251, 2407, 1107, 1315, 1569, 642, 313, 1141, 432, 27, 1254, 1306, 958, 1896, 1486, 747, 1120, 546, 2562, 2078, 2033, 295, 415, 666, 772, 151, 1402, 1257, 347, 1378, 260, 2125, 1453, 1369, 935, 1827, 1729, 1585, 620, 100, 1899, 866, 734, 1226, 1596, 325, 1178, 2143, 1564, 1022, 877, 1902, 2135, 2599, 1076, 1093, 1707, 130, 2114, 1476, 648, 1980, 1847, 1398, 1149, 756, 2181, 2179, 520, 2140, 1144, 413, 807, 851, 1668, 1404, 768, 1752, 2333, 2505, 1415, 2414, 410, 2687, 1223, 2482, 1541, 1217, 549, 2406, 1410, 619, 659, 2301, 251, 1449, 172, 1156, 2291, 1004, 2086, 1921, 2516, 1769, 1042, 658, 497, 2282, 464, 707, 20, 1207, 2576, 2362, 2550, 335, 334, 350, 1976, 1147, 1878, 105, 702, 2293, 1967, 229, 1836, 641, 1586, 643, 1566, 198, 1548, 1840, 1469, 2437, 2096, 1704, 479, 1950, 2610, 87, 1499, 1714, 2604, 208, 2304, 1305, 1612, 2488, 1937, 2595, 2009, 1248, 324, 367, 2389, 2391, 1861, 822, 1743, 2600, 1724, 2011, 204, 827, 930, 125, 396, 469, 1818, 1429, 468, 1518, 164, 14, 610, 1529, 169, 2121, 936, 2006, 1229, 352, 978, 1629, 1939, 1506, 875, 1158, 1193, 1381, 684, 608, 2324, 506, 457, 2141, 739, 378, 1070, 2477, 750, 853, 331, 1995, 1241, 2263, 2026, 780, 2244, 42, 1637, 1216, 2427, 1734, 1412, 2098, 2110, 84, 1331, 2647, 1136, 2556, 1330, 141, 306, 792, 245, 1137, 2093, 488, 392, 460, 445, 845, 405, 2190, 2588, 2475, 898, 2580, 80, 2444, 839, 211, 1110, 2064, 1805, 477, 1801, 1792, 2531, 2003, 1260, 705, 1816, 1164, 110, 1952, 763, 1640, 1862, 552, 1638, 965, 291, 2097, 508, 1030, 1502, 2203, 2290, 1437, 411, 1646, 368, 459, 1920, 485, 537, 1296, 155, 640, 1972, 2465, 2115, 881, 236, 436, 2163, 2245, 2643, 2527, 811, 994, 2544, 1394, 431, 2370, 206, 270, 454, 744, 733, 2440, 427, 592, 276, 30, 1959, 2316, 433, 918, 1270, 203, 135, 390, 1214, 779, 195, 2272, 1953, 2563, 1292, 1255, 1710, 952, 1606, 93, 10, 1329, 2645, 1644, 1616, 1037, 1484, 2090, 2045, 372, 2079, 2392, 73, 486, 1964, 1181, 2463, 1676, 2177, 595, 1263, 86, 492, 886, 2703, 1845, 31, 2213, 749, 2522, 121, 2591, 1463, 1864, 1622, 117, 2150, 2412, 0, 2671, 2022, 2636, 1218, 713, 2367, 1197, 2651, 1793, 865, 1140, 504, 218, 2315, 1356, 292, 2016, 727, 2134, 712, 2283, 1355, 222, 1029, 2099, 808, 136, 200, 267, 831, 293, 166, 2625, 1066, 2145, 440, 1194, 2498, 156, 1641, 2322, 821, 2557, 131, 2511, 1186, 1236, 2151, 2031, 1011, 2499, 1621, 1654, 1372, 679, 2236, 1715, 928, 2521, 982, 1195, 257, 1085, 1741, 478, 1462, 2359, 2691, 751, 2690, 232, 1790, 92, 2159, 1111, 1515, 2507, 908, 320, 868, 899, 114, 2471, 1855, 439, 2635, 1112, 2311, 2146, 628, 675, 2547, 1046, 1154, 1922, 1530, 1722, 1221, 1590, 1954, 938, 1981, 144, 1568, 802, 850, 1611, 1196, 2278, 2613, 542, 2627, 794, 53, 1870, 412, 360, 163, 1091, 1965, 1408, 2136, 2317, 1345, 526, 1882, 1656, 1936, 1555, 987, 956, 2222, 401, 999, 47, 1249, 2081, 2170, 2374, 691, 407, 1685, 869, 854, 55, 583, 68, 2458, 223, 1074, 302, 998, 979, 1097, 1399, 1200, 2515, 787, 2378, 1395, 2323, 568, 1879, 1745, 1064, 362, 565, 1379, 393, 2607, 1746, 837, 547, 2210, 129, 846, 13, 2673, 2598, 997, 2542, 101, 1073, 483, 848, 2041, 2207, 858, 383, 797, 56, 760, 1574, 1865, 2486, 1361, 2438, 1856, 446, 2260, 430, 421, 2451, 921, 1300, 290, 70, 1017, 2485, 1094, 67, 986, 584, 406, 1443, 2423, 1119, 1799, 1343, 1058, 2654, 1467, 1754, 2039, 785, 1975, 2400, 2241, 950, 499, 1319, 1843, 1039, 2191, 1496, 2025, 948, 502, 2275, 562, 573, 1874, 333, 1771, 798, 1083, 2664, 2197, 146, 1281, 1675, 1155, 699, 897, 283, 519, 1436, 651, 108, 2080, 330, 1636, 1212, 2346, 64, 1716, 1321, 376, 1098, 2021, 1973, 1796, 344, 2010, 1165, 1182, 721, 2023, 1951, 345, 120, 1352, 2070, 227, 1887, 252, 11, 1014, 634, 2256, 2667, 627, 2043, 1180, 2546, 1427, 1159, 1947, 2285, 1491, 1245, 284, 82, 1360]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 9.5095 - sparse_categorical_accuracy: 0.1516 - val_loss: 2.9935 - val_sparse_categorical_accuracy: 0.2790 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.7034 - sparse_categorical_accuracy: 0.1645 - val_loss: 2.9168 - val_sparse_categorical_accuracy: 0.2950 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.6780 - sparse_categorical_accuracy: 0.1891 - val_loss: 2.2845 - val_sparse_categorical_accuracy: 0.2930 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 6.4143 - sparse_categorical_accuracy: 0.1950 - val_loss: 2.9565 - val_sparse_categorical_accuracy: 0.2960 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.1149 - sparse_categorical_accuracy: 0.2061 - val_loss: 2.3263 - val_sparse_categorical_accuracy: 0.3040 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.4999 - sparse_categorical_accuracy: 0.2143 - val_loss: 1.8674 - val_sparse_categorical_accuracy: 0.3320 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.2848 - sparse_categorical_accuracy: 0.2155 - val_loss: 1.9577 - val_sparse_categorical_accuracy: 0.3340 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.0060 - sparse_categorical_accuracy: 0.2207 - val_loss: 1.7478 - val_sparse_categorical_accuracy: 0.3540 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.4512 - sparse_categorical_accuracy: 0.2471 - val_loss: 1.7954 - val_sparse_categorical_accuracy: 0.3540 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 4.0946 - sparse_categorical_accuracy: 0.2588 - val_loss: 1.6776 - val_sparse_categorical_accuracy: 0.3550 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.8113 - sparse_categorical_accuracy: 0.2787 - val_loss: 1.4239 - val_sparse_categorical_accuracy: 0.4550 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.6868 - sparse_categorical_accuracy: 0.2787 - val_loss: 1.4132 - val_sparse_categorical_accuracy: 0.4490 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.4805 - sparse_categorical_accuracy: 0.2974 - val_loss: 1.3774 - val_sparse_categorical_accuracy: 0.4800 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.2771 - sparse_categorical_accuracy: 0.3039 - val_loss: 1.2183 - val_sparse_categorical_accuracy: 0.5340 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8866 - sparse_categorical_accuracy: 0.3220 - val_loss: 1.2933 - val_sparse_categorical_accuracy: 0.4950 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8158 - sparse_categorical_accuracy: 0.3167 - val_loss: 1.1104 - val_sparse_categorical_accuracy: 0.5880 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.6265 - sparse_categorical_accuracy: 0.3296 - val_loss: 1.1503 - val_sparse_categorical_accuracy: 0.5500 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5429 - sparse_categorical_accuracy: 0.3507 - val_loss: 1.0594 - val_sparse_categorical_accuracy: 0.5720 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3036 - sparse_categorical_accuracy: 0.3735 - val_loss: 0.9820 - val_sparse_categorical_accuracy: 0.6580 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2059 - sparse_categorical_accuracy: 0.3893 - val_loss: 0.9314 - val_sparse_categorical_accuracy: 0.6710 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1690 - sparse_categorical_accuracy: 0.4028 - val_loss: 0.9178 - val_sparse_categorical_accuracy: 0.6800 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9754 - sparse_categorical_accuracy: 0.4110 - val_loss: 0.9159 - val_sparse_categorical_accuracy: 0.6640 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8863 - sparse_categorical_accuracy: 0.4403 - val_loss: 0.9083 - val_sparse_categorical_accuracy: 0.6700 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.8232 - sparse_categorical_accuracy: 0.4379 - val_loss: 0.8569 - val_sparse_categorical_accuracy: 0.7030 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6795 - sparse_categorical_accuracy: 0.4602 - val_loss: 0.8187 - val_sparse_categorical_accuracy: 0.7580 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6563 - sparse_categorical_accuracy: 0.4696 - val_loss: 0.8139 - val_sparse_categorical_accuracy: 0.7210 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6195 - sparse_categorical_accuracy: 0.4930 - val_loss: 0.7843 - val_sparse_categorical_accuracy: 0.7510 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5328 - sparse_categorical_accuracy: 0.4883 - val_loss: 0.7706 - val_sparse_categorical_accuracy: 0.7570 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5245 - sparse_categorical_accuracy: 0.4947 - val_loss: 0.7715 - val_sparse_categorical_accuracy: 0.7460 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3669 - sparse_categorical_accuracy: 0.5468 - val_loss: 0.7398 - val_sparse_categorical_accuracy: 0.7800 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3076 - sparse_categorical_accuracy: 0.5474 - val_loss: 0.7397 - val_sparse_categorical_accuracy: 0.7650 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2998 - sparse_categorical_accuracy: 0.5550 - val_loss: 0.7423 - val_sparse_categorical_accuracy: 0.7410 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2850 - sparse_categorical_accuracy: 0.5345 - val_loss: 0.7034 - val_sparse_categorical_accuracy: 0.7850 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2040 - sparse_categorical_accuracy: 0.5749 - val_loss: 0.6944 - val_sparse_categorical_accuracy: 0.7740 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1761 - sparse_categorical_accuracy: 0.5902 - val_loss: 0.6816 - val_sparse_categorical_accuracy: 0.7790 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1166 - sparse_categorical_accuracy: 0.6001 - val_loss: 0.6731 - val_sparse_categorical_accuracy: 0.7950 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1540 - sparse_categorical_accuracy: 0.5878 - val_loss: 0.6645 - val_sparse_categorical_accuracy: 0.7920 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0494 - sparse_categorical_accuracy: 0.6148 - val_loss: 0.6617 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0117 - sparse_categorical_accuracy: 0.6434 - val_loss: 0.6448 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9674 - sparse_categorical_accuracy: 0.6522 - val_loss: 0.6407 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0035 - sparse_categorical_accuracy: 0.6276 - val_loss: 0.6332 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9806 - sparse_categorical_accuracy: 0.6405 - val_loss: 0.6209 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9469 - sparse_categorical_accuracy: 0.6522 - val_loss: 0.6374 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9224 - sparse_categorical_accuracy: 0.6598 - val_loss: 0.6227 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8874 - sparse_categorical_accuracy: 0.6628 - val_loss: 0.6124 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9106 - sparse_categorical_accuracy: 0.6669 - val_loss: 0.6037 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8919 - sparse_categorical_accuracy: 0.6639 - val_loss: 0.6059 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8337 - sparse_categorical_accuracy: 0.6996 - val_loss: 0.5955 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8467 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.5862 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8205 - sparse_categorical_accuracy: 0.6991 - val_loss: 0.5845 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8027 - sparse_categorical_accuracy: 0.7049 - val_loss: 0.5790 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8113 - sparse_categorical_accuracy: 0.7032 - val_loss: 0.5779 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7686 - sparse_categorical_accuracy: 0.7190 - val_loss: 0.5700 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7521 - sparse_categorical_accuracy: 0.7201 - val_loss: 0.5624 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7635 - sparse_categorical_accuracy: 0.7231 - val_loss: 0.5718 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7606 - sparse_categorical_accuracy: 0.7237 - val_loss: 0.5567 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7341 - sparse_categorical_accuracy: 0.7371 - val_loss: 0.5568 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.7204 - sparse_categorical_accuracy: 0.7283 - val_loss: 0.5511 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7215 - sparse_categorical_accuracy: 0.7424 - val_loss: 0.5436 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7113 - sparse_categorical_accuracy: 0.7400 - val_loss: 0.5432 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7167 - sparse_categorical_accuracy: 0.7365 - val_loss: 0.5379 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6937 - sparse_categorical_accuracy: 0.7453 - val_loss: 0.5368 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7183 - sparse_categorical_accuracy: 0.7365 - val_loss: 0.5332 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6905 - sparse_categorical_accuracy: 0.7389 - val_loss: 0.5335 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6643 - sparse_categorical_accuracy: 0.7477 - val_loss: 0.5248 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6655 - sparse_categorical_accuracy: 0.7570 - val_loss: 0.5261 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6811 - sparse_categorical_accuracy: 0.7588 - val_loss: 0.5191 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6383 - sparse_categorical_accuracy: 0.7728 - val_loss: 0.5185 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6562 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.5157 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6435 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.5111 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6493 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.5106 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6439 - sparse_categorical_accuracy: 0.7570 - val_loss: 0.5067 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6283 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.5013 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6361 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.4997 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6438 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.5047 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6061 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.5012 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6169 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.4951 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6280 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.4933 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6163 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.4890 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6205 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.4902 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6037 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.4879 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6187 - sparse_categorical_accuracy: 0.7769 - val_loss: 0.4857 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6123 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.4857 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6145 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4787 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5734 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.4801 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5855 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.4769 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5834 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.4761 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6018 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4720 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5822 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.4746 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5816 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4724 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5835 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4681 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5854 - sparse_categorical_accuracy: 0.7804 - val_loss: 0.4678 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5673 - sparse_categorical_accuracy: 0.7927 - val_loss: 0.4689 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5614 - sparse_categorical_accuracy: 0.8015 - val_loss: 0.4604 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5638 - sparse_categorical_accuracy: 0.8021 - val_loss: 0.4590 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5613 - sparse_categorical_accuracy: 0.7910 - val_loss: 0.4578 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5766 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.4573 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.5668 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4578 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5805 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.4571 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5541 - sparse_categorical_accuracy: 0.8068 - val_loss: 0.4546 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 5, test #3, 2nd iteration: loss of 0.4578492045402527; sparse_categorical_accuracy of 84.10000205039978%\n",
            "appending basis + selective feature vector data\n",
            "[192, 2155, 1264, 363, 1554, 234, 387, 26, 742, 274, 2208, 1933, 106, 2568, 976, 980, 240, 2337, 2661, 513, 398, 434, 1081, 660, 1063, 2653, 585, 2200, 2509, 2266, 109, 1262, 1115, 757, 2554, 1991, 1087, 1050, 570, 1118, 132, 266, 782, 2543, 1635, 2286, 1588, 2019, 493, 1433, 1594, 1307, 653, 2227, 1618, 1015, 2013, 2112, 1442, 1019, 644, 949, 1822, 2572, 2224, 1079, 158, 1997, 843, 2666, 1391, 1130, 885, 810, 1049, 2558, 1610, 1674, 894, 2533, 2223, 1426, 2512, 85, 1934, 2644, 1550, 1552, 2659, 1543, 1466, 1615, 241, 1346, 2548, 729, 1277, 1242, 1387, 1185, 50, 2276, 812, 1293, 2254, 2174, 1253, 1808, 422, 2119, 1559, 1231, 2360, 2490, 2489, 1487, 261, 706, 2184, 1521, 1078, 876, 359, 1023, 2435, 676, 548, 1192, 1597, 1540, 740, 2153, 1956, 1114, 1830, 1320, 752, 2622, 1489, 2068, 1607, 872, 1284, 2219, 280, 220, 2393, 174, 1516, 528, 1084, 1777, 2662, 693, 1833, 2352, 960, 2198, 2441, 1946, 193, 414, 2149, 2608, 804, 2073, 2156, 637, 1006, 1302, 1672, 180, 1069, 2603, 2369, 2502, 1203, 2185, 1265, 2329, 2409, 28, 2468, 2514, 1161, 1536, 893, 1477, 849, 538, 1735, 2575, 1211, 1823, 1048, 1895, 655, 1757, 1102, 2456, 991, 1201, 1057, 1748, 1661, 191, 2088, 1397, 1624, 2689, 282, 2084, 1664, 2240, 1500, 509, 946, 2334, 1670, 686, 286, 1849, 2402, 438, 1238, 474, 1535, 880, 793, 1171, 1524, 1167, 2171, 1133, 1431, 2302, 1002, 268, 1721, 2074, 1055, 311, 83, 2173, 2029, 1886, 379, 2520, 1170, 1679, 2321, 221, 354, 1696, 2532, 1619, 148, 932, 2107, 81, 1368, 911, 2517, 490, 1582, 1604, 2264, 448, 796, 237, 111, 1220, 1365, 2461, 1198, 904, 262, 2255, 563, 182, 1662, 2581, 942, 1803, 1708, 769, 2111, 2430, 567, 2242, 510, 2281, 1125, 178, 607, 186, 1168, 2313, 550, 2338, 2650, 2429, 1577, 803, 496, 1648, 815, 616, 1561, 2694, 778, 1860, 1791, 480, 1349, 1825, 1363, 614, 194, 1907, 2259, 743, 1288, 1542, 1411, 2139, 581, 1251, 2407, 1107, 1315, 1569, 642, 313, 1141, 432, 27, 1254, 1306, 958, 1896, 1486, 747, 1120, 546, 2562, 2078, 2033, 295, 415, 666, 772, 151, 1402, 1257, 347, 1378, 260, 2125, 1453, 1369, 935, 1827, 1729, 1585, 620, 100, 1899, 866, 734, 1226, 1596, 325, 1178, 2143, 1564, 1022, 877, 1902, 2135, 2599, 1076, 1093, 1707, 130, 2114, 1476, 648, 1980, 1847, 1398, 1149, 756, 2181, 2179, 520, 2140, 1144, 413, 807, 851, 1668, 1404, 768, 1752, 2333, 2505, 1415, 2414, 410, 2687, 1223, 2482, 1541, 1217, 549, 2406, 1410, 619, 659, 2301, 251, 1449, 172, 1156, 2291, 1004, 2086, 1921, 2516, 1769, 1042, 658, 497, 2282, 464, 707, 20, 1207, 2576, 2362, 2550, 335, 334, 350, 1976, 1147, 1878, 105, 702, 2293, 1967, 229, 1836, 641, 1586, 643, 1566, 198, 1548, 1840, 1469, 2437, 2096, 1704, 479, 1950, 2610, 87, 1499, 1714, 2604, 208, 2304, 1305, 1612, 2488, 1937, 2595, 2009, 1248, 324, 367, 2389, 2391, 1861, 822, 1743, 2600, 1724, 2011, 204, 827, 930, 125, 396, 469, 1818, 1429, 468, 1518, 164, 14, 610, 1529, 169, 2121, 936, 2006, 1229, 352, 978, 1629, 1939, 1506, 875, 1158, 1193, 1381, 684, 608, 2324, 506, 457, 2141, 739, 378, 1070, 2477, 750, 853, 331, 1995, 1241, 2263, 2026, 780, 2244, 42, 1637, 1216, 2427, 1734, 1412, 2098, 2110, 84, 1331, 2647, 1136, 2556, 1330, 141, 306, 792, 245, 1137, 2093, 488, 392, 460, 445, 845, 405, 2190, 2588, 2475, 898, 2580, 80, 2444, 839, 211, 1110, 2064, 1805, 477, 1801, 1792, 2531, 2003, 1260, 705, 1816, 1164, 110, 1952, 763, 1640, 1862, 552, 1638, 965, 291, 2097, 508, 1030, 1502, 2203, 2290, 1437, 411, 1646, 368, 459, 1920, 485, 537, 1296, 155, 640, 1972, 2465, 2115, 881, 236, 436, 2163, 2245, 2643, 2527, 811, 994, 2544, 1394, 431, 2370, 206, 270, 454, 744, 733, 2440, 427, 592, 276, 30, 1959, 2316, 433, 918, 1270, 203, 135, 390, 1214, 779, 195, 2272, 1953, 2563, 1292, 1255, 1710, 952, 1606, 93, 10, 1329, 2645, 1644, 1616, 1037, 1484, 2090, 2045, 372, 2079, 2392, 73, 486, 1964, 1181, 2463, 1676, 2177, 595, 1263, 86, 492, 886, 2703, 1845, 31, 2213, 749, 2522, 121, 2591, 1463, 1864, 1622, 117, 2150, 2412, 0, 2671, 2022, 2636, 1218, 713, 2367, 1197, 2651, 1793, 865, 1140, 504, 218, 2315, 1356, 292, 2016, 727, 2134, 712, 2283, 1355, 222, 1029, 2099, 808, 136, 200, 267, 831, 293, 166, 2625, 1066, 2145, 440, 1194, 2498, 156, 1641, 2322, 821, 2557, 131, 2511, 1186, 1236, 2151, 2031, 1011, 2499, 1621, 1654, 1372, 679, 2236, 1715, 928, 2521, 982, 1195, 257, 1085, 1741, 478, 1462, 2359, 2691, 751, 2690, 232, 1790, 92, 2159, 1111, 1515, 2507, 908, 320, 868, 899, 114, 2471, 1855, 439, 2635, 1112, 2311, 2146, 628, 675, 2547, 1046, 1154, 1922, 1530, 1722, 1221, 1590, 1954, 938, 1981, 144, 1568, 802, 850, 1611, 1196, 2278, 2613, 542, 2627, 794, 53, 1870, 412, 360, 163, 1091, 1965, 1408, 2136, 2317, 1345, 526, 1882, 1656, 1936, 1555, 987, 956, 2222, 401, 999, 47, 1249, 2081, 2170, 2374, 691, 407, 1685, 869, 854, 55, 583, 68, 2458, 223, 1074, 302, 998, 979, 1097, 1399, 1200, 2515, 787, 2378, 1395, 2323, 568, 1879, 1745, 1064, 362, 565, 1379, 393, 2607, 1746, 837, 547, 2210, 129, 846, 13, 2673, 2598, 997, 2542, 101, 1073, 483, 848, 2041, 2207, 858, 383, 797, 56, 760, 1574, 1865, 2486, 1361, 2438, 1856, 446, 2260, 430, 421, 2451, 921, 1300, 290, 70, 1017, 2485, 1094, 67, 986, 584, 406, 1443, 2423, 1119, 1799, 1343, 1058, 2654, 1467, 1754, 2039, 785, 1975, 2400, 2241, 950, 499, 1319, 1843, 1039, 2191, 1496, 2025, 948, 502, 2275, 562, 573, 1874, 333, 1771, 798, 1083, 2664, 2197, 146, 1281, 1675, 1155, 699, 897, 283, 519, 1436, 651, 108, 2080, 330, 1636, 1212, 2346, 64, 1716, 1321, 376, 1098, 2021, 1973, 1796, 344, 2010, 1165, 1182, 721, 2023, 1951, 345, 120, 1352, 2070, 227, 1887, 252, 11, 1014, 634, 2256, 2667, 627, 2043, 1180, 2546, 1427, 1159, 1947, 2285, 1491, 1245, 284, 82, 1360]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 12.0009 - sparse_categorical_accuracy: 0.1669 - val_loss: 4.5302 - val_sparse_categorical_accuracy: 0.1770 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 7.5133 - sparse_categorical_accuracy: 0.1715 - val_loss: 3.1091 - val_sparse_categorical_accuracy: 0.2700 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.0882 - sparse_categorical_accuracy: 0.1833 - val_loss: 2.3289 - val_sparse_categorical_accuracy: 0.2580 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.2783 - sparse_categorical_accuracy: 0.2008 - val_loss: 2.3103 - val_sparse_categorical_accuracy: 0.2860 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.7432 - sparse_categorical_accuracy: 0.2049 - val_loss: 2.1132 - val_sparse_categorical_accuracy: 0.3040 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.2812 - sparse_categorical_accuracy: 0.2131 - val_loss: 1.9562 - val_sparse_categorical_accuracy: 0.3130 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.0567 - sparse_categorical_accuracy: 0.2137 - val_loss: 1.9237 - val_sparse_categorical_accuracy: 0.3260 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.6730 - sparse_categorical_accuracy: 0.2395 - val_loss: 1.7756 - val_sparse_categorical_accuracy: 0.3480 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.1720 - sparse_categorical_accuracy: 0.2301 - val_loss: 1.6049 - val_sparse_categorical_accuracy: 0.3630 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.8752 - sparse_categorical_accuracy: 0.2623 - val_loss: 1.5663 - val_sparse_categorical_accuracy: 0.3820 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5629 - sparse_categorical_accuracy: 0.2652 - val_loss: 1.4517 - val_sparse_categorical_accuracy: 0.4140 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.4785 - sparse_categorical_accuracy: 0.2676 - val_loss: 1.3377 - val_sparse_categorical_accuracy: 0.5170 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.0866 - sparse_categorical_accuracy: 0.3039 - val_loss: 1.2508 - val_sparse_categorical_accuracy: 0.5290 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.9620 - sparse_categorical_accuracy: 0.3191 - val_loss: 1.1441 - val_sparse_categorical_accuracy: 0.5790 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.7307 - sparse_categorical_accuracy: 0.3255 - val_loss: 1.1746 - val_sparse_categorical_accuracy: 0.5630 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4473 - sparse_categorical_accuracy: 0.3700 - val_loss: 1.0854 - val_sparse_categorical_accuracy: 0.6070 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4064 - sparse_categorical_accuracy: 0.3794 - val_loss: 1.0529 - val_sparse_categorical_accuracy: 0.6130 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2043 - sparse_categorical_accuracy: 0.3782 - val_loss: 1.0200 - val_sparse_categorical_accuracy: 0.6380 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1054 - sparse_categorical_accuracy: 0.3905 - val_loss: 0.9416 - val_sparse_categorical_accuracy: 0.6740 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9533 - sparse_categorical_accuracy: 0.4169 - val_loss: 0.9336 - val_sparse_categorical_accuracy: 0.6570 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8418 - sparse_categorical_accuracy: 0.4379 - val_loss: 0.9050 - val_sparse_categorical_accuracy: 0.6940 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7301 - sparse_categorical_accuracy: 0.4578 - val_loss: 0.8547 - val_sparse_categorical_accuracy: 0.7280 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7373 - sparse_categorical_accuracy: 0.4561 - val_loss: 0.8521 - val_sparse_categorical_accuracy: 0.7220 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5515 - sparse_categorical_accuracy: 0.4883 - val_loss: 0.8318 - val_sparse_categorical_accuracy: 0.7190 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5302 - sparse_categorical_accuracy: 0.4871 - val_loss: 0.8141 - val_sparse_categorical_accuracy: 0.7310 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4477 - sparse_categorical_accuracy: 0.5012 - val_loss: 0.7889 - val_sparse_categorical_accuracy: 0.7370 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3821 - sparse_categorical_accuracy: 0.5252 - val_loss: 0.7804 - val_sparse_categorical_accuracy: 0.7380 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3307 - sparse_categorical_accuracy: 0.5345 - val_loss: 0.7526 - val_sparse_categorical_accuracy: 0.7610 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2656 - sparse_categorical_accuracy: 0.5673 - val_loss: 0.7500 - val_sparse_categorical_accuracy: 0.7680 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2279 - sparse_categorical_accuracy: 0.5714 - val_loss: 0.7372 - val_sparse_categorical_accuracy: 0.7580 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2182 - sparse_categorical_accuracy: 0.5779 - val_loss: 0.7195 - val_sparse_categorical_accuracy: 0.7640 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1489 - sparse_categorical_accuracy: 0.5808 - val_loss: 0.7013 - val_sparse_categorical_accuracy: 0.7700 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0921 - sparse_categorical_accuracy: 0.6194 - val_loss: 0.6918 - val_sparse_categorical_accuracy: 0.7770 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0976 - sparse_categorical_accuracy: 0.6165 - val_loss: 0.6747 - val_sparse_categorical_accuracy: 0.7800 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0549 - sparse_categorical_accuracy: 0.6095 - val_loss: 0.6703 - val_sparse_categorical_accuracy: 0.7770 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0073 - sparse_categorical_accuracy: 0.6288 - val_loss: 0.6583 - val_sparse_categorical_accuracy: 0.7810 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9875 - sparse_categorical_accuracy: 0.6505 - val_loss: 0.6652 - val_sparse_categorical_accuracy: 0.7670 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9639 - sparse_categorical_accuracy: 0.6563 - val_loss: 0.6498 - val_sparse_categorical_accuracy: 0.7780 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9471 - sparse_categorical_accuracy: 0.6639 - val_loss: 0.6342 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9112 - sparse_categorical_accuracy: 0.6721 - val_loss: 0.6334 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9279 - sparse_categorical_accuracy: 0.6516 - val_loss: 0.6261 - val_sparse_categorical_accuracy: 0.7830 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8646 - sparse_categorical_accuracy: 0.6821 - val_loss: 0.6189 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8727 - sparse_categorical_accuracy: 0.6792 - val_loss: 0.6113 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8390 - sparse_categorical_accuracy: 0.6915 - val_loss: 0.6053 - val_sparse_categorical_accuracy: 0.7950 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8186 - sparse_categorical_accuracy: 0.6956 - val_loss: 0.5935 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8364 - sparse_categorical_accuracy: 0.6879 - val_loss: 0.5941 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8003 - sparse_categorical_accuracy: 0.7043 - val_loss: 0.5956 - val_sparse_categorical_accuracy: 0.7860 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8033 - sparse_categorical_accuracy: 0.7090 - val_loss: 0.5843 - val_sparse_categorical_accuracy: 0.8040 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7922 - sparse_categorical_accuracy: 0.7002 - val_loss: 0.5761 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7513 - sparse_categorical_accuracy: 0.7119 - val_loss: 0.5684 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7663 - sparse_categorical_accuracy: 0.7225 - val_loss: 0.5647 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7753 - sparse_categorical_accuracy: 0.7207 - val_loss: 0.5606 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7126 - sparse_categorical_accuracy: 0.7295 - val_loss: 0.5540 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7505 - sparse_categorical_accuracy: 0.7160 - val_loss: 0.5526 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7168 - sparse_categorical_accuracy: 0.7354 - val_loss: 0.5480 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7092 - sparse_categorical_accuracy: 0.7436 - val_loss: 0.5469 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7152 - sparse_categorical_accuracy: 0.7418 - val_loss: 0.5411 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7181 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.5378 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7004 - sparse_categorical_accuracy: 0.7359 - val_loss: 0.5329 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6832 - sparse_categorical_accuracy: 0.7553 - val_loss: 0.5335 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6781 - sparse_categorical_accuracy: 0.7459 - val_loss: 0.5241 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6684 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.5219 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6609 - sparse_categorical_accuracy: 0.7611 - val_loss: 0.5202 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6622 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.5150 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6795 - sparse_categorical_accuracy: 0.7459 - val_loss: 0.5134 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6511 - sparse_categorical_accuracy: 0.7547 - val_loss: 0.5118 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6646 - sparse_categorical_accuracy: 0.7465 - val_loss: 0.5087 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6248 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.5022 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6305 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.5000 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6158 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.4956 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6379 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.4953 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6363 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.4970 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6285 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.4899 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6235 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.4927 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6268 - sparse_categorical_accuracy: 0.7635 - val_loss: 0.4871 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5987 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.4890 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5942 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.4806 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5979 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.4797 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5996 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.4729 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6027 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.4754 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5931 - sparse_categorical_accuracy: 0.7904 - val_loss: 0.4749 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5724 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.4664 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6157 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.4653 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5962 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4676 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5691 - sparse_categorical_accuracy: 0.7951 - val_loss: 0.4672 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5935 - sparse_categorical_accuracy: 0.7758 - val_loss: 0.4646 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5766 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4586 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5606 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.4631 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.5646 - sparse_categorical_accuracy: 0.7904 - val_loss: 0.4573 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5779 - sparse_categorical_accuracy: 0.7916 - val_loss: 0.4563 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5626 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4561 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5651 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.4578 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5683 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4539 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5471 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4509 - val_sparse_categorical_accuracy: 0.8470 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5540 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.4480 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5517 - sparse_categorical_accuracy: 0.8015 - val_loss: 0.4498 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5567 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4453 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5634 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4470 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5733 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.4514 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5608 - sparse_categorical_accuracy: 0.7904 - val_loss: 0.4434 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 5, test #4, 2nd iteration: loss of 0.4508742392063141; sparse_categorical_accuracy of 84.7000002861023%\n",
            "appending basis + selective feature vector data\n",
            "[1572, 1496, 2152, 860, 1669, 2597, 1474, 1177, 1939, 698, 1543, 704, 90, 2006, 798, 1671, 1821, 521, 1329, 1268, 1290, 219, 1596, 2143, 1271, 576, 1232, 209, 503, 2693, 2535, 2164, 1832, 1096, 1612, 1425, 2149, 2055, 1365, 2371, 703, 636, 431, 1116, 665, 2514, 307, 366, 1767, 2471, 1569, 2647, 2260, 80, 1182, 1609, 1840, 2590, 2349, 837, 1033, 500, 794, 418, 285, 1559, 1998, 982, 1170, 1592, 1930, 1051, 232, 2355, 1335, 1087, 992, 811, 2641, 1042, 756, 292, 2689, 1192, 544, 302, 174, 763, 2491, 2469, 2425, 1444, 777, 1476, 658, 1999, 1280, 2224, 1732, 1052, 2208, 1751, 242, 1884, 170, 732, 1808, 323, 1949, 912, 1301, 1911, 1856, 143, 1881, 1660, 1637, 1979, 2385, 802, 1187, 481, 2598, 2438, 2457, 2499, 274, 1161, 1498, 1216, 2624, 565, 2110, 502, 2156, 549, 15, 640, 271, 328, 2068, 448, 983, 938, 2070, 293, 214, 33, 1397, 357, 1865, 1736, 1419, 2604, 2027, 17, 735, 142, 1966, 985, 27, 842, 480, 1446, 2517, 2279, 2596, 1362, 1619, 2148, 479, 2667, 1073, 254, 714, 713, 429, 1490, 674, 404, 1873, 2183, 1851, 1105, 1403, 1152, 839, 616, 1670, 192, 1229, 545, 2634, 314, 1848, 2008, 988, 967, 1098, 1734, 2346, 2402, 1327, 2571, 2052, 2488, 1247, 2089, 1508, 1043, 2684, 2036, 1738, 1205, 1275, 2081, 1311, 2128, 1960, 2572, 2266, 2368, 670, 1101, 2160, 2630, 2441, 1639, 2417, 278, 1399, 2087, 1635, 1647, 439, 1754, 677, 541, 1198, 902, 1664, 2289, 101, 1128, 2194, 761, 1632, 266, 1159, 88, 996, 169, 1519, 1279, 408, 30, 2334, 2682, 1039, 1017, 157, 1270, 2118, 1689, 1891, 1836, 124, 1542, 1570, 1293, 1197, 1129, 2436, 85, 880, 1014, 1226, 858, 2235, 627, 2256, 1480, 1595, 21, 1457, 1643, 57, 945, 348, 1994, 2054, 452, 2228, 2010, 1409, 1108, 994, 2381, 2200, 1787, 486, 2225, 725, 2307, 1818, 2298, 845, 470, 1951, 532, 2094, 1277, 79, 462, 288, 2157, 1812, 122, 155, 821, 918, 2009, 2403, 2050, 371, 806, 2347, 186, 1828, 2592, 2479, 2001, 2310, 805, 1426, 1799, 623, 1667, 1259, 1202, 1679, 135, 966, 1340, 991, 2405, 2541, 1278, 396, 830, 1363, 644, 1675, 571, 126, 867, 1118, 2141, 1102, 188, 318, 92, 1729, 2691, 738, 317, 682, 2522, 185, 1374, 1876, 647, 1764, 1605, 1459, 315, 1852, 524, 675, 1036, 2024, 1915, 2649, 1601, 238, 877, 303, 1516, 582, 507, 2704, 1218, 1139, 350, 2187, 1378, 2046, 1031, 1195, 1010, 229, 299, 2646, 216, 2664, 378, 979, 1907, 1726, 1587, 2570, 1148, 2348, 60, 971, 577, 1254, 768, 1208, 2179, 1656, 1291, 663, 540, 1286, 1773, 2327, 140, 1354, 2139, 787, 2005, 993, 1273, 1322, 1784, 1971, 1686, 1267, 1392, 2452, 1412, 1946, 715, 252, 1962, 1739, 2626, 995, 2186, 568, 338, 2323, 1173, 2003, 1523, 1234, 436, 1649, 2614, 886, 2584, 2169, 260, 45, 1814, 2142, 934, 91, 2397, 1728, 2352, 536, 1598, 1744, 1372, 1415, 2062, 801, 2171, 2018, 2677, 1375, 873, 676, 2673, 2529, 2607, 597, 2201, 594, 861, 639, 1708, 1800, 40, 316, 74, 1228, 1175, 2563, 528, 87, 1520, 896, 2632, 201, 667, 1931, 1618, 1627, 2165, 13, 1769, 1505, 276, 728, 82, 1302, 1843, 1231, 750, 1925, 2351, 1795, 1104, 558, 287, 2213, 767, 2386, 342, 34, 2032, 496, 8, 836, 946, 2650, 1548, 1961, 2099, 2532, 956, 2302, 2576, 748, 904, 841, 2017, 1866, 160, 1318, 771, 1501, 1240, 2063, 1807, 865, 2069, 258, 1191, 572, 2413, 1644, 1097, 2463, 997, 1062, 1390, 1136, 2540, 369, 823, 97, 876, 1309, 1634, 816, 1972, 330, 601, 2185, 891, 741, 2464, 2182, 931, 1299, 1796, 614, 202, 289, 1165, 1420, 526, 2406, 1803, 401, 921, 83, 1347, 716, 1077, 516, 1089, 298, 2161, 2184, 441, 776, 2395, 364, 438, 1448, 697, 38, 2257, 2007, 1688, 887, 1274, 963, 2702, 2451, 1730, 2387, 256, 1657, 2021, 227, 2480, 2170, 628, 395, 1566, 1172, 2443, 1323, 2080, 2507, 2663, 632, 980, 2252, 998, 1983, 440, 61, 1537, 1407, 2560, 2574, 659, 1339, 618, 2544, 604, 1356, 1315, 1287, 421, 960, 543, 1078, 2315, 1810, 329, 692, 1759, 2470, 1860, 1120, 607, 2687, 1046, 2105, 2555, 433, 1203, 1009, 1645, 368, 2683, 605, 892, 560, 1037, 1883, 2608, 694, 1741, 2291, 458, 2267, 592, 2272, 1965, 325, 2431, 351, 2393, 96, 2483, 1833, 1975, 1027, 2015, 1651, 598, 2106, 972, 2137, 2206, 356, 1714, 1477, 68, 2273, 1350, 94, 478, 881, 1005, 2412, 505, 1512, 1348, 2193, 1238, 784, 1292, 2549, 1909, 1035, 2581, 1668, 1261, 1066, 1389, 1071, 1772, 1831, 2414, 2116, 976, 863, 2306, 1849, 1482, 2303, 655, 2498, 2475, 2255, 662, 1662, 279, 1421, 2019, 1088, 2230, 150, 1468, 824, 1976, 2559, 187, 53, 326, 1393, 740, 2, 1410, 1504, 1981, 26, 2561, 2506, 1690, 2472, 2460, 2296, 781, 2078, 875, 2433, 1781, 2311, 567, 1887, 1400, 2415, 1382, 253, 1550, 1053, 1258, 2410, 301, 346, 1536, 1386, 701, 2223, 1007, 1305, 1406, 1859, 1720, 1151, 78, 901, 2558, 1028, 365, 306, 611, 924, 2445, 2034, 2363, 525, 2239, 1434, 2287, 377, 1132, 1798, 2195, 1019, 2537, 730, 133, 1484, 1980, 2284, 1483, 2424, 1654, 2031, 792, 1223, 2542, 1924, 900, 2033, 173, 2685, 826, 178, 1969, 184, 2619, 1922, 1903, 2519, 1346, 780, 1682, 1879, 2378, 1110, 2222, 1867, 917, 2209, 2670, 151, 574, 2073, 1371, 2269, 1541, 1557, 1551, 2281, 1134, 98, 1636, 2242, 4, 2221, 2398, 1659, 327, 831, 257, 1802, 1294, 1604, 1207, 58, 2625, 1509, 2138, 49, 432, 2694, 634, 1584, 559, 1528, 1253, 406, 2053, 1607, 102, 2338, 2331, 974, 986, 519, 1414, 2697, 791, 103, 2335, 1780, 990, 1761, 360, 2678, 819, 118, 1464, 1215, 944, 1912, 220, 2029, 2042, 466, 1692, 1982, 1850, 1941, 1481, 1150, 1076, 2123, 109, 1676, 2057, 2668, 1906, 390, 148, 412, 1284, 1190, 168, 2588, 213, 810, 1788, 483, 2390, 1206, 1631, 1168, 32, 1157, 240, 720, 683, 277, 2199, 1308, 1082, 1819, 2356, 2051, 759, 1146, 2067, 2210, 866, 2680, 1691, 363, 447, 2461, 221, 426, 1967, 2136, 1221, 2652, 1326, 2350, 2231, 2226, 1135, 2304, 1262, 1593, 1411, 894, 1661]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 9.3551 - sparse_categorical_accuracy: 0.1423 - val_loss: 2.6725 - val_sparse_categorical_accuracy: 0.2940 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.4871 - sparse_categorical_accuracy: 0.1985 - val_loss: 2.4743 - val_sparse_categorical_accuracy: 0.3140 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.1142 - sparse_categorical_accuracy: 0.1803 - val_loss: 2.2862 - val_sparse_categorical_accuracy: 0.3230 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.6747 - sparse_categorical_accuracy: 0.1739 - val_loss: 2.0549 - val_sparse_categorical_accuracy: 0.3320 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.1667 - sparse_categorical_accuracy: 0.1979 - val_loss: 1.9286 - val_sparse_categorical_accuracy: 0.3680 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.6758 - sparse_categorical_accuracy: 0.2178 - val_loss: 1.7009 - val_sparse_categorical_accuracy: 0.3760 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.0396 - sparse_categorical_accuracy: 0.2365 - val_loss: 1.5502 - val_sparse_categorical_accuracy: 0.4270 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.7892 - sparse_categorical_accuracy: 0.2354 - val_loss: 1.4928 - val_sparse_categorical_accuracy: 0.4440 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.4397 - sparse_categorical_accuracy: 0.2477 - val_loss: 1.4553 - val_sparse_categorical_accuracy: 0.4540 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 4.3207 - sparse_categorical_accuracy: 0.2529 - val_loss: 1.3206 - val_sparse_categorical_accuracy: 0.4990 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.9817 - sparse_categorical_accuracy: 0.2641 - val_loss: 1.3326 - val_sparse_categorical_accuracy: 0.4690 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.6117 - sparse_categorical_accuracy: 0.2787 - val_loss: 1.2090 - val_sparse_categorical_accuracy: 0.5350 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.2740 - sparse_categorical_accuracy: 0.3044 - val_loss: 1.2112 - val_sparse_categorical_accuracy: 0.5030 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 3.2402 - sparse_categorical_accuracy: 0.2974 - val_loss: 1.0748 - val_sparse_categorical_accuracy: 0.6090 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.9119 - sparse_categorical_accuracy: 0.3185 - val_loss: 1.0915 - val_sparse_categorical_accuracy: 0.5780 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.7082 - sparse_categorical_accuracy: 0.3285 - val_loss: 1.0237 - val_sparse_categorical_accuracy: 0.6400 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5311 - sparse_categorical_accuracy: 0.3454 - val_loss: 0.9838 - val_sparse_categorical_accuracy: 0.6340 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4452 - sparse_categorical_accuracy: 0.3735 - val_loss: 0.9561 - val_sparse_categorical_accuracy: 0.6590 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1305 - sparse_categorical_accuracy: 0.4104 - val_loss: 0.9012 - val_sparse_categorical_accuracy: 0.6960 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0733 - sparse_categorical_accuracy: 0.4104 - val_loss: 0.8752 - val_sparse_categorical_accuracy: 0.7000 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9662 - sparse_categorical_accuracy: 0.4204 - val_loss: 0.8590 - val_sparse_categorical_accuracy: 0.7030 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9385 - sparse_categorical_accuracy: 0.4391 - val_loss: 0.8374 - val_sparse_categorical_accuracy: 0.7200 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8353 - sparse_categorical_accuracy: 0.4491 - val_loss: 0.8257 - val_sparse_categorical_accuracy: 0.7190 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6904 - sparse_categorical_accuracy: 0.4655 - val_loss: 0.8163 - val_sparse_categorical_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6334 - sparse_categorical_accuracy: 0.4725 - val_loss: 0.7882 - val_sparse_categorical_accuracy: 0.7380 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4827 - sparse_categorical_accuracy: 0.5006 - val_loss: 0.7721 - val_sparse_categorical_accuracy: 0.7450 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5125 - sparse_categorical_accuracy: 0.5064 - val_loss: 0.7588 - val_sparse_categorical_accuracy: 0.7600 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4177 - sparse_categorical_accuracy: 0.5228 - val_loss: 0.7445 - val_sparse_categorical_accuracy: 0.7610 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3641 - sparse_categorical_accuracy: 0.5211 - val_loss: 0.7382 - val_sparse_categorical_accuracy: 0.7580 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2952 - sparse_categorical_accuracy: 0.5433 - val_loss: 0.7213 - val_sparse_categorical_accuracy: 0.7810 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2713 - sparse_categorical_accuracy: 0.5673 - val_loss: 0.7112 - val_sparse_categorical_accuracy: 0.7790 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2562 - sparse_categorical_accuracy: 0.5621 - val_loss: 0.7068 - val_sparse_categorical_accuracy: 0.7790 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1911 - sparse_categorical_accuracy: 0.5638 - val_loss: 0.7069 - val_sparse_categorical_accuracy: 0.7670 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1989 - sparse_categorical_accuracy: 0.5849 - val_loss: 0.6843 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1367 - sparse_categorical_accuracy: 0.5937 - val_loss: 0.6819 - val_sparse_categorical_accuracy: 0.7850 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0843 - sparse_categorical_accuracy: 0.5948 - val_loss: 0.6769 - val_sparse_categorical_accuracy: 0.7860 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1043 - sparse_categorical_accuracy: 0.5749 - val_loss: 0.6640 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0459 - sparse_categorical_accuracy: 0.6194 - val_loss: 0.6586 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9839 - sparse_categorical_accuracy: 0.6393 - val_loss: 0.6548 - val_sparse_categorical_accuracy: 0.7880 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9899 - sparse_categorical_accuracy: 0.6399 - val_loss: 0.6454 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9153 - sparse_categorical_accuracy: 0.6499 - val_loss: 0.6400 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9023 - sparse_categorical_accuracy: 0.6622 - val_loss: 0.6372 - val_sparse_categorical_accuracy: 0.7890 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8941 - sparse_categorical_accuracy: 0.6704 - val_loss: 0.6316 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8743 - sparse_categorical_accuracy: 0.6786 - val_loss: 0.6244 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8708 - sparse_categorical_accuracy: 0.6751 - val_loss: 0.6178 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8495 - sparse_categorical_accuracy: 0.6879 - val_loss: 0.6125 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8312 - sparse_categorical_accuracy: 0.6862 - val_loss: 0.6080 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8423 - sparse_categorical_accuracy: 0.6815 - val_loss: 0.6043 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8297 - sparse_categorical_accuracy: 0.6838 - val_loss: 0.6022 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8141 - sparse_categorical_accuracy: 0.6915 - val_loss: 0.5949 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8013 - sparse_categorical_accuracy: 0.6961 - val_loss: 0.5901 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7854 - sparse_categorical_accuracy: 0.7149 - val_loss: 0.5869 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7723 - sparse_categorical_accuracy: 0.7125 - val_loss: 0.5819 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7545 - sparse_categorical_accuracy: 0.7143 - val_loss: 0.5793 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7477 - sparse_categorical_accuracy: 0.7301 - val_loss: 0.5761 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7215 - sparse_categorical_accuracy: 0.7377 - val_loss: 0.5729 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7184 - sparse_categorical_accuracy: 0.7266 - val_loss: 0.5678 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6965 - sparse_categorical_accuracy: 0.7412 - val_loss: 0.5634 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7037 - sparse_categorical_accuracy: 0.7307 - val_loss: 0.5613 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6757 - sparse_categorical_accuracy: 0.7400 - val_loss: 0.5613 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6816 - sparse_categorical_accuracy: 0.7482 - val_loss: 0.5557 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6481 - sparse_categorical_accuracy: 0.7541 - val_loss: 0.5511 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6855 - sparse_categorical_accuracy: 0.7453 - val_loss: 0.5478 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6705 - sparse_categorical_accuracy: 0.7412 - val_loss: 0.5485 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6797 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.5423 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6516 - sparse_categorical_accuracy: 0.7576 - val_loss: 0.5389 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6527 - sparse_categorical_accuracy: 0.7564 - val_loss: 0.5397 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6315 - sparse_categorical_accuracy: 0.7588 - val_loss: 0.5344 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6575 - sparse_categorical_accuracy: 0.7553 - val_loss: 0.5292 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6333 - sparse_categorical_accuracy: 0.7641 - val_loss: 0.5294 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6397 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.5271 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6201 - sparse_categorical_accuracy: 0.7758 - val_loss: 0.5282 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6154 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.5239 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6057 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.5197 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6024 - sparse_categorical_accuracy: 0.7851 - val_loss: 0.5169 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5889 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.5133 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6103 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.5105 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5675 - sparse_categorical_accuracy: 0.7963 - val_loss: 0.5102 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5897 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.5059 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5790 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.5059 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6077 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.5021 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5693 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.5043 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5948 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.5006 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5749 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4950 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5818 - sparse_categorical_accuracy: 0.7822 - val_loss: 0.4932 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5864 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.4987 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5577 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4909 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5461 - sparse_categorical_accuracy: 0.7922 - val_loss: 0.4892 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7881 - val_loss: 0.4883 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5646 - sparse_categorical_accuracy: 0.7951 - val_loss: 0.4881 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5529 - sparse_categorical_accuracy: 0.8056 - val_loss: 0.4818 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5423 - sparse_categorical_accuracy: 0.8033 - val_loss: 0.4845 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5396 - sparse_categorical_accuracy: 0.7992 - val_loss: 0.4811 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5520 - sparse_categorical_accuracy: 0.7881 - val_loss: 0.4814 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5364 - sparse_categorical_accuracy: 0.8027 - val_loss: 0.4811 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5283 - sparse_categorical_accuracy: 0.8015 - val_loss: 0.4765 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5375 - sparse_categorical_accuracy: 0.8004 - val_loss: 0.4742 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5459 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4758 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5368 - sparse_categorical_accuracy: 0.7910 - val_loss: 0.4712 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5464 - sparse_categorical_accuracy: 0.7916 - val_loss: 0.4677 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 6, test #0, 2nd iteration: loss of 0.4814126789569855; sparse_categorical_accuracy of 83.60000252723694%\n",
            "appending basis + selective feature vector data\n",
            "[1572, 1496, 2152, 860, 1669, 2597, 1474, 1177, 1939, 698, 1543, 704, 90, 2006, 798, 1671, 1821, 521, 1329, 1268, 1290, 219, 1596, 2143, 1271, 576, 1232, 209, 503, 2693, 2535, 2164, 1832, 1096, 1612, 1425, 2149, 2055, 1365, 2371, 703, 636, 431, 1116, 665, 2514, 307, 366, 1767, 2471, 1569, 2647, 2260, 80, 1182, 1609, 1840, 2590, 2349, 837, 1033, 500, 794, 418, 285, 1559, 1998, 982, 1170, 1592, 1930, 1051, 232, 2355, 1335, 1087, 992, 811, 2641, 1042, 756, 292, 2689, 1192, 544, 302, 174, 763, 2491, 2469, 2425, 1444, 777, 1476, 658, 1999, 1280, 2224, 1732, 1052, 2208, 1751, 242, 1884, 170, 732, 1808, 323, 1949, 912, 1301, 1911, 1856, 143, 1881, 1660, 1637, 1979, 2385, 802, 1187, 481, 2598, 2438, 2457, 2499, 274, 1161, 1498, 1216, 2624, 565, 2110, 502, 2156, 549, 15, 640, 271, 328, 2068, 448, 983, 938, 2070, 293, 214, 33, 1397, 357, 1865, 1736, 1419, 2604, 2027, 17, 735, 142, 1966, 985, 27, 842, 480, 1446, 2517, 2279, 2596, 1362, 1619, 2148, 479, 2667, 1073, 254, 714, 713, 429, 1490, 674, 404, 1873, 2183, 1851, 1105, 1403, 1152, 839, 616, 1670, 192, 1229, 545, 2634, 314, 1848, 2008, 988, 967, 1098, 1734, 2346, 2402, 1327, 2571, 2052, 2488, 1247, 2089, 1508, 1043, 2684, 2036, 1738, 1205, 1275, 2081, 1311, 2128, 1960, 2572, 2266, 2368, 670, 1101, 2160, 2630, 2441, 1639, 2417, 278, 1399, 2087, 1635, 1647, 439, 1754, 677, 541, 1198, 902, 1664, 2289, 101, 1128, 2194, 761, 1632, 266, 1159, 88, 996, 169, 1519, 1279, 408, 30, 2334, 2682, 1039, 1017, 157, 1270, 2118, 1689, 1891, 1836, 124, 1542, 1570, 1293, 1197, 1129, 2436, 85, 880, 1014, 1226, 858, 2235, 627, 2256, 1480, 1595, 21, 1457, 1643, 57, 945, 348, 1994, 2054, 452, 2228, 2010, 1409, 1108, 994, 2381, 2200, 1787, 486, 2225, 725, 2307, 1818, 2298, 845, 470, 1951, 532, 2094, 1277, 79, 462, 288, 2157, 1812, 122, 155, 821, 918, 2009, 2403, 2050, 371, 806, 2347, 186, 1828, 2592, 2479, 2001, 2310, 805, 1426, 1799, 623, 1667, 1259, 1202, 1679, 135, 966, 1340, 991, 2405, 2541, 1278, 396, 830, 1363, 644, 1675, 571, 126, 867, 1118, 2141, 1102, 188, 318, 92, 1729, 2691, 738, 317, 682, 2522, 185, 1374, 1876, 647, 1764, 1605, 1459, 315, 1852, 524, 675, 1036, 2024, 1915, 2649, 1601, 238, 877, 303, 1516, 582, 507, 2704, 1218, 1139, 350, 2187, 1378, 2046, 1031, 1195, 1010, 229, 299, 2646, 216, 2664, 378, 979, 1907, 1726, 1587, 2570, 1148, 2348, 60, 971, 577, 1254, 768, 1208, 2179, 1656, 1291, 663, 540, 1286, 1773, 2327, 140, 1354, 2139, 787, 2005, 993, 1273, 1322, 1784, 1971, 1686, 1267, 1392, 2452, 1412, 1946, 715, 252, 1962, 1739, 2626, 995, 2186, 568, 338, 2323, 1173, 2003, 1523, 1234, 436, 1649, 2614, 886, 2584, 2169, 260, 45, 1814, 2142, 934, 91, 2397, 1728, 2352, 536, 1598, 1744, 1372, 1415, 2062, 801, 2171, 2018, 2677, 1375, 873, 676, 2673, 2529, 2607, 597, 2201, 594, 861, 639, 1708, 1800, 40, 316, 74, 1228, 1175, 2563, 528, 87, 1520, 896, 2632, 201, 667, 1931, 1618, 1627, 2165, 13, 1769, 1505, 276, 728, 82, 1302, 1843, 1231, 750, 1925, 2351, 1795, 1104, 558, 287, 2213, 767, 2386, 342, 34, 2032, 496, 8, 836, 946, 2650, 1548, 1961, 2099, 2532, 956, 2302, 2576, 748, 904, 841, 2017, 1866, 160, 1318, 771, 1501, 1240, 2063, 1807, 865, 2069, 258, 1191, 572, 2413, 1644, 1097, 2463, 997, 1062, 1390, 1136, 2540, 369, 823, 97, 876, 1309, 1634, 816, 1972, 330, 601, 2185, 891, 741, 2464, 2182, 931, 1299, 1796, 614, 202, 289, 1165, 1420, 526, 2406, 1803, 401, 921, 83, 1347, 716, 1077, 516, 1089, 298, 2161, 2184, 441, 776, 2395, 364, 438, 1448, 697, 38, 2257, 2007, 1688, 887, 1274, 963, 2702, 2451, 1730, 2387, 256, 1657, 2021, 227, 2480, 2170, 628, 395, 1566, 1172, 2443, 1323, 2080, 2507, 2663, 632, 980, 2252, 998, 1983, 440, 61, 1537, 1407, 2560, 2574, 659, 1339, 618, 2544, 604, 1356, 1315, 1287, 421, 960, 543, 1078, 2315, 1810, 329, 692, 1759, 2470, 1860, 1120, 607, 2687, 1046, 2105, 2555, 433, 1203, 1009, 1645, 368, 2683, 605, 892, 560, 1037, 1883, 2608, 694, 1741, 2291, 458, 2267, 592, 2272, 1965, 325, 2431, 351, 2393, 96, 2483, 1833, 1975, 1027, 2015, 1651, 598, 2106, 972, 2137, 2206, 356, 1714, 1477, 68, 2273, 1350, 94, 478, 881, 1005, 2412, 505, 1512, 1348, 2193, 1238, 784, 1292, 2549, 1909, 1035, 2581, 1668, 1261, 1066, 1389, 1071, 1772, 1831, 2414, 2116, 976, 863, 2306, 1849, 1482, 2303, 655, 2498, 2475, 2255, 662, 1662, 279, 1421, 2019, 1088, 2230, 150, 1468, 824, 1976, 2559, 187, 53, 326, 1393, 740, 2, 1410, 1504, 1981, 26, 2561, 2506, 1690, 2472, 2460, 2296, 781, 2078, 875, 2433, 1781, 2311, 567, 1887, 1400, 2415, 1382, 253, 1550, 1053, 1258, 2410, 301, 346, 1536, 1386, 701, 2223, 1007, 1305, 1406, 1859, 1720, 1151, 78, 901, 2558, 1028, 365, 306, 611, 924, 2445, 2034, 2363, 525, 2239, 1434, 2287, 377, 1132, 1798, 2195, 1019, 2537, 730, 133, 1484, 1980, 2284, 1483, 2424, 1654, 2031, 792, 1223, 2542, 1924, 900, 2033, 173, 2685, 826, 178, 1969, 184, 2619, 1922, 1903, 2519, 1346, 780, 1682, 1879, 2378, 1110, 2222, 1867, 917, 2209, 2670, 151, 574, 2073, 1371, 2269, 1541, 1557, 1551, 2281, 1134, 98, 1636, 2242, 4, 2221, 2398, 1659, 327, 831, 257, 1802, 1294, 1604, 1207, 58, 2625, 1509, 2138, 49, 432, 2694, 634, 1584, 559, 1528, 1253, 406, 2053, 1607, 102, 2338, 2331, 974, 986, 519, 1414, 2697, 791, 103, 2335, 1780, 990, 1761, 360, 2678, 819, 118, 1464, 1215, 944, 1912, 220, 2029, 2042, 466, 1692, 1982, 1850, 1941, 1481, 1150, 1076, 2123, 109, 1676, 2057, 2668, 1906, 390, 148, 412, 1284, 1190, 168, 2588, 213, 810, 1788, 483, 2390, 1206, 1631, 1168, 32, 1157, 240, 720, 683, 277, 2199, 1308, 1082, 1819, 2356, 2051, 759, 1146, 2067, 2210, 866, 2680, 1691, 363, 447, 2461, 221, 426, 1967, 2136, 1221, 2652, 1326, 2350, 2231, 2226, 1135, 2304, 1262, 1593, 1411, 894, 1661]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 11.5298 - sparse_categorical_accuracy: 0.1434 - val_loss: 4.1138 - val_sparse_categorical_accuracy: 0.3190 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 8.2036 - sparse_categorical_accuracy: 0.1891 - val_loss: 2.7665 - val_sparse_categorical_accuracy: 0.3140 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.4449 - sparse_categorical_accuracy: 0.1874 - val_loss: 2.6926 - val_sparse_categorical_accuracy: 0.3270 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.1421 - sparse_categorical_accuracy: 0.1920 - val_loss: 2.6356 - val_sparse_categorical_accuracy: 0.3300 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.6697 - sparse_categorical_accuracy: 0.2043 - val_loss: 2.3097 - val_sparse_categorical_accuracy: 0.3390 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.1035 - sparse_categorical_accuracy: 0.2078 - val_loss: 2.2723 - val_sparse_categorical_accuracy: 0.3320 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.6139 - sparse_categorical_accuracy: 0.2307 - val_loss: 1.9480 - val_sparse_categorical_accuracy: 0.3670 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.2126 - sparse_categorical_accuracy: 0.2301 - val_loss: 1.7778 - val_sparse_categorical_accuracy: 0.3840 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.9515 - sparse_categorical_accuracy: 0.2418 - val_loss: 1.6201 - val_sparse_categorical_accuracy: 0.4040 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.5363 - sparse_categorical_accuracy: 0.2518 - val_loss: 1.5669 - val_sparse_categorical_accuracy: 0.4160 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.1319 - sparse_categorical_accuracy: 0.2605 - val_loss: 1.5512 - val_sparse_categorical_accuracy: 0.4130 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.8463 - sparse_categorical_accuracy: 0.2804 - val_loss: 1.2872 - val_sparse_categorical_accuracy: 0.4940 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.7271 - sparse_categorical_accuracy: 0.2898 - val_loss: 1.2095 - val_sparse_categorical_accuracy: 0.5060 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5548 - sparse_categorical_accuracy: 0.2951 - val_loss: 1.2407 - val_sparse_categorical_accuracy: 0.5070 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1935 - sparse_categorical_accuracy: 0.3220 - val_loss: 1.0700 - val_sparse_categorical_accuracy: 0.5780 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.0595 - sparse_categorical_accuracy: 0.3214 - val_loss: 1.0311 - val_sparse_categorical_accuracy: 0.5960 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8209 - sparse_categorical_accuracy: 0.3413 - val_loss: 1.0434 - val_sparse_categorical_accuracy: 0.6020 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.6508 - sparse_categorical_accuracy: 0.3466 - val_loss: 0.9242 - val_sparse_categorical_accuracy: 0.6670 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4188 - sparse_categorical_accuracy: 0.3718 - val_loss: 0.9438 - val_sparse_categorical_accuracy: 0.6520 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2711 - sparse_categorical_accuracy: 0.3905 - val_loss: 0.8913 - val_sparse_categorical_accuracy: 0.6780 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3278 - sparse_categorical_accuracy: 0.3811 - val_loss: 0.8578 - val_sparse_categorical_accuracy: 0.7020 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1180 - sparse_categorical_accuracy: 0.4151 - val_loss: 0.8566 - val_sparse_categorical_accuracy: 0.6880 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0126 - sparse_categorical_accuracy: 0.4397 - val_loss: 0.8242 - val_sparse_categorical_accuracy: 0.7080 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8359 - sparse_categorical_accuracy: 0.4491 - val_loss: 0.8016 - val_sparse_categorical_accuracy: 0.7270 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7816 - sparse_categorical_accuracy: 0.4573 - val_loss: 0.7883 - val_sparse_categorical_accuracy: 0.7390 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6703 - sparse_categorical_accuracy: 0.4795 - val_loss: 0.7541 - val_sparse_categorical_accuracy: 0.7650 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6216 - sparse_categorical_accuracy: 0.4895 - val_loss: 0.7583 - val_sparse_categorical_accuracy: 0.7470 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5344 - sparse_categorical_accuracy: 0.4941 - val_loss: 0.7417 - val_sparse_categorical_accuracy: 0.7590 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5282 - sparse_categorical_accuracy: 0.5047 - val_loss: 0.7285 - val_sparse_categorical_accuracy: 0.7570 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4268 - sparse_categorical_accuracy: 0.5234 - val_loss: 0.7343 - val_sparse_categorical_accuracy: 0.7580 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3853 - sparse_categorical_accuracy: 0.5451 - val_loss: 0.7027 - val_sparse_categorical_accuracy: 0.7800 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2840 - sparse_categorical_accuracy: 0.5691 - val_loss: 0.7058 - val_sparse_categorical_accuracy: 0.7680 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2454 - sparse_categorical_accuracy: 0.5603 - val_loss: 0.6960 - val_sparse_categorical_accuracy: 0.7820 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2532 - sparse_categorical_accuracy: 0.5703 - val_loss: 0.6864 - val_sparse_categorical_accuracy: 0.7880 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2018 - sparse_categorical_accuracy: 0.5931 - val_loss: 0.6718 - val_sparse_categorical_accuracy: 0.7890 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.1790 - sparse_categorical_accuracy: 0.5872 - val_loss: 0.6668 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1615 - sparse_categorical_accuracy: 0.5960 - val_loss: 0.6576 - val_sparse_categorical_accuracy: 0.7890 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1059 - sparse_categorical_accuracy: 0.6130 - val_loss: 0.6574 - val_sparse_categorical_accuracy: 0.7830 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0628 - sparse_categorical_accuracy: 0.6376 - val_loss: 0.6569 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0299 - sparse_categorical_accuracy: 0.6317 - val_loss: 0.6439 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9963 - sparse_categorical_accuracy: 0.6434 - val_loss: 0.6452 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0606 - sparse_categorical_accuracy: 0.6241 - val_loss: 0.6373 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9796 - sparse_categorical_accuracy: 0.6411 - val_loss: 0.6271 - val_sparse_categorical_accuracy: 0.8010 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9301 - sparse_categorical_accuracy: 0.6780 - val_loss: 0.6251 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9112 - sparse_categorical_accuracy: 0.6809 - val_loss: 0.6246 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9409 - sparse_categorical_accuracy: 0.6610 - val_loss: 0.6181 - val_sparse_categorical_accuracy: 0.8010 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9009 - sparse_categorical_accuracy: 0.6721 - val_loss: 0.6172 - val_sparse_categorical_accuracy: 0.8010 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8762 - sparse_categorical_accuracy: 0.6844 - val_loss: 0.6108 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8502 - sparse_categorical_accuracy: 0.6868 - val_loss: 0.6126 - val_sparse_categorical_accuracy: 0.7950 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8376 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.6034 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.8442 - sparse_categorical_accuracy: 0.6909 - val_loss: 0.6002 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8021 - sparse_categorical_accuracy: 0.7102 - val_loss: 0.5996 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7871 - sparse_categorical_accuracy: 0.7184 - val_loss: 0.6009 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7999 - sparse_categorical_accuracy: 0.6944 - val_loss: 0.5910 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7620 - sparse_categorical_accuracy: 0.7043 - val_loss: 0.5850 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7722 - sparse_categorical_accuracy: 0.7207 - val_loss: 0.5846 - val_sparse_categorical_accuracy: 0.8040 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7903 - sparse_categorical_accuracy: 0.7049 - val_loss: 0.5821 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7755 - sparse_categorical_accuracy: 0.7166 - val_loss: 0.5764 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7373 - sparse_categorical_accuracy: 0.7248 - val_loss: 0.5764 - val_sparse_categorical_accuracy: 0.8040 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7355 - sparse_categorical_accuracy: 0.7254 - val_loss: 0.5757 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7420 - sparse_categorical_accuracy: 0.7184 - val_loss: 0.5678 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7408 - sparse_categorical_accuracy: 0.7283 - val_loss: 0.5648 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6844 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.5653 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7202 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.5635 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6933 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.5577 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6913 - sparse_categorical_accuracy: 0.7488 - val_loss: 0.5566 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6872 - sparse_categorical_accuracy: 0.7494 - val_loss: 0.5557 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6611 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.5514 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6632 - sparse_categorical_accuracy: 0.7611 - val_loss: 0.5505 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6572 - sparse_categorical_accuracy: 0.7459 - val_loss: 0.5505 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6616 - sparse_categorical_accuracy: 0.7600 - val_loss: 0.5477 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6292 - sparse_categorical_accuracy: 0.7594 - val_loss: 0.5422 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6451 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.5405 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6539 - sparse_categorical_accuracy: 0.7541 - val_loss: 0.5398 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6365 - sparse_categorical_accuracy: 0.7576 - val_loss: 0.5350 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6082 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.5308 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6018 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.5300 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6057 - sparse_categorical_accuracy: 0.7769 - val_loss: 0.5316 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6028 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.5307 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6095 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.5266 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5953 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.5260 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5974 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.5244 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6131 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.5198 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6063 - sparse_categorical_accuracy: 0.7769 - val_loss: 0.5208 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5836 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.5158 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5677 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.5169 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5786 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.5122 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5774 - sparse_categorical_accuracy: 0.7881 - val_loss: 0.5119 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5751 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.5093 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5938 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.5084 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5790 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.5127 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5738 - sparse_categorical_accuracy: 0.7892 - val_loss: 0.5015 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5721 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.5001 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5814 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.4995 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5776 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4991 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5831 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.4969 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5810 - sparse_categorical_accuracy: 0.7769 - val_loss: 0.4968 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5664 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.4975 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5719 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.4910 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5667 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4927 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 6, test #1, 2nd iteration: loss of 0.49678218364715576; sparse_categorical_accuracy of 83.20000171661377%\n",
            "appending basis + selective feature vector data\n",
            "[1572, 1496, 2152, 860, 1669, 2597, 1474, 1177, 1939, 698, 1543, 704, 90, 2006, 798, 1671, 1821, 521, 1329, 1268, 1290, 219, 1596, 2143, 1271, 576, 1232, 209, 503, 2693, 2535, 2164, 1832, 1096, 1612, 1425, 2149, 2055, 1365, 2371, 703, 636, 431, 1116, 665, 2514, 307, 366, 1767, 2471, 1569, 2647, 2260, 80, 1182, 1609, 1840, 2590, 2349, 837, 1033, 500, 794, 418, 285, 1559, 1998, 982, 1170, 1592, 1930, 1051, 232, 2355, 1335, 1087, 992, 811, 2641, 1042, 756, 292, 2689, 1192, 544, 302, 174, 763, 2491, 2469, 2425, 1444, 777, 1476, 658, 1999, 1280, 2224, 1732, 1052, 2208, 1751, 242, 1884, 170, 732, 1808, 323, 1949, 912, 1301, 1911, 1856, 143, 1881, 1660, 1637, 1979, 2385, 802, 1187, 481, 2598, 2438, 2457, 2499, 274, 1161, 1498, 1216, 2624, 565, 2110, 502, 2156, 549, 15, 640, 271, 328, 2068, 448, 983, 938, 2070, 293, 214, 33, 1397, 357, 1865, 1736, 1419, 2604, 2027, 17, 735, 142, 1966, 985, 27, 842, 480, 1446, 2517, 2279, 2596, 1362, 1619, 2148, 479, 2667, 1073, 254, 714, 713, 429, 1490, 674, 404, 1873, 2183, 1851, 1105, 1403, 1152, 839, 616, 1670, 192, 1229, 545, 2634, 314, 1848, 2008, 988, 967, 1098, 1734, 2346, 2402, 1327, 2571, 2052, 2488, 1247, 2089, 1508, 1043, 2684, 2036, 1738, 1205, 1275, 2081, 1311, 2128, 1960, 2572, 2266, 2368, 670, 1101, 2160, 2630, 2441, 1639, 2417, 278, 1399, 2087, 1635, 1647, 439, 1754, 677, 541, 1198, 902, 1664, 2289, 101, 1128, 2194, 761, 1632, 266, 1159, 88, 996, 169, 1519, 1279, 408, 30, 2334, 2682, 1039, 1017, 157, 1270, 2118, 1689, 1891, 1836, 124, 1542, 1570, 1293, 1197, 1129, 2436, 85, 880, 1014, 1226, 858, 2235, 627, 2256, 1480, 1595, 21, 1457, 1643, 57, 945, 348, 1994, 2054, 452, 2228, 2010, 1409, 1108, 994, 2381, 2200, 1787, 486, 2225, 725, 2307, 1818, 2298, 845, 470, 1951, 532, 2094, 1277, 79, 462, 288, 2157, 1812, 122, 155, 821, 918, 2009, 2403, 2050, 371, 806, 2347, 186, 1828, 2592, 2479, 2001, 2310, 805, 1426, 1799, 623, 1667, 1259, 1202, 1679, 135, 966, 1340, 991, 2405, 2541, 1278, 396, 830, 1363, 644, 1675, 571, 126, 867, 1118, 2141, 1102, 188, 318, 92, 1729, 2691, 738, 317, 682, 2522, 185, 1374, 1876, 647, 1764, 1605, 1459, 315, 1852, 524, 675, 1036, 2024, 1915, 2649, 1601, 238, 877, 303, 1516, 582, 507, 2704, 1218, 1139, 350, 2187, 1378, 2046, 1031, 1195, 1010, 229, 299, 2646, 216, 2664, 378, 979, 1907, 1726, 1587, 2570, 1148, 2348, 60, 971, 577, 1254, 768, 1208, 2179, 1656, 1291, 663, 540, 1286, 1773, 2327, 140, 1354, 2139, 787, 2005, 993, 1273, 1322, 1784, 1971, 1686, 1267, 1392, 2452, 1412, 1946, 715, 252, 1962, 1739, 2626, 995, 2186, 568, 338, 2323, 1173, 2003, 1523, 1234, 436, 1649, 2614, 886, 2584, 2169, 260, 45, 1814, 2142, 934, 91, 2397, 1728, 2352, 536, 1598, 1744, 1372, 1415, 2062, 801, 2171, 2018, 2677, 1375, 873, 676, 2673, 2529, 2607, 597, 2201, 594, 861, 639, 1708, 1800, 40, 316, 74, 1228, 1175, 2563, 528, 87, 1520, 896, 2632, 201, 667, 1931, 1618, 1627, 2165, 13, 1769, 1505, 276, 728, 82, 1302, 1843, 1231, 750, 1925, 2351, 1795, 1104, 558, 287, 2213, 767, 2386, 342, 34, 2032, 496, 8, 836, 946, 2650, 1548, 1961, 2099, 2532, 956, 2302, 2576, 748, 904, 841, 2017, 1866, 160, 1318, 771, 1501, 1240, 2063, 1807, 865, 2069, 258, 1191, 572, 2413, 1644, 1097, 2463, 997, 1062, 1390, 1136, 2540, 369, 823, 97, 876, 1309, 1634, 816, 1972, 330, 601, 2185, 891, 741, 2464, 2182, 931, 1299, 1796, 614, 202, 289, 1165, 1420, 526, 2406, 1803, 401, 921, 83, 1347, 716, 1077, 516, 1089, 298, 2161, 2184, 441, 776, 2395, 364, 438, 1448, 697, 38, 2257, 2007, 1688, 887, 1274, 963, 2702, 2451, 1730, 2387, 256, 1657, 2021, 227, 2480, 2170, 628, 395, 1566, 1172, 2443, 1323, 2080, 2507, 2663, 632, 980, 2252, 998, 1983, 440, 61, 1537, 1407, 2560, 2574, 659, 1339, 618, 2544, 604, 1356, 1315, 1287, 421, 960, 543, 1078, 2315, 1810, 329, 692, 1759, 2470, 1860, 1120, 607, 2687, 1046, 2105, 2555, 433, 1203, 1009, 1645, 368, 2683, 605, 892, 560, 1037, 1883, 2608, 694, 1741, 2291, 458, 2267, 592, 2272, 1965, 325, 2431, 351, 2393, 96, 2483, 1833, 1975, 1027, 2015, 1651, 598, 2106, 972, 2137, 2206, 356, 1714, 1477, 68, 2273, 1350, 94, 478, 881, 1005, 2412, 505, 1512, 1348, 2193, 1238, 784, 1292, 2549, 1909, 1035, 2581, 1668, 1261, 1066, 1389, 1071, 1772, 1831, 2414, 2116, 976, 863, 2306, 1849, 1482, 2303, 655, 2498, 2475, 2255, 662, 1662, 279, 1421, 2019, 1088, 2230, 150, 1468, 824, 1976, 2559, 187, 53, 326, 1393, 740, 2, 1410, 1504, 1981, 26, 2561, 2506, 1690, 2472, 2460, 2296, 781, 2078, 875, 2433, 1781, 2311, 567, 1887, 1400, 2415, 1382, 253, 1550, 1053, 1258, 2410, 301, 346, 1536, 1386, 701, 2223, 1007, 1305, 1406, 1859, 1720, 1151, 78, 901, 2558, 1028, 365, 306, 611, 924, 2445, 2034, 2363, 525, 2239, 1434, 2287, 377, 1132, 1798, 2195, 1019, 2537, 730, 133, 1484, 1980, 2284, 1483, 2424, 1654, 2031, 792, 1223, 2542, 1924, 900, 2033, 173, 2685, 826, 178, 1969, 184, 2619, 1922, 1903, 2519, 1346, 780, 1682, 1879, 2378, 1110, 2222, 1867, 917, 2209, 2670, 151, 574, 2073, 1371, 2269, 1541, 1557, 1551, 2281, 1134, 98, 1636, 2242, 4, 2221, 2398, 1659, 327, 831, 257, 1802, 1294, 1604, 1207, 58, 2625, 1509, 2138, 49, 432, 2694, 634, 1584, 559, 1528, 1253, 406, 2053, 1607, 102, 2338, 2331, 974, 986, 519, 1414, 2697, 791, 103, 2335, 1780, 990, 1761, 360, 2678, 819, 118, 1464, 1215, 944, 1912, 220, 2029, 2042, 466, 1692, 1982, 1850, 1941, 1481, 1150, 1076, 2123, 109, 1676, 2057, 2668, 1906, 390, 148, 412, 1284, 1190, 168, 2588, 213, 810, 1788, 483, 2390, 1206, 1631, 1168, 32, 1157, 240, 720, 683, 277, 2199, 1308, 1082, 1819, 2356, 2051, 759, 1146, 2067, 2210, 866, 2680, 1691, 363, 447, 2461, 221, 426, 1967, 2136, 1221, 2652, 1326, 2350, 2231, 2226, 1135, 2304, 1262, 1593, 1411, 894, 1661]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 9.0291 - sparse_categorical_accuracy: 0.1809 - val_loss: 2.5669 - val_sparse_categorical_accuracy: 0.2710 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.5948 - sparse_categorical_accuracy: 0.1856 - val_loss: 2.7603 - val_sparse_categorical_accuracy: 0.3190 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.3221 - sparse_categorical_accuracy: 0.1710 - val_loss: 2.2673 - val_sparse_categorical_accuracy: 0.3360 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.5035 - sparse_categorical_accuracy: 0.1950 - val_loss: 2.1265 - val_sparse_categorical_accuracy: 0.3510 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.3974 - sparse_categorical_accuracy: 0.1979 - val_loss: 2.0180 - val_sparse_categorical_accuracy: 0.3640 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.5960 - sparse_categorical_accuracy: 0.2237 - val_loss: 1.8084 - val_sparse_categorical_accuracy: 0.3770 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.5857 - sparse_categorical_accuracy: 0.2125 - val_loss: 1.7922 - val_sparse_categorical_accuracy: 0.3920 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.9395 - sparse_categorical_accuracy: 0.2225 - val_loss: 1.6106 - val_sparse_categorical_accuracy: 0.4120 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.6537 - sparse_categorical_accuracy: 0.2436 - val_loss: 1.5689 - val_sparse_categorical_accuracy: 0.4140 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.3899 - sparse_categorical_accuracy: 0.2453 - val_loss: 1.4013 - val_sparse_categorical_accuracy: 0.5100 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.1260 - sparse_categorical_accuracy: 0.2576 - val_loss: 1.3651 - val_sparse_categorical_accuracy: 0.4950 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.8600 - sparse_categorical_accuracy: 0.2623 - val_loss: 1.3135 - val_sparse_categorical_accuracy: 0.5140 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5811 - sparse_categorical_accuracy: 0.2910 - val_loss: 1.2904 - val_sparse_categorical_accuracy: 0.4930 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.2541 - sparse_categorical_accuracy: 0.3050 - val_loss: 1.2025 - val_sparse_categorical_accuracy: 0.5670 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 3.1842 - sparse_categorical_accuracy: 0.2922 - val_loss: 1.1533 - val_sparse_categorical_accuracy: 0.5860 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.0097 - sparse_categorical_accuracy: 0.3261 - val_loss: 1.0913 - val_sparse_categorical_accuracy: 0.6110 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.7438 - sparse_categorical_accuracy: 0.3331 - val_loss: 1.1071 - val_sparse_categorical_accuracy: 0.5910 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5596 - sparse_categorical_accuracy: 0.3495 - val_loss: 1.0516 - val_sparse_categorical_accuracy: 0.6060 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5320 - sparse_categorical_accuracy: 0.3583 - val_loss: 0.9966 - val_sparse_categorical_accuracy: 0.6530 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2560 - sparse_categorical_accuracy: 0.3882 - val_loss: 0.9780 - val_sparse_categorical_accuracy: 0.6490 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1871 - sparse_categorical_accuracy: 0.3893 - val_loss: 0.9511 - val_sparse_categorical_accuracy: 0.6640 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0616 - sparse_categorical_accuracy: 0.4192 - val_loss: 0.9168 - val_sparse_categorical_accuracy: 0.6960 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9906 - sparse_categorical_accuracy: 0.4251 - val_loss: 0.8990 - val_sparse_categorical_accuracy: 0.6960 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9058 - sparse_categorical_accuracy: 0.4344 - val_loss: 0.8789 - val_sparse_categorical_accuracy: 0.7030 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7678 - sparse_categorical_accuracy: 0.4532 - val_loss: 0.8560 - val_sparse_categorical_accuracy: 0.7200 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6666 - sparse_categorical_accuracy: 0.4678 - val_loss: 0.8541 - val_sparse_categorical_accuracy: 0.7020 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.6769 - sparse_categorical_accuracy: 0.4520 - val_loss: 0.8235 - val_sparse_categorical_accuracy: 0.7340 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6133 - sparse_categorical_accuracy: 0.4684 - val_loss: 0.8156 - val_sparse_categorical_accuracy: 0.7360 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5148 - sparse_categorical_accuracy: 0.4988 - val_loss: 0.7979 - val_sparse_categorical_accuracy: 0.7370 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4743 - sparse_categorical_accuracy: 0.5070 - val_loss: 0.7907 - val_sparse_categorical_accuracy: 0.7430 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4283 - sparse_categorical_accuracy: 0.5111 - val_loss: 0.7758 - val_sparse_categorical_accuracy: 0.7560 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3557 - sparse_categorical_accuracy: 0.5392 - val_loss: 0.7644 - val_sparse_categorical_accuracy: 0.7540 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3229 - sparse_categorical_accuracy: 0.5392 - val_loss: 0.7593 - val_sparse_categorical_accuracy: 0.7600 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2442 - sparse_categorical_accuracy: 0.5626 - val_loss: 0.7514 - val_sparse_categorical_accuracy: 0.7640 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2079 - sparse_categorical_accuracy: 0.5831 - val_loss: 0.7414 - val_sparse_categorical_accuracy: 0.7630 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1984 - sparse_categorical_accuracy: 0.5738 - val_loss: 0.7204 - val_sparse_categorical_accuracy: 0.7740 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1932 - sparse_categorical_accuracy: 0.5843 - val_loss: 0.7111 - val_sparse_categorical_accuracy: 0.7810 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1464 - sparse_categorical_accuracy: 0.5954 - val_loss: 0.7020 - val_sparse_categorical_accuracy: 0.7920 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0888 - sparse_categorical_accuracy: 0.6054 - val_loss: 0.7023 - val_sparse_categorical_accuracy: 0.7920 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0887 - sparse_categorical_accuracy: 0.6095 - val_loss: 0.6872 - val_sparse_categorical_accuracy: 0.7890 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0233 - sparse_categorical_accuracy: 0.6270 - val_loss: 0.6780 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0165 - sparse_categorical_accuracy: 0.6218 - val_loss: 0.6724 - val_sparse_categorical_accuracy: 0.7950 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0040 - sparse_categorical_accuracy: 0.6183 - val_loss: 0.6740 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9401 - sparse_categorical_accuracy: 0.6493 - val_loss: 0.6547 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9548 - sparse_categorical_accuracy: 0.6452 - val_loss: 0.6567 - val_sparse_categorical_accuracy: 0.8040 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9038 - sparse_categorical_accuracy: 0.6528 - val_loss: 0.6456 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9197 - sparse_categorical_accuracy: 0.6552 - val_loss: 0.6433 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8955 - sparse_categorical_accuracy: 0.6692 - val_loss: 0.6371 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8901 - sparse_categorical_accuracy: 0.6680 - val_loss: 0.6356 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8500 - sparse_categorical_accuracy: 0.6879 - val_loss: 0.6239 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8219 - sparse_categorical_accuracy: 0.6985 - val_loss: 0.6156 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8155 - sparse_categorical_accuracy: 0.6991 - val_loss: 0.6187 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8262 - sparse_categorical_accuracy: 0.6844 - val_loss: 0.6118 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7852 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.6079 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7849 - sparse_categorical_accuracy: 0.7061 - val_loss: 0.6022 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7698 - sparse_categorical_accuracy: 0.7266 - val_loss: 0.5976 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7729 - sparse_categorical_accuracy: 0.7172 - val_loss: 0.5930 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7521 - sparse_categorical_accuracy: 0.7237 - val_loss: 0.5862 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7724 - sparse_categorical_accuracy: 0.7084 - val_loss: 0.5801 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7387 - sparse_categorical_accuracy: 0.7184 - val_loss: 0.5796 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7340 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5718 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7176 - sparse_categorical_accuracy: 0.7354 - val_loss: 0.5755 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7078 - sparse_categorical_accuracy: 0.7441 - val_loss: 0.5671 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6824 - sparse_categorical_accuracy: 0.7477 - val_loss: 0.5635 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7068 - sparse_categorical_accuracy: 0.7336 - val_loss: 0.5606 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6911 - sparse_categorical_accuracy: 0.7365 - val_loss: 0.5576 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6746 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.5560 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6920 - sparse_categorical_accuracy: 0.7406 - val_loss: 0.5539 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6614 - sparse_categorical_accuracy: 0.7635 - val_loss: 0.5526 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6754 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.5480 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6707 - sparse_categorical_accuracy: 0.7529 - val_loss: 0.5449 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6536 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5389 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6477 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.5340 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6496 - sparse_categorical_accuracy: 0.7570 - val_loss: 0.5323 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6336 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.5267 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6183 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.5238 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6070 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.5194 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6324 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.5223 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6095 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.5159 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6388 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5140 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6142 - sparse_categorical_accuracy: 0.7728 - val_loss: 0.5082 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5815 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.5077 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6156 - sparse_categorical_accuracy: 0.7693 - val_loss: 0.5079 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6081 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.5100 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6073 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.5044 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5873 - sparse_categorical_accuracy: 0.7728 - val_loss: 0.4986 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6077 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.5018 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5819 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.4936 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5856 - sparse_categorical_accuracy: 0.7822 - val_loss: 0.4928 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5675 - sparse_categorical_accuracy: 0.7951 - val_loss: 0.4910 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5816 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.4902 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5456 - sparse_categorical_accuracy: 0.8027 - val_loss: 0.4856 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5763 - sparse_categorical_accuracy: 0.7787 - val_loss: 0.4853 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5877 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4871 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5670 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4857 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5615 - sparse_categorical_accuracy: 0.7881 - val_loss: 0.4821 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5698 - sparse_categorical_accuracy: 0.7851 - val_loss: 0.4808 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5386 - sparse_categorical_accuracy: 0.8009 - val_loss: 0.4800 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5647 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.4782 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5451 - sparse_categorical_accuracy: 0.7939 - val_loss: 0.4759 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 6, test #2, 2nd iteration: loss of 0.4781656265258789; sparse_categorical_accuracy of 84.10000205039978%\n",
            "appending basis + selective feature vector data\n",
            "[1572, 1496, 2152, 860, 1669, 2597, 1474, 1177, 1939, 698, 1543, 704, 90, 2006, 798, 1671, 1821, 521, 1329, 1268, 1290, 219, 1596, 2143, 1271, 576, 1232, 209, 503, 2693, 2535, 2164, 1832, 1096, 1612, 1425, 2149, 2055, 1365, 2371, 703, 636, 431, 1116, 665, 2514, 307, 366, 1767, 2471, 1569, 2647, 2260, 80, 1182, 1609, 1840, 2590, 2349, 837, 1033, 500, 794, 418, 285, 1559, 1998, 982, 1170, 1592, 1930, 1051, 232, 2355, 1335, 1087, 992, 811, 2641, 1042, 756, 292, 2689, 1192, 544, 302, 174, 763, 2491, 2469, 2425, 1444, 777, 1476, 658, 1999, 1280, 2224, 1732, 1052, 2208, 1751, 242, 1884, 170, 732, 1808, 323, 1949, 912, 1301, 1911, 1856, 143, 1881, 1660, 1637, 1979, 2385, 802, 1187, 481, 2598, 2438, 2457, 2499, 274, 1161, 1498, 1216, 2624, 565, 2110, 502, 2156, 549, 15, 640, 271, 328, 2068, 448, 983, 938, 2070, 293, 214, 33, 1397, 357, 1865, 1736, 1419, 2604, 2027, 17, 735, 142, 1966, 985, 27, 842, 480, 1446, 2517, 2279, 2596, 1362, 1619, 2148, 479, 2667, 1073, 254, 714, 713, 429, 1490, 674, 404, 1873, 2183, 1851, 1105, 1403, 1152, 839, 616, 1670, 192, 1229, 545, 2634, 314, 1848, 2008, 988, 967, 1098, 1734, 2346, 2402, 1327, 2571, 2052, 2488, 1247, 2089, 1508, 1043, 2684, 2036, 1738, 1205, 1275, 2081, 1311, 2128, 1960, 2572, 2266, 2368, 670, 1101, 2160, 2630, 2441, 1639, 2417, 278, 1399, 2087, 1635, 1647, 439, 1754, 677, 541, 1198, 902, 1664, 2289, 101, 1128, 2194, 761, 1632, 266, 1159, 88, 996, 169, 1519, 1279, 408, 30, 2334, 2682, 1039, 1017, 157, 1270, 2118, 1689, 1891, 1836, 124, 1542, 1570, 1293, 1197, 1129, 2436, 85, 880, 1014, 1226, 858, 2235, 627, 2256, 1480, 1595, 21, 1457, 1643, 57, 945, 348, 1994, 2054, 452, 2228, 2010, 1409, 1108, 994, 2381, 2200, 1787, 486, 2225, 725, 2307, 1818, 2298, 845, 470, 1951, 532, 2094, 1277, 79, 462, 288, 2157, 1812, 122, 155, 821, 918, 2009, 2403, 2050, 371, 806, 2347, 186, 1828, 2592, 2479, 2001, 2310, 805, 1426, 1799, 623, 1667, 1259, 1202, 1679, 135, 966, 1340, 991, 2405, 2541, 1278, 396, 830, 1363, 644, 1675, 571, 126, 867, 1118, 2141, 1102, 188, 318, 92, 1729, 2691, 738, 317, 682, 2522, 185, 1374, 1876, 647, 1764, 1605, 1459, 315, 1852, 524, 675, 1036, 2024, 1915, 2649, 1601, 238, 877, 303, 1516, 582, 507, 2704, 1218, 1139, 350, 2187, 1378, 2046, 1031, 1195, 1010, 229, 299, 2646, 216, 2664, 378, 979, 1907, 1726, 1587, 2570, 1148, 2348, 60, 971, 577, 1254, 768, 1208, 2179, 1656, 1291, 663, 540, 1286, 1773, 2327, 140, 1354, 2139, 787, 2005, 993, 1273, 1322, 1784, 1971, 1686, 1267, 1392, 2452, 1412, 1946, 715, 252, 1962, 1739, 2626, 995, 2186, 568, 338, 2323, 1173, 2003, 1523, 1234, 436, 1649, 2614, 886, 2584, 2169, 260, 45, 1814, 2142, 934, 91, 2397, 1728, 2352, 536, 1598, 1744, 1372, 1415, 2062, 801, 2171, 2018, 2677, 1375, 873, 676, 2673, 2529, 2607, 597, 2201, 594, 861, 639, 1708, 1800, 40, 316, 74, 1228, 1175, 2563, 528, 87, 1520, 896, 2632, 201, 667, 1931, 1618, 1627, 2165, 13, 1769, 1505, 276, 728, 82, 1302, 1843, 1231, 750, 1925, 2351, 1795, 1104, 558, 287, 2213, 767, 2386, 342, 34, 2032, 496, 8, 836, 946, 2650, 1548, 1961, 2099, 2532, 956, 2302, 2576, 748, 904, 841, 2017, 1866, 160, 1318, 771, 1501, 1240, 2063, 1807, 865, 2069, 258, 1191, 572, 2413, 1644, 1097, 2463, 997, 1062, 1390, 1136, 2540, 369, 823, 97, 876, 1309, 1634, 816, 1972, 330, 601, 2185, 891, 741, 2464, 2182, 931, 1299, 1796, 614, 202, 289, 1165, 1420, 526, 2406, 1803, 401, 921, 83, 1347, 716, 1077, 516, 1089, 298, 2161, 2184, 441, 776, 2395, 364, 438, 1448, 697, 38, 2257, 2007, 1688, 887, 1274, 963, 2702, 2451, 1730, 2387, 256, 1657, 2021, 227, 2480, 2170, 628, 395, 1566, 1172, 2443, 1323, 2080, 2507, 2663, 632, 980, 2252, 998, 1983, 440, 61, 1537, 1407, 2560, 2574, 659, 1339, 618, 2544, 604, 1356, 1315, 1287, 421, 960, 543, 1078, 2315, 1810, 329, 692, 1759, 2470, 1860, 1120, 607, 2687, 1046, 2105, 2555, 433, 1203, 1009, 1645, 368, 2683, 605, 892, 560, 1037, 1883, 2608, 694, 1741, 2291, 458, 2267, 592, 2272, 1965, 325, 2431, 351, 2393, 96, 2483, 1833, 1975, 1027, 2015, 1651, 598, 2106, 972, 2137, 2206, 356, 1714, 1477, 68, 2273, 1350, 94, 478, 881, 1005, 2412, 505, 1512, 1348, 2193, 1238, 784, 1292, 2549, 1909, 1035, 2581, 1668, 1261, 1066, 1389, 1071, 1772, 1831, 2414, 2116, 976, 863, 2306, 1849, 1482, 2303, 655, 2498, 2475, 2255, 662, 1662, 279, 1421, 2019, 1088, 2230, 150, 1468, 824, 1976, 2559, 187, 53, 326, 1393, 740, 2, 1410, 1504, 1981, 26, 2561, 2506, 1690, 2472, 2460, 2296, 781, 2078, 875, 2433, 1781, 2311, 567, 1887, 1400, 2415, 1382, 253, 1550, 1053, 1258, 2410, 301, 346, 1536, 1386, 701, 2223, 1007, 1305, 1406, 1859, 1720, 1151, 78, 901, 2558, 1028, 365, 306, 611, 924, 2445, 2034, 2363, 525, 2239, 1434, 2287, 377, 1132, 1798, 2195, 1019, 2537, 730, 133, 1484, 1980, 2284, 1483, 2424, 1654, 2031, 792, 1223, 2542, 1924, 900, 2033, 173, 2685, 826, 178, 1969, 184, 2619, 1922, 1903, 2519, 1346, 780, 1682, 1879, 2378, 1110, 2222, 1867, 917, 2209, 2670, 151, 574, 2073, 1371, 2269, 1541, 1557, 1551, 2281, 1134, 98, 1636, 2242, 4, 2221, 2398, 1659, 327, 831, 257, 1802, 1294, 1604, 1207, 58, 2625, 1509, 2138, 49, 432, 2694, 634, 1584, 559, 1528, 1253, 406, 2053, 1607, 102, 2338, 2331, 974, 986, 519, 1414, 2697, 791, 103, 2335, 1780, 990, 1761, 360, 2678, 819, 118, 1464, 1215, 944, 1912, 220, 2029, 2042, 466, 1692, 1982, 1850, 1941, 1481, 1150, 1076, 2123, 109, 1676, 2057, 2668, 1906, 390, 148, 412, 1284, 1190, 168, 2588, 213, 810, 1788, 483, 2390, 1206, 1631, 1168, 32, 1157, 240, 720, 683, 277, 2199, 1308, 1082, 1819, 2356, 2051, 759, 1146, 2067, 2210, 866, 2680, 1691, 363, 447, 2461, 221, 426, 1967, 2136, 1221, 2652, 1326, 2350, 2231, 2226, 1135, 2304, 1262, 1593, 1411, 894, 1661]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 8.1792 - sparse_categorical_accuracy: 0.1593 - val_loss: 3.2578 - val_sparse_categorical_accuracy: 0.2560 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.3116 - sparse_categorical_accuracy: 0.1657 - val_loss: 2.6119 - val_sparse_categorical_accuracy: 0.2560 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.7983 - sparse_categorical_accuracy: 0.1657 - val_loss: 2.5210 - val_sparse_categorical_accuracy: 0.2810 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.2961 - sparse_categorical_accuracy: 0.1809 - val_loss: 2.1540 - val_sparse_categorical_accuracy: 0.2720 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.7749 - sparse_categorical_accuracy: 0.1985 - val_loss: 2.0368 - val_sparse_categorical_accuracy: 0.3090 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.5376 - sparse_categorical_accuracy: 0.1909 - val_loss: 1.9776 - val_sparse_categorical_accuracy: 0.3220 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.9866 - sparse_categorical_accuracy: 0.1991 - val_loss: 1.7680 - val_sparse_categorical_accuracy: 0.3830 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.5955 - sparse_categorical_accuracy: 0.2067 - val_loss: 1.6781 - val_sparse_categorical_accuracy: 0.3790 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.1152 - sparse_categorical_accuracy: 0.2348 - val_loss: 1.6494 - val_sparse_categorical_accuracy: 0.3810 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.8250 - sparse_categorical_accuracy: 0.2506 - val_loss: 1.4303 - val_sparse_categorical_accuracy: 0.4700 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.7421 - sparse_categorical_accuracy: 0.2547 - val_loss: 1.4799 - val_sparse_categorical_accuracy: 0.4380 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.4916 - sparse_categorical_accuracy: 0.2752 - val_loss: 1.2995 - val_sparse_categorical_accuracy: 0.5030 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1883 - sparse_categorical_accuracy: 0.2869 - val_loss: 1.2064 - val_sparse_categorical_accuracy: 0.5510 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.0320 - sparse_categorical_accuracy: 0.3050 - val_loss: 1.1897 - val_sparse_categorical_accuracy: 0.5750 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.7503 - sparse_categorical_accuracy: 0.3279 - val_loss: 1.1355 - val_sparse_categorical_accuracy: 0.5740 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.6643 - sparse_categorical_accuracy: 0.3197 - val_loss: 1.0542 - val_sparse_categorical_accuracy: 0.6410 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4418 - sparse_categorical_accuracy: 0.3437 - val_loss: 1.0267 - val_sparse_categorical_accuracy: 0.6510 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2024 - sparse_categorical_accuracy: 0.3841 - val_loss: 1.0028 - val_sparse_categorical_accuracy: 0.6630 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1768 - sparse_categorical_accuracy: 0.3741 - val_loss: 1.0060 - val_sparse_categorical_accuracy: 0.6330 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0152 - sparse_categorical_accuracy: 0.3782 - val_loss: 0.9519 - val_sparse_categorical_accuracy: 0.6780 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9906 - sparse_categorical_accuracy: 0.4063 - val_loss: 0.9160 - val_sparse_categorical_accuracy: 0.6800 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8036 - sparse_categorical_accuracy: 0.4391 - val_loss: 0.8910 - val_sparse_categorical_accuracy: 0.7060 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.7567 - sparse_categorical_accuracy: 0.4374 - val_loss: 0.8681 - val_sparse_categorical_accuracy: 0.7100 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6828 - sparse_categorical_accuracy: 0.4467 - val_loss: 0.8435 - val_sparse_categorical_accuracy: 0.7360 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5997 - sparse_categorical_accuracy: 0.4660 - val_loss: 0.8256 - val_sparse_categorical_accuracy: 0.7340 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5203 - sparse_categorical_accuracy: 0.4778 - val_loss: 0.8061 - val_sparse_categorical_accuracy: 0.7410 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4692 - sparse_categorical_accuracy: 0.5094 - val_loss: 0.7981 - val_sparse_categorical_accuracy: 0.7460 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4209 - sparse_categorical_accuracy: 0.5100 - val_loss: 0.7843 - val_sparse_categorical_accuracy: 0.7420 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3358 - sparse_categorical_accuracy: 0.5410 - val_loss: 0.7735 - val_sparse_categorical_accuracy: 0.7530 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3187 - sparse_categorical_accuracy: 0.5381 - val_loss: 0.7643 - val_sparse_categorical_accuracy: 0.7520 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2522 - sparse_categorical_accuracy: 0.5527 - val_loss: 0.7439 - val_sparse_categorical_accuracy: 0.7650 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2184 - sparse_categorical_accuracy: 0.5779 - val_loss: 0.7315 - val_sparse_categorical_accuracy: 0.7670 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1732 - sparse_categorical_accuracy: 0.5773 - val_loss: 0.7250 - val_sparse_categorical_accuracy: 0.7690 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.1646 - sparse_categorical_accuracy: 0.5790 - val_loss: 0.7167 - val_sparse_categorical_accuracy: 0.7750 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1107 - sparse_categorical_accuracy: 0.5937 - val_loss: 0.6982 - val_sparse_categorical_accuracy: 0.7810 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0600 - sparse_categorical_accuracy: 0.6206 - val_loss: 0.6965 - val_sparse_categorical_accuracy: 0.7780 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0325 - sparse_categorical_accuracy: 0.6165 - val_loss: 0.6825 - val_sparse_categorical_accuracy: 0.7820 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9884 - sparse_categorical_accuracy: 0.6306 - val_loss: 0.6742 - val_sparse_categorical_accuracy: 0.7850 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9898 - sparse_categorical_accuracy: 0.6323 - val_loss: 0.6717 - val_sparse_categorical_accuracy: 0.7840 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9639 - sparse_categorical_accuracy: 0.6516 - val_loss: 0.6639 - val_sparse_categorical_accuracy: 0.7860 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9371 - sparse_categorical_accuracy: 0.6540 - val_loss: 0.6540 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9256 - sparse_categorical_accuracy: 0.6546 - val_loss: 0.6544 - val_sparse_categorical_accuracy: 0.7840 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8451 - sparse_categorical_accuracy: 0.6874 - val_loss: 0.6422 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8854 - sparse_categorical_accuracy: 0.6844 - val_loss: 0.6344 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8802 - sparse_categorical_accuracy: 0.6552 - val_loss: 0.6282 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8204 - sparse_categorical_accuracy: 0.7002 - val_loss: 0.6235 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8104 - sparse_categorical_accuracy: 0.7008 - val_loss: 0.6173 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8134 - sparse_categorical_accuracy: 0.7037 - val_loss: 0.6163 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7837 - sparse_categorical_accuracy: 0.7119 - val_loss: 0.6113 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7660 - sparse_categorical_accuracy: 0.7237 - val_loss: 0.5994 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.7882 - sparse_categorical_accuracy: 0.6985 - val_loss: 0.5946 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7623 - sparse_categorical_accuracy: 0.7219 - val_loss: 0.5921 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7475 - sparse_categorical_accuracy: 0.7272 - val_loss: 0.5917 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7466 - sparse_categorical_accuracy: 0.7336 - val_loss: 0.5810 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7422 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5842 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7142 - sparse_categorical_accuracy: 0.7307 - val_loss: 0.5738 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7099 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5701 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6858 - sparse_categorical_accuracy: 0.7500 - val_loss: 0.5636 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6989 - sparse_categorical_accuracy: 0.7488 - val_loss: 0.5591 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6810 - sparse_categorical_accuracy: 0.7512 - val_loss: 0.5560 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6668 - sparse_categorical_accuracy: 0.7611 - val_loss: 0.5531 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6718 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.5537 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6447 - sparse_categorical_accuracy: 0.7635 - val_loss: 0.5453 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6483 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5418 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6372 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.5364 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6480 - sparse_categorical_accuracy: 0.7594 - val_loss: 0.5347 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6270 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.5340 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6353 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.5270 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6393 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.5243 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5999 - sparse_categorical_accuracy: 0.7892 - val_loss: 0.5228 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6138 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.5263 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6038 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.5160 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6059 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.5172 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5837 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.5114 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6048 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.5118 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6056 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.5072 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5846 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.5062 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5865 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.5036 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5711 - sparse_categorical_accuracy: 0.7910 - val_loss: 0.4987 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6036 - sparse_categorical_accuracy: 0.7758 - val_loss: 0.4996 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5776 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.4939 - val_sparse_categorical_accuracy: 0.8450 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5776 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.4972 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5761 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.4913 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5823 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.4908 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5721 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.4945 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.5471 - sparse_categorical_accuracy: 0.7951 - val_loss: 0.4833 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5569 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.4831 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5504 - sparse_categorical_accuracy: 0.7980 - val_loss: 0.4807 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5469 - sparse_categorical_accuracy: 0.7851 - val_loss: 0.4804 - val_sparse_categorical_accuracy: 0.8440 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5372 - sparse_categorical_accuracy: 0.8015 - val_loss: 0.4785 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5430 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4751 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5387 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.4763 - val_sparse_categorical_accuracy: 0.8460 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5353 - sparse_categorical_accuracy: 0.7986 - val_loss: 0.4751 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5391 - sparse_categorical_accuracy: 0.7951 - val_loss: 0.4779 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5459 - sparse_categorical_accuracy: 0.8015 - val_loss: 0.4747 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5381 - sparse_categorical_accuracy: 0.7974 - val_loss: 0.4687 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5416 - sparse_categorical_accuracy: 0.7939 - val_loss: 0.4658 - val_sparse_categorical_accuracy: 0.8440 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5370 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.4650 - val_sparse_categorical_accuracy: 0.8470 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5341 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.4656 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5290 - sparse_categorical_accuracy: 0.8033 - val_loss: 0.4672 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 6, test #3, 2nd iteration: loss of 0.46497344970703125; sparse_categorical_accuracy of 84.7000002861023%\n",
            "appending basis + selective feature vector data\n",
            "[1572, 1496, 2152, 860, 1669, 2597, 1474, 1177, 1939, 698, 1543, 704, 90, 2006, 798, 1671, 1821, 521, 1329, 1268, 1290, 219, 1596, 2143, 1271, 576, 1232, 209, 503, 2693, 2535, 2164, 1832, 1096, 1612, 1425, 2149, 2055, 1365, 2371, 703, 636, 431, 1116, 665, 2514, 307, 366, 1767, 2471, 1569, 2647, 2260, 80, 1182, 1609, 1840, 2590, 2349, 837, 1033, 500, 794, 418, 285, 1559, 1998, 982, 1170, 1592, 1930, 1051, 232, 2355, 1335, 1087, 992, 811, 2641, 1042, 756, 292, 2689, 1192, 544, 302, 174, 763, 2491, 2469, 2425, 1444, 777, 1476, 658, 1999, 1280, 2224, 1732, 1052, 2208, 1751, 242, 1884, 170, 732, 1808, 323, 1949, 912, 1301, 1911, 1856, 143, 1881, 1660, 1637, 1979, 2385, 802, 1187, 481, 2598, 2438, 2457, 2499, 274, 1161, 1498, 1216, 2624, 565, 2110, 502, 2156, 549, 15, 640, 271, 328, 2068, 448, 983, 938, 2070, 293, 214, 33, 1397, 357, 1865, 1736, 1419, 2604, 2027, 17, 735, 142, 1966, 985, 27, 842, 480, 1446, 2517, 2279, 2596, 1362, 1619, 2148, 479, 2667, 1073, 254, 714, 713, 429, 1490, 674, 404, 1873, 2183, 1851, 1105, 1403, 1152, 839, 616, 1670, 192, 1229, 545, 2634, 314, 1848, 2008, 988, 967, 1098, 1734, 2346, 2402, 1327, 2571, 2052, 2488, 1247, 2089, 1508, 1043, 2684, 2036, 1738, 1205, 1275, 2081, 1311, 2128, 1960, 2572, 2266, 2368, 670, 1101, 2160, 2630, 2441, 1639, 2417, 278, 1399, 2087, 1635, 1647, 439, 1754, 677, 541, 1198, 902, 1664, 2289, 101, 1128, 2194, 761, 1632, 266, 1159, 88, 996, 169, 1519, 1279, 408, 30, 2334, 2682, 1039, 1017, 157, 1270, 2118, 1689, 1891, 1836, 124, 1542, 1570, 1293, 1197, 1129, 2436, 85, 880, 1014, 1226, 858, 2235, 627, 2256, 1480, 1595, 21, 1457, 1643, 57, 945, 348, 1994, 2054, 452, 2228, 2010, 1409, 1108, 994, 2381, 2200, 1787, 486, 2225, 725, 2307, 1818, 2298, 845, 470, 1951, 532, 2094, 1277, 79, 462, 288, 2157, 1812, 122, 155, 821, 918, 2009, 2403, 2050, 371, 806, 2347, 186, 1828, 2592, 2479, 2001, 2310, 805, 1426, 1799, 623, 1667, 1259, 1202, 1679, 135, 966, 1340, 991, 2405, 2541, 1278, 396, 830, 1363, 644, 1675, 571, 126, 867, 1118, 2141, 1102, 188, 318, 92, 1729, 2691, 738, 317, 682, 2522, 185, 1374, 1876, 647, 1764, 1605, 1459, 315, 1852, 524, 675, 1036, 2024, 1915, 2649, 1601, 238, 877, 303, 1516, 582, 507, 2704, 1218, 1139, 350, 2187, 1378, 2046, 1031, 1195, 1010, 229, 299, 2646, 216, 2664, 378, 979, 1907, 1726, 1587, 2570, 1148, 2348, 60, 971, 577, 1254, 768, 1208, 2179, 1656, 1291, 663, 540, 1286, 1773, 2327, 140, 1354, 2139, 787, 2005, 993, 1273, 1322, 1784, 1971, 1686, 1267, 1392, 2452, 1412, 1946, 715, 252, 1962, 1739, 2626, 995, 2186, 568, 338, 2323, 1173, 2003, 1523, 1234, 436, 1649, 2614, 886, 2584, 2169, 260, 45, 1814, 2142, 934, 91, 2397, 1728, 2352, 536, 1598, 1744, 1372, 1415, 2062, 801, 2171, 2018, 2677, 1375, 873, 676, 2673, 2529, 2607, 597, 2201, 594, 861, 639, 1708, 1800, 40, 316, 74, 1228, 1175, 2563, 528, 87, 1520, 896, 2632, 201, 667, 1931, 1618, 1627, 2165, 13, 1769, 1505, 276, 728, 82, 1302, 1843, 1231, 750, 1925, 2351, 1795, 1104, 558, 287, 2213, 767, 2386, 342, 34, 2032, 496, 8, 836, 946, 2650, 1548, 1961, 2099, 2532, 956, 2302, 2576, 748, 904, 841, 2017, 1866, 160, 1318, 771, 1501, 1240, 2063, 1807, 865, 2069, 258, 1191, 572, 2413, 1644, 1097, 2463, 997, 1062, 1390, 1136, 2540, 369, 823, 97, 876, 1309, 1634, 816, 1972, 330, 601, 2185, 891, 741, 2464, 2182, 931, 1299, 1796, 614, 202, 289, 1165, 1420, 526, 2406, 1803, 401, 921, 83, 1347, 716, 1077, 516, 1089, 298, 2161, 2184, 441, 776, 2395, 364, 438, 1448, 697, 38, 2257, 2007, 1688, 887, 1274, 963, 2702, 2451, 1730, 2387, 256, 1657, 2021, 227, 2480, 2170, 628, 395, 1566, 1172, 2443, 1323, 2080, 2507, 2663, 632, 980, 2252, 998, 1983, 440, 61, 1537, 1407, 2560, 2574, 659, 1339, 618, 2544, 604, 1356, 1315, 1287, 421, 960, 543, 1078, 2315, 1810, 329, 692, 1759, 2470, 1860, 1120, 607, 2687, 1046, 2105, 2555, 433, 1203, 1009, 1645, 368, 2683, 605, 892, 560, 1037, 1883, 2608, 694, 1741, 2291, 458, 2267, 592, 2272, 1965, 325, 2431, 351, 2393, 96, 2483, 1833, 1975, 1027, 2015, 1651, 598, 2106, 972, 2137, 2206, 356, 1714, 1477, 68, 2273, 1350, 94, 478, 881, 1005, 2412, 505, 1512, 1348, 2193, 1238, 784, 1292, 2549, 1909, 1035, 2581, 1668, 1261, 1066, 1389, 1071, 1772, 1831, 2414, 2116, 976, 863, 2306, 1849, 1482, 2303, 655, 2498, 2475, 2255, 662, 1662, 279, 1421, 2019, 1088, 2230, 150, 1468, 824, 1976, 2559, 187, 53, 326, 1393, 740, 2, 1410, 1504, 1981, 26, 2561, 2506, 1690, 2472, 2460, 2296, 781, 2078, 875, 2433, 1781, 2311, 567, 1887, 1400, 2415, 1382, 253, 1550, 1053, 1258, 2410, 301, 346, 1536, 1386, 701, 2223, 1007, 1305, 1406, 1859, 1720, 1151, 78, 901, 2558, 1028, 365, 306, 611, 924, 2445, 2034, 2363, 525, 2239, 1434, 2287, 377, 1132, 1798, 2195, 1019, 2537, 730, 133, 1484, 1980, 2284, 1483, 2424, 1654, 2031, 792, 1223, 2542, 1924, 900, 2033, 173, 2685, 826, 178, 1969, 184, 2619, 1922, 1903, 2519, 1346, 780, 1682, 1879, 2378, 1110, 2222, 1867, 917, 2209, 2670, 151, 574, 2073, 1371, 2269, 1541, 1557, 1551, 2281, 1134, 98, 1636, 2242, 4, 2221, 2398, 1659, 327, 831, 257, 1802, 1294, 1604, 1207, 58, 2625, 1509, 2138, 49, 432, 2694, 634, 1584, 559, 1528, 1253, 406, 2053, 1607, 102, 2338, 2331, 974, 986, 519, 1414, 2697, 791, 103, 2335, 1780, 990, 1761, 360, 2678, 819, 118, 1464, 1215, 944, 1912, 220, 2029, 2042, 466, 1692, 1982, 1850, 1941, 1481, 1150, 1076, 2123, 109, 1676, 2057, 2668, 1906, 390, 148, 412, 1284, 1190, 168, 2588, 213, 810, 1788, 483, 2390, 1206, 1631, 1168, 32, 1157, 240, 720, 683, 277, 2199, 1308, 1082, 1819, 2356, 2051, 759, 1146, 2067, 2210, 866, 2680, 1691, 363, 447, 2461, 221, 426, 1967, 2136, 1221, 2652, 1326, 2350, 2231, 2226, 1135, 2304, 1262, 1593, 1411, 894, 1661]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 2s 5ms/step - loss: 7.1359 - sparse_categorical_accuracy: 0.1674 - val_loss: 2.8423 - val_sparse_categorical_accuracy: 0.3110 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.3015 - sparse_categorical_accuracy: 0.1768 - val_loss: 2.4669 - val_sparse_categorical_accuracy: 0.3080 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.8226 - sparse_categorical_accuracy: 0.1856 - val_loss: 2.2309 - val_sparse_categorical_accuracy: 0.2900 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 5.5972 - sparse_categorical_accuracy: 0.1950 - val_loss: 2.1514 - val_sparse_categorical_accuracy: 0.3280 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.8648 - sparse_categorical_accuracy: 0.2026 - val_loss: 1.9692 - val_sparse_categorical_accuracy: 0.3650 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.5385 - sparse_categorical_accuracy: 0.2102 - val_loss: 1.9695 - val_sparse_categorical_accuracy: 0.3790 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.1900 - sparse_categorical_accuracy: 0.2143 - val_loss: 1.7331 - val_sparse_categorical_accuracy: 0.3850 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.9766 - sparse_categorical_accuracy: 0.2125 - val_loss: 1.5684 - val_sparse_categorical_accuracy: 0.4050 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5507 - sparse_categorical_accuracy: 0.2547 - val_loss: 1.5401 - val_sparse_categorical_accuracy: 0.4280 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.3372 - sparse_categorical_accuracy: 0.2740 - val_loss: 1.4439 - val_sparse_categorical_accuracy: 0.4730 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.0536 - sparse_categorical_accuracy: 0.2963 - val_loss: 1.3455 - val_sparse_categorical_accuracy: 0.5070 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8991 - sparse_categorical_accuracy: 0.2974 - val_loss: 1.2929 - val_sparse_categorical_accuracy: 0.5310 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 2.6846 - sparse_categorical_accuracy: 0.3056 - val_loss: 1.2244 - val_sparse_categorical_accuracy: 0.5500 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5201 - sparse_categorical_accuracy: 0.3208 - val_loss: 1.2174 - val_sparse_categorical_accuracy: 0.5420 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 2.3187 - sparse_categorical_accuracy: 0.3542 - val_loss: 1.1810 - val_sparse_categorical_accuracy: 0.5670 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2206 - sparse_categorical_accuracy: 0.3665 - val_loss: 1.0886 - val_sparse_categorical_accuracy: 0.6180 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0799 - sparse_categorical_accuracy: 0.3911 - val_loss: 1.0541 - val_sparse_categorical_accuracy: 0.6270 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8919 - sparse_categorical_accuracy: 0.4174 - val_loss: 1.0246 - val_sparse_categorical_accuracy: 0.6370 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8564 - sparse_categorical_accuracy: 0.3987 - val_loss: 0.9865 - val_sparse_categorical_accuracy: 0.6540 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.7818 - sparse_categorical_accuracy: 0.4157 - val_loss: 0.9708 - val_sparse_categorical_accuracy: 0.6660 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6691 - sparse_categorical_accuracy: 0.4584 - val_loss: 0.9406 - val_sparse_categorical_accuracy: 0.6740 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6228 - sparse_categorical_accuracy: 0.4660 - val_loss: 0.9201 - val_sparse_categorical_accuracy: 0.6790 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5345 - sparse_categorical_accuracy: 0.4865 - val_loss: 0.8965 - val_sparse_categorical_accuracy: 0.6870 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5004 - sparse_categorical_accuracy: 0.4766 - val_loss: 0.8544 - val_sparse_categorical_accuracy: 0.7260 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3851 - sparse_categorical_accuracy: 0.5146 - val_loss: 0.8474 - val_sparse_categorical_accuracy: 0.7300 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3754 - sparse_categorical_accuracy: 0.5269 - val_loss: 0.8262 - val_sparse_categorical_accuracy: 0.7350 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3246 - sparse_categorical_accuracy: 0.5340 - val_loss: 0.8109 - val_sparse_categorical_accuracy: 0.7380 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2280 - sparse_categorical_accuracy: 0.5504 - val_loss: 0.8042 - val_sparse_categorical_accuracy: 0.7340 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2592 - sparse_categorical_accuracy: 0.5322 - val_loss: 0.7855 - val_sparse_categorical_accuracy: 0.7350 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1995 - sparse_categorical_accuracy: 0.5691 - val_loss: 0.7658 - val_sparse_categorical_accuracy: 0.7530 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1061 - sparse_categorical_accuracy: 0.6159 - val_loss: 0.7545 - val_sparse_categorical_accuracy: 0.7470 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1076 - sparse_categorical_accuracy: 0.6007 - val_loss: 0.7444 - val_sparse_categorical_accuracy: 0.7630 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0898 - sparse_categorical_accuracy: 0.5925 - val_loss: 0.7296 - val_sparse_categorical_accuracy: 0.7700 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0335 - sparse_categorical_accuracy: 0.6165 - val_loss: 0.7195 - val_sparse_categorical_accuracy: 0.7800 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.0209 - sparse_categorical_accuracy: 0.6306 - val_loss: 0.7106 - val_sparse_categorical_accuracy: 0.7850 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9861 - sparse_categorical_accuracy: 0.6475 - val_loss: 0.6983 - val_sparse_categorical_accuracy: 0.7930 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9676 - sparse_categorical_accuracy: 0.6417 - val_loss: 0.6948 - val_sparse_categorical_accuracy: 0.7850 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.9374 - sparse_categorical_accuracy: 0.6569 - val_loss: 0.6817 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9074 - sparse_categorical_accuracy: 0.6762 - val_loss: 0.6771 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8794 - sparse_categorical_accuracy: 0.6862 - val_loss: 0.6661 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8821 - sparse_categorical_accuracy: 0.6727 - val_loss: 0.6606 - val_sparse_categorical_accuracy: 0.8010 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8649 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.6517 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8648 - sparse_categorical_accuracy: 0.6874 - val_loss: 0.6490 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8385 - sparse_categorical_accuracy: 0.6967 - val_loss: 0.6401 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8119 - sparse_categorical_accuracy: 0.7084 - val_loss: 0.6309 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8198 - sparse_categorical_accuracy: 0.7002 - val_loss: 0.6241 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8036 - sparse_categorical_accuracy: 0.6938 - val_loss: 0.6183 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7620 - sparse_categorical_accuracy: 0.7260 - val_loss: 0.6119 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7585 - sparse_categorical_accuracy: 0.7172 - val_loss: 0.6062 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7593 - sparse_categorical_accuracy: 0.7196 - val_loss: 0.6038 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7297 - sparse_categorical_accuracy: 0.7389 - val_loss: 0.5970 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.7173 - sparse_categorical_accuracy: 0.7377 - val_loss: 0.5900 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7387 - sparse_categorical_accuracy: 0.7301 - val_loss: 0.5916 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7322 - sparse_categorical_accuracy: 0.7260 - val_loss: 0.5841 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7007 - sparse_categorical_accuracy: 0.7477 - val_loss: 0.5786 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7040 - sparse_categorical_accuracy: 0.7389 - val_loss: 0.5731 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6985 - sparse_categorical_accuracy: 0.7365 - val_loss: 0.5706 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6836 - sparse_categorical_accuracy: 0.7547 - val_loss: 0.5667 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6726 - sparse_categorical_accuracy: 0.7541 - val_loss: 0.5598 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6650 - sparse_categorical_accuracy: 0.7611 - val_loss: 0.5615 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6735 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.5535 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6463 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.5504 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6565 - sparse_categorical_accuracy: 0.7564 - val_loss: 0.5477 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6424 - sparse_categorical_accuracy: 0.7588 - val_loss: 0.5425 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6421 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.5391 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6199 - sparse_categorical_accuracy: 0.7635 - val_loss: 0.5336 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6237 - sparse_categorical_accuracy: 0.7693 - val_loss: 0.5366 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6263 - sparse_categorical_accuracy: 0.7600 - val_loss: 0.5305 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6064 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.5250 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6173 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.5259 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6019 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.5220 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6044 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.5216 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6001 - sparse_categorical_accuracy: 0.7787 - val_loss: 0.5140 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5963 - sparse_categorical_accuracy: 0.7787 - val_loss: 0.5164 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5862 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.5139 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5948 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.5077 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5943 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.5095 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5851 - sparse_categorical_accuracy: 0.7822 - val_loss: 0.5015 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5833 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.5026 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5861 - sparse_categorical_accuracy: 0.7804 - val_loss: 0.4972 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5827 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.4959 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5726 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4916 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5677 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4892 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5571 - sparse_categorical_accuracy: 0.7922 - val_loss: 0.4884 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5592 - sparse_categorical_accuracy: 0.7881 - val_loss: 0.4916 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5423 - sparse_categorical_accuracy: 0.7957 - val_loss: 0.4854 - val_sparse_categorical_accuracy: 0.8440 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5579 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4859 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5636 - sparse_categorical_accuracy: 0.7916 - val_loss: 0.4819 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5584 - sparse_categorical_accuracy: 0.7916 - val_loss: 0.4796 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5397 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.4781 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5549 - sparse_categorical_accuracy: 0.7851 - val_loss: 0.4787 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5844 - sparse_categorical_accuracy: 0.7822 - val_loss: 0.4793 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5364 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.4771 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5544 - sparse_categorical_accuracy: 0.7992 - val_loss: 0.4755 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5280 - sparse_categorical_accuracy: 0.8097 - val_loss: 0.4709 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5274 - sparse_categorical_accuracy: 0.7974 - val_loss: 0.4669 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5477 - sparse_categorical_accuracy: 0.8015 - val_loss: 0.4686 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5409 - sparse_categorical_accuracy: 0.7963 - val_loss: 0.4628 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5435 - sparse_categorical_accuracy: 0.7910 - val_loss: 0.4617 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5240 - sparse_categorical_accuracy: 0.7998 - val_loss: 0.4616 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 6, test #4, 2nd iteration: loss of 0.48536375164985657; sparse_categorical_accuracy of 84.3999981880188%\n",
            "appending basis + selective feature vector data\n",
            "[1352, 1784, 1831, 659, 1127, 358, 648, 715, 2341, 965, 886, 584, 512, 518, 1577, 78, 1270, 2623, 660, 830, 440, 732, 1347, 2642, 1960, 2017, 2305, 1859, 1845, 2521, 852, 189, 2217, 656, 1707, 384, 2075, 528, 1585, 2256, 1391, 1749, 1432, 577, 1888, 2707, 1718, 2459, 99, 1358, 1404, 1504, 2250, 1973, 1147, 2371, 969, 306, 2172, 866, 297, 312, 1062, 1679, 2290, 2063, 385, 799, 1416, 1914, 2660, 1414, 2209, 826, 1238, 787, 1761, 414, 1314, 1256, 65, 1, 2193, 1324, 419, 738, 1491, 1087, 1195, 1216, 1363, 1925, 1615, 553, 2315, 2274, 2310, 2407, 300, 646, 1431, 1008, 466, 80, 727, 2477, 1365, 824, 1167, 0, 378, 1961, 505, 1640, 1792, 2339, 1326, 1231, 1834, 2262, 1780, 1959, 2045, 1455, 2220, 2653, 631, 898, 87, 1145, 956, 188, 2563, 2259, 27, 2033, 1651, 1320, 8, 507, 663, 843, 966, 856, 2203, 1233, 2004, 2288, 344, 1217, 2602, 919, 2353, 1159, 359, 2355, 1747, 2489, 1220, 1659, 1853, 1670, 662, 970, 2266, 1796, 1265, 132, 2159, 1222, 862, 1378, 850, 1578, 563, 1303, 1906, 1113, 2512, 1486, 2679, 2141, 360, 1336, 552, 2456, 535, 637, 691, 2218, 1178, 1102, 1809, 654, 2344, 1984, 1800, 404, 128, 2600, 1111, 481, 447, 1444, 2502, 923, 110, 1144, 271, 1164, 564, 2325, 1876, 2140, 1778, 1632, 704, 1209, 2419, 311, 983, 1807, 2372, 53, 615, 1642, 1474, 937, 2428, 1586, 1169, 415, 1283, 1436, 967, 1720, 1771, 1017, 5, 2579, 1538, 2480, 1032, 122, 233, 899, 2149, 2050, 2119, 2029, 1604, 335, 138, 598, 884, 209, 1500, 1426, 974, 1529, 370, 1348, 1692, 159, 730, 90, 1489, 2402, 2071, 2655, 2550, 986, 291, 1524, 797, 1228, 2632, 1084, 231, 1867, 1452, 58, 1610, 2520, 1133, 750, 1899, 2417, 599, 2647, 2018, 1983, 435, 581, 1884, 1668, 2275, 2082, 1760, 2072, 2246, 1457, 1739, 706, 2143, 1612, 612, 2066, 685, 278, 2375, 2155, 2354, 234, 1094, 2446, 262, 678, 1993, 945, 2431, 427, 1061, 1107, 299, 361, 227, 1873, 1653, 2139, 1465, 43, 889, 2576, 1862, 1191, 1518, 453, 2030, 1700, 2326, 2444, 2106, 2195, 1218, 486, 708, 2525, 1280, 2363, 1997, 1654, 100, 867, 1036, 499, 2443, 2049, 2158, 677, 321, 2239, 418, 2191, 1816, 1208, 1362, 1907, 711, 313, 2323, 2346, 245, 1662, 1450, 1468, 1935, 1354, 1293, 714, 2131, 1756, 1157, 1344, 1069, 1331, 1706, 493, 1944, 1267, 725, 1965, 1574, 2546, 1185, 562, 115, 1912, 165, 1394, 172, 753, 375, 1618, 1559, 1345, 1386, 865, 2557, 1029, 1963, 1932, 1379, 2387, 774, 1949, 836, 692, 1798, 2214, 13, 1072, 104, 2294, 1681, 1835, 1449, 2595, 2279, 1795, 2662, 1028, 1989, 495, 1016, 601, 2376, 149, 1081, 2360, 2533, 1261, 492, 1545, 225, 1368, 524, 2581, 2493, 2694, 94, 1136, 1494, 124, 2586, 647, 2044, 1571, 1746, 2621, 2374, 653, 1292, 1506, 1879, 2547, 2406, 540, 275, 1641, 2613, 1729, 838, 1701, 1397, 1889, 921, 261, 241, 2528, 2212, 1523, 1026, 1875, 203, 2554, 2364, 560, 1981, 2687, 1264, 2047, 2384, 2534, 2270, 400, 2630, 2122, 59, 1332, 160, 963, 2377, 634, 982, 1407, 2095, 2695, 1828, 932, 696, 253, 2465, 1403, 1053, 1006, 519, 1339, 537, 718, 534, 1333, 1398, 1012, 170, 1895, 421, 1953, 1092, 1541, 1806, 851, 2380, 1893, 314, 1629, 546, 1300, 1762, 1916, 1433, 2236, 1410, 2460, 772, 1674, 1357, 2627, 664, 2603, 593, 2500, 2691, 1512, 559, 322, 572, 667, 556, 2152, 1179, 2412, 1166, 1740, 2343, 1389, 336, 433, 1279, 2399, 845, 891, 2005, 2508, 1725, 2350, 2468, 213, 1025, 2175, 494, 1823, 1657, 1901, 1498, 2128, 1665, 630, 2479, 632, 1815, 2634, 1015, 905, 2681, 32, 1234, 1882, 2515, 1402, 2319, 1584, 1696, 2574, 705, 1180, 1395, 408, 2186, 439, 244, 2166, 143, 980, 2373, 841, 2161, 1877, 757, 775, 1976, 1969, 1709, 913, 482, 338, 2482, 2100, 279, 501, 1991, 29, 116, 1647, 1075, 2449, 2132, 990, 825, 670, 57, 218, 1067, 1548, 2226, 1572, 1799, 2680, 1878, 987, 2342, 1967, 145, 561, 1509, 1499, 173, 1168, 2242, 1269, 1210, 2179, 325, 2173, 2154, 1669, 610, 820, 2169, 2464, 2002, 2088, 522, 1312, 2471, 1341, 452, 1120, 1703, 713, 902, 319, 2649, 2240, 515, 2276, 2064, 796, 441, 2366, 216, 620, 1323, 627, 1313, 1088, 2296, 236, 837, 1007, 156, 1109, 2257, 1413, 2156, 1447, 256, 1583, 1866, 643, 2255, 1802, 911, 1198, 283, 2062, 352, 1972, 1549, 2040, 2135, 2618, 2556, 1235, 2622, 1479, 2247, 2145, 2187, 767, 1649, 2338, 462, 179, 1998, 168, 2260, 1297, 18, 1278, 2059, 1458, 821, 1384, 460, 1857, 1082, 863, 1417, 2599, 1009, 301, 1052, 940, 1958, 739, 2531, 1110, 147, 1887, 2510, 2656, 330, 1205, 1931, 1539, 182, 1172, 2297, 62, 878, 274, 1020, 289, 1909, 981, 2204, 2177, 766, 2229, 2400, 264, 1631, 1566, 1045, 2497, 2575, 1097, 2028, 1608, 468, 869, 485, 1832, 1684, 2137, 764, 1207, 2170, 376, 1723, 2277, 2388, 2385, 2481, 476, 298, 2441, 1716, 1090, 2633, 371, 469, 859, 2564, 1863, 272, 1050, 1272, 1223, 1712, 1639, 106, 2452, 861, 953, 251, 412, 2345, 742, 2611, 623, 947, 1183, 2605, 1467, 237, 1438, 2268, 1850, 1797, 54, 1776, 1904, 1192, 2052, 548, 1146, 736, 2077, 2495, 1686, 2409, 1135, 2541, 2442, 2254, 510, 2450, 260, 759, 930, 1519, 1315, 417, 1319, 480, 929, 1840, 1593, 2183, 214, 800, 353, 1564, 1918, 1334, 1225, 1869, 395, 1675, 410, 1552, 701, 2559, 1623, 1516, 2612, 2225, 151, 1841, 193, 1260, 1367, 1846, 2118, 1021, 834, 316, 549, 644, 1520, 2111, 2626, 328, 2253, 2461, 1383, 895, 1057, 24, 2107, 1408, 2688, 150, 2463, 720, 2472, 1540, 1013, 1089, 365, 1558, 1469, 1098, 2699, 1605, 320, 2222, 1138, 2038, 1377, 19, 369, 1661, 2232, 1077, 1942, 883, 250, 686, 2151, 1837, 2176, 808, 107, 1401, 392, 2289, 1289, 1360, 1385, 2301, 2058, 2184, 1374, 28, 431, 184, 1031, 1532, 1619, 1338, 167, 1396, 935, 290, 1947, 2379, 2549, 2065, 1448, 2598, 792, 1805, 304, 2027, 1496, 2086, 197, 285, 238, 894, 2530, 748, 1783, 2644, 961, 2083, 2561, 2285, 2631, 1644, 487, 1291, 668, 1655, 2675, 366, 1301]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 10.7201 - sparse_categorical_accuracy: 0.1540 - val_loss: 3.0666 - val_sparse_categorical_accuracy: 0.2160 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 6.6578 - sparse_categorical_accuracy: 0.1920 - val_loss: 2.8183 - val_sparse_categorical_accuracy: 0.3030 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.1319 - sparse_categorical_accuracy: 0.2032 - val_loss: 2.5478 - val_sparse_categorical_accuracy: 0.3100 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.6696 - sparse_categorical_accuracy: 0.1991 - val_loss: 2.3738 - val_sparse_categorical_accuracy: 0.3140 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.4167 - sparse_categorical_accuracy: 0.2032 - val_loss: 2.3904 - val_sparse_categorical_accuracy: 0.3170 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.7976 - sparse_categorical_accuracy: 0.2395 - val_loss: 2.0309 - val_sparse_categorical_accuracy: 0.3530 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.6033 - sparse_categorical_accuracy: 0.2365 - val_loss: 1.9729 - val_sparse_categorical_accuracy: 0.3580 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.1865 - sparse_categorical_accuracy: 0.2635 - val_loss: 1.6982 - val_sparse_categorical_accuracy: 0.4260 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.9581 - sparse_categorical_accuracy: 0.2541 - val_loss: 1.6211 - val_sparse_categorical_accuracy: 0.4240 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.7921 - sparse_categorical_accuracy: 0.2711 - val_loss: 1.4485 - val_sparse_categorical_accuracy: 0.4830 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5327 - sparse_categorical_accuracy: 0.2717 - val_loss: 1.3439 - val_sparse_categorical_accuracy: 0.5010 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.2717 - sparse_categorical_accuracy: 0.2746 - val_loss: 1.3882 - val_sparse_categorical_accuracy: 0.4730 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 3.0826 - sparse_categorical_accuracy: 0.2875 - val_loss: 1.2922 - val_sparse_categorical_accuracy: 0.5230 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8352 - sparse_categorical_accuracy: 0.3039 - val_loss: 1.1816 - val_sparse_categorical_accuracy: 0.5590 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.6559 - sparse_categorical_accuracy: 0.3507 - val_loss: 1.1243 - val_sparse_categorical_accuracy: 0.5820 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4802 - sparse_categorical_accuracy: 0.3402 - val_loss: 1.1503 - val_sparse_categorical_accuracy: 0.5670 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 2.3908 - sparse_categorical_accuracy: 0.3718 - val_loss: 1.0645 - val_sparse_categorical_accuracy: 0.6060 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2703 - sparse_categorical_accuracy: 0.3612 - val_loss: 1.0616 - val_sparse_categorical_accuracy: 0.6000 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2174 - sparse_categorical_accuracy: 0.3911 - val_loss: 1.0151 - val_sparse_categorical_accuracy: 0.6300 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0900 - sparse_categorical_accuracy: 0.3876 - val_loss: 0.9817 - val_sparse_categorical_accuracy: 0.6230 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9772 - sparse_categorical_accuracy: 0.4139 - val_loss: 0.9423 - val_sparse_categorical_accuracy: 0.6640 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8177 - sparse_categorical_accuracy: 0.4391 - val_loss: 0.9271 - val_sparse_categorical_accuracy: 0.6620 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7871 - sparse_categorical_accuracy: 0.4456 - val_loss: 0.9303 - val_sparse_categorical_accuracy: 0.6420 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7583 - sparse_categorical_accuracy: 0.4327 - val_loss: 0.8782 - val_sparse_categorical_accuracy: 0.6890 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5818 - sparse_categorical_accuracy: 0.4748 - val_loss: 0.8717 - val_sparse_categorical_accuracy: 0.6830 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5041 - sparse_categorical_accuracy: 0.4807 - val_loss: 0.8373 - val_sparse_categorical_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5631 - sparse_categorical_accuracy: 0.4725 - val_loss: 0.8179 - val_sparse_categorical_accuracy: 0.7240 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4609 - sparse_categorical_accuracy: 0.5041 - val_loss: 0.8203 - val_sparse_categorical_accuracy: 0.7120 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4228 - sparse_categorical_accuracy: 0.4912 - val_loss: 0.8030 - val_sparse_categorical_accuracy: 0.7220 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3888 - sparse_categorical_accuracy: 0.5164 - val_loss: 0.7708 - val_sparse_categorical_accuracy: 0.7510 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3179 - sparse_categorical_accuracy: 0.5398 - val_loss: 0.7589 - val_sparse_categorical_accuracy: 0.7510 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2211 - sparse_categorical_accuracy: 0.5550 - val_loss: 0.7546 - val_sparse_categorical_accuracy: 0.7430 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2496 - sparse_categorical_accuracy: 0.5515 - val_loss: 0.7312 - val_sparse_categorical_accuracy: 0.7720 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2104 - sparse_categorical_accuracy: 0.5691 - val_loss: 0.7232 - val_sparse_categorical_accuracy: 0.7590 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1337 - sparse_categorical_accuracy: 0.5849 - val_loss: 0.7128 - val_sparse_categorical_accuracy: 0.7770 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1152 - sparse_categorical_accuracy: 0.5867 - val_loss: 0.6990 - val_sparse_categorical_accuracy: 0.7840 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0651 - sparse_categorical_accuracy: 0.5925 - val_loss: 0.6993 - val_sparse_categorical_accuracy: 0.7710 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0264 - sparse_categorical_accuracy: 0.6206 - val_loss: 0.6929 - val_sparse_categorical_accuracy: 0.7740 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0134 - sparse_categorical_accuracy: 0.6136 - val_loss: 0.6820 - val_sparse_categorical_accuracy: 0.7850 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.0139 - sparse_categorical_accuracy: 0.6130 - val_loss: 0.6682 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9566 - sparse_categorical_accuracy: 0.6376 - val_loss: 0.6582 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9270 - sparse_categorical_accuracy: 0.6511 - val_loss: 0.6548 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9610 - sparse_categorical_accuracy: 0.6288 - val_loss: 0.6414 - val_sparse_categorical_accuracy: 0.8010 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8762 - sparse_categorical_accuracy: 0.6786 - val_loss: 0.6389 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9083 - sparse_categorical_accuracy: 0.6499 - val_loss: 0.6284 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8948 - sparse_categorical_accuracy: 0.6692 - val_loss: 0.6235 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8570 - sparse_categorical_accuracy: 0.6862 - val_loss: 0.6146 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8619 - sparse_categorical_accuracy: 0.6598 - val_loss: 0.6107 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8280 - sparse_categorical_accuracy: 0.7108 - val_loss: 0.6065 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8410 - sparse_categorical_accuracy: 0.6827 - val_loss: 0.5984 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.7882 - sparse_categorical_accuracy: 0.7242 - val_loss: 0.5933 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7977 - sparse_categorical_accuracy: 0.7078 - val_loss: 0.5868 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7893 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.5858 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7600 - sparse_categorical_accuracy: 0.7225 - val_loss: 0.5815 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7605 - sparse_categorical_accuracy: 0.7166 - val_loss: 0.5731 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7369 - sparse_categorical_accuracy: 0.7219 - val_loss: 0.5707 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7346 - sparse_categorical_accuracy: 0.7295 - val_loss: 0.5656 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7408 - sparse_categorical_accuracy: 0.7219 - val_loss: 0.5600 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7339 - sparse_categorical_accuracy: 0.7155 - val_loss: 0.5619 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.7086 - sparse_categorical_accuracy: 0.7389 - val_loss: 0.5546 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7125 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5480 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7083 - sparse_categorical_accuracy: 0.7348 - val_loss: 0.5441 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6783 - sparse_categorical_accuracy: 0.7488 - val_loss: 0.5402 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6979 - sparse_categorical_accuracy: 0.7371 - val_loss: 0.5363 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6873 - sparse_categorical_accuracy: 0.7529 - val_loss: 0.5333 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6695 - sparse_categorical_accuracy: 0.7535 - val_loss: 0.5336 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6822 - sparse_categorical_accuracy: 0.7523 - val_loss: 0.5287 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6700 - sparse_categorical_accuracy: 0.7441 - val_loss: 0.5239 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6464 - sparse_categorical_accuracy: 0.7693 - val_loss: 0.5231 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6629 - sparse_categorical_accuracy: 0.7529 - val_loss: 0.5170 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6549 - sparse_categorical_accuracy: 0.7523 - val_loss: 0.5146 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6287 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.5102 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6371 - sparse_categorical_accuracy: 0.7611 - val_loss: 0.5084 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6093 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.5049 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6167 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.5036 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6374 - sparse_categorical_accuracy: 0.7547 - val_loss: 0.5037 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6298 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.4986 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6037 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.4986 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6123 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.4933 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6149 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.4895 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6025 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.4880 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6107 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.4891 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.5957 - sparse_categorical_accuracy: 0.7705 - val_loss: 0.4863 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5958 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4835 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5920 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4799 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5935 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.4790 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5838 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.4776 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5719 - sparse_categorical_accuracy: 0.7881 - val_loss: 0.4754 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5925 - sparse_categorical_accuracy: 0.7822 - val_loss: 0.4782 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5723 - sparse_categorical_accuracy: 0.7804 - val_loss: 0.4694 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5790 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.4720 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5788 - sparse_categorical_accuracy: 0.7916 - val_loss: 0.4693 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5921 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.4689 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5752 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.4663 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5742 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.4680 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5697 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4637 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5601 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4634 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5632 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.4612 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5579 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4564 - val_sparse_categorical_accuracy: 0.8450 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5444 - sparse_categorical_accuracy: 0.7892 - val_loss: 0.4599 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 7, test #0, 2nd iteration: loss of 0.45641210675239563; sparse_categorical_accuracy of 84.50000286102295%\n",
            "appending basis + selective feature vector data\n",
            "[1352, 1784, 1831, 659, 1127, 358, 648, 715, 2341, 965, 886, 584, 512, 518, 1577, 78, 1270, 2623, 660, 830, 440, 732, 1347, 2642, 1960, 2017, 2305, 1859, 1845, 2521, 852, 189, 2217, 656, 1707, 384, 2075, 528, 1585, 2256, 1391, 1749, 1432, 577, 1888, 2707, 1718, 2459, 99, 1358, 1404, 1504, 2250, 1973, 1147, 2371, 969, 306, 2172, 866, 297, 312, 1062, 1679, 2290, 2063, 385, 799, 1416, 1914, 2660, 1414, 2209, 826, 1238, 787, 1761, 414, 1314, 1256, 65, 1, 2193, 1324, 419, 738, 1491, 1087, 1195, 1216, 1363, 1925, 1615, 553, 2315, 2274, 2310, 2407, 300, 646, 1431, 1008, 466, 80, 727, 2477, 1365, 824, 1167, 0, 378, 1961, 505, 1640, 1792, 2339, 1326, 1231, 1834, 2262, 1780, 1959, 2045, 1455, 2220, 2653, 631, 898, 87, 1145, 956, 188, 2563, 2259, 27, 2033, 1651, 1320, 8, 507, 663, 843, 966, 856, 2203, 1233, 2004, 2288, 344, 1217, 2602, 919, 2353, 1159, 359, 2355, 1747, 2489, 1220, 1659, 1853, 1670, 662, 970, 2266, 1796, 1265, 132, 2159, 1222, 862, 1378, 850, 1578, 563, 1303, 1906, 1113, 2512, 1486, 2679, 2141, 360, 1336, 552, 2456, 535, 637, 691, 2218, 1178, 1102, 1809, 654, 2344, 1984, 1800, 404, 128, 2600, 1111, 481, 447, 1444, 2502, 923, 110, 1144, 271, 1164, 564, 2325, 1876, 2140, 1778, 1632, 704, 1209, 2419, 311, 983, 1807, 2372, 53, 615, 1642, 1474, 937, 2428, 1586, 1169, 415, 1283, 1436, 967, 1720, 1771, 1017, 5, 2579, 1538, 2480, 1032, 122, 233, 899, 2149, 2050, 2119, 2029, 1604, 335, 138, 598, 884, 209, 1500, 1426, 974, 1529, 370, 1348, 1692, 159, 730, 90, 1489, 2402, 2071, 2655, 2550, 986, 291, 1524, 797, 1228, 2632, 1084, 231, 1867, 1452, 58, 1610, 2520, 1133, 750, 1899, 2417, 599, 2647, 2018, 1983, 435, 581, 1884, 1668, 2275, 2082, 1760, 2072, 2246, 1457, 1739, 706, 2143, 1612, 612, 2066, 685, 278, 2375, 2155, 2354, 234, 1094, 2446, 262, 678, 1993, 945, 2431, 427, 1061, 1107, 299, 361, 227, 1873, 1653, 2139, 1465, 43, 889, 2576, 1862, 1191, 1518, 453, 2030, 1700, 2326, 2444, 2106, 2195, 1218, 486, 708, 2525, 1280, 2363, 1997, 1654, 100, 867, 1036, 499, 2443, 2049, 2158, 677, 321, 2239, 418, 2191, 1816, 1208, 1362, 1907, 711, 313, 2323, 2346, 245, 1662, 1450, 1468, 1935, 1354, 1293, 714, 2131, 1756, 1157, 1344, 1069, 1331, 1706, 493, 1944, 1267, 725, 1965, 1574, 2546, 1185, 562, 115, 1912, 165, 1394, 172, 753, 375, 1618, 1559, 1345, 1386, 865, 2557, 1029, 1963, 1932, 1379, 2387, 774, 1949, 836, 692, 1798, 2214, 13, 1072, 104, 2294, 1681, 1835, 1449, 2595, 2279, 1795, 2662, 1028, 1989, 495, 1016, 601, 2376, 149, 1081, 2360, 2533, 1261, 492, 1545, 225, 1368, 524, 2581, 2493, 2694, 94, 1136, 1494, 124, 2586, 647, 2044, 1571, 1746, 2621, 2374, 653, 1292, 1506, 1879, 2547, 2406, 540, 275, 1641, 2613, 1729, 838, 1701, 1397, 1889, 921, 261, 241, 2528, 2212, 1523, 1026, 1875, 203, 2554, 2364, 560, 1981, 2687, 1264, 2047, 2384, 2534, 2270, 400, 2630, 2122, 59, 1332, 160, 963, 2377, 634, 982, 1407, 2095, 2695, 1828, 932, 696, 253, 2465, 1403, 1053, 1006, 519, 1339, 537, 718, 534, 1333, 1398, 1012, 170, 1895, 421, 1953, 1092, 1541, 1806, 851, 2380, 1893, 314, 1629, 546, 1300, 1762, 1916, 1433, 2236, 1410, 2460, 772, 1674, 1357, 2627, 664, 2603, 593, 2500, 2691, 1512, 559, 322, 572, 667, 556, 2152, 1179, 2412, 1166, 1740, 2343, 1389, 336, 433, 1279, 2399, 845, 891, 2005, 2508, 1725, 2350, 2468, 213, 1025, 2175, 494, 1823, 1657, 1901, 1498, 2128, 1665, 630, 2479, 632, 1815, 2634, 1015, 905, 2681, 32, 1234, 1882, 2515, 1402, 2319, 1584, 1696, 2574, 705, 1180, 1395, 408, 2186, 439, 244, 2166, 143, 980, 2373, 841, 2161, 1877, 757, 775, 1976, 1969, 1709, 913, 482, 338, 2482, 2100, 279, 501, 1991, 29, 116, 1647, 1075, 2449, 2132, 990, 825, 670, 57, 218, 1067, 1548, 2226, 1572, 1799, 2680, 1878, 987, 2342, 1967, 145, 561, 1509, 1499, 173, 1168, 2242, 1269, 1210, 2179, 325, 2173, 2154, 1669, 610, 820, 2169, 2464, 2002, 2088, 522, 1312, 2471, 1341, 452, 1120, 1703, 713, 902, 319, 2649, 2240, 515, 2276, 2064, 796, 441, 2366, 216, 620, 1323, 627, 1313, 1088, 2296, 236, 837, 1007, 156, 1109, 2257, 1413, 2156, 1447, 256, 1583, 1866, 643, 2255, 1802, 911, 1198, 283, 2062, 352, 1972, 1549, 2040, 2135, 2618, 2556, 1235, 2622, 1479, 2247, 2145, 2187, 767, 1649, 2338, 462, 179, 1998, 168, 2260, 1297, 18, 1278, 2059, 1458, 821, 1384, 460, 1857, 1082, 863, 1417, 2599, 1009, 301, 1052, 940, 1958, 739, 2531, 1110, 147, 1887, 2510, 2656, 330, 1205, 1931, 1539, 182, 1172, 2297, 62, 878, 274, 1020, 289, 1909, 981, 2204, 2177, 766, 2229, 2400, 264, 1631, 1566, 1045, 2497, 2575, 1097, 2028, 1608, 468, 869, 485, 1832, 1684, 2137, 764, 1207, 2170, 376, 1723, 2277, 2388, 2385, 2481, 476, 298, 2441, 1716, 1090, 2633, 371, 469, 859, 2564, 1863, 272, 1050, 1272, 1223, 1712, 1639, 106, 2452, 861, 953, 251, 412, 2345, 742, 2611, 623, 947, 1183, 2605, 1467, 237, 1438, 2268, 1850, 1797, 54, 1776, 1904, 1192, 2052, 548, 1146, 736, 2077, 2495, 1686, 2409, 1135, 2541, 2442, 2254, 510, 2450, 260, 759, 930, 1519, 1315, 417, 1319, 480, 929, 1840, 1593, 2183, 214, 800, 353, 1564, 1918, 1334, 1225, 1869, 395, 1675, 410, 1552, 701, 2559, 1623, 1516, 2612, 2225, 151, 1841, 193, 1260, 1367, 1846, 2118, 1021, 834, 316, 549, 644, 1520, 2111, 2626, 328, 2253, 2461, 1383, 895, 1057, 24, 2107, 1408, 2688, 150, 2463, 720, 2472, 1540, 1013, 1089, 365, 1558, 1469, 1098, 2699, 1605, 320, 2222, 1138, 2038, 1377, 19, 369, 1661, 2232, 1077, 1942, 883, 250, 686, 2151, 1837, 2176, 808, 107, 1401, 392, 2289, 1289, 1360, 1385, 2301, 2058, 2184, 1374, 28, 431, 184, 1031, 1532, 1619, 1338, 167, 1396, 935, 290, 1947, 2379, 2549, 2065, 1448, 2598, 792, 1805, 304, 2027, 1496, 2086, 197, 285, 238, 894, 2530, 748, 1783, 2644, 961, 2083, 2561, 2285, 2631, 1644, 487, 1291, 668, 1655, 2675, 366, 1301]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 8.1614 - sparse_categorical_accuracy: 0.1956 - val_loss: 2.7041 - val_sparse_categorical_accuracy: 0.2960 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.9708 - sparse_categorical_accuracy: 0.2067 - val_loss: 1.9723 - val_sparse_categorical_accuracy: 0.3020 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.5370 - sparse_categorical_accuracy: 0.2143 - val_loss: 2.0314 - val_sparse_categorical_accuracy: 0.3230 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.9083 - sparse_categorical_accuracy: 0.1985 - val_loss: 1.8878 - val_sparse_categorical_accuracy: 0.3490 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.4609 - sparse_categorical_accuracy: 0.2213 - val_loss: 1.8197 - val_sparse_categorical_accuracy: 0.3410 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.0194 - sparse_categorical_accuracy: 0.2266 - val_loss: 1.7714 - val_sparse_categorical_accuracy: 0.3480 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.7571 - sparse_categorical_accuracy: 0.2266 - val_loss: 1.6973 - val_sparse_categorical_accuracy: 0.3440 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.3700 - sparse_categorical_accuracy: 0.2436 - val_loss: 1.4016 - val_sparse_categorical_accuracy: 0.4240 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.9883 - sparse_categorical_accuracy: 0.2535 - val_loss: 1.2944 - val_sparse_categorical_accuracy: 0.4790 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.8225 - sparse_categorical_accuracy: 0.2605 - val_loss: 1.2938 - val_sparse_categorical_accuracy: 0.4510 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 3.3734 - sparse_categorical_accuracy: 0.2828 - val_loss: 1.1886 - val_sparse_categorical_accuracy: 0.5230 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1311 - sparse_categorical_accuracy: 0.3050 - val_loss: 1.1420 - val_sparse_categorical_accuracy: 0.5500 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.0650 - sparse_categorical_accuracy: 0.3015 - val_loss: 1.1953 - val_sparse_categorical_accuracy: 0.4700 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.7690 - sparse_categorical_accuracy: 0.3396 - val_loss: 1.0939 - val_sparse_categorical_accuracy: 0.5580 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5585 - sparse_categorical_accuracy: 0.3566 - val_loss: 1.0105 - val_sparse_categorical_accuracy: 0.6240 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4458 - sparse_categorical_accuracy: 0.3630 - val_loss: 0.9924 - val_sparse_categorical_accuracy: 0.6350 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 2.3217 - sparse_categorical_accuracy: 0.3817 - val_loss: 0.9593 - val_sparse_categorical_accuracy: 0.6310 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2017 - sparse_categorical_accuracy: 0.3958 - val_loss: 0.9123 - val_sparse_categorical_accuracy: 0.6780 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1045 - sparse_categorical_accuracy: 0.4028 - val_loss: 0.8979 - val_sparse_categorical_accuracy: 0.6760 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9396 - sparse_categorical_accuracy: 0.4151 - val_loss: 0.9286 - val_sparse_categorical_accuracy: 0.6400 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8852 - sparse_categorical_accuracy: 0.4280 - val_loss: 0.8691 - val_sparse_categorical_accuracy: 0.6880 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8056 - sparse_categorical_accuracy: 0.4338 - val_loss: 0.8402 - val_sparse_categorical_accuracy: 0.7100 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6219 - sparse_categorical_accuracy: 0.4766 - val_loss: 0.8220 - val_sparse_categorical_accuracy: 0.7120 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6217 - sparse_categorical_accuracy: 0.4637 - val_loss: 0.7998 - val_sparse_categorical_accuracy: 0.7210 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5935 - sparse_categorical_accuracy: 0.4778 - val_loss: 0.7837 - val_sparse_categorical_accuracy: 0.7400 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.4396 - sparse_categorical_accuracy: 0.5252 - val_loss: 0.7772 - val_sparse_categorical_accuracy: 0.7470 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4721 - sparse_categorical_accuracy: 0.4941 - val_loss: 0.7665 - val_sparse_categorical_accuracy: 0.7360 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3481 - sparse_categorical_accuracy: 0.5416 - val_loss: 0.7717 - val_sparse_categorical_accuracy: 0.7170 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2933 - sparse_categorical_accuracy: 0.5386 - val_loss: 0.7337 - val_sparse_categorical_accuracy: 0.7640 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3251 - sparse_categorical_accuracy: 0.5416 - val_loss: 0.7239 - val_sparse_categorical_accuracy: 0.7570 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2352 - sparse_categorical_accuracy: 0.5621 - val_loss: 0.7088 - val_sparse_categorical_accuracy: 0.7780 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2052 - sparse_categorical_accuracy: 0.5849 - val_loss: 0.6993 - val_sparse_categorical_accuracy: 0.7810 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1468 - sparse_categorical_accuracy: 0.5890 - val_loss: 0.7003 - val_sparse_categorical_accuracy: 0.7690 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0801 - sparse_categorical_accuracy: 0.6148 - val_loss: 0.6846 - val_sparse_categorical_accuracy: 0.7720 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0842 - sparse_categorical_accuracy: 0.6101 - val_loss: 0.6807 - val_sparse_categorical_accuracy: 0.7770 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0414 - sparse_categorical_accuracy: 0.6253 - val_loss: 0.6660 - val_sparse_categorical_accuracy: 0.7930 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0175 - sparse_categorical_accuracy: 0.6276 - val_loss: 0.6609 - val_sparse_categorical_accuracy: 0.7810 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0379 - sparse_categorical_accuracy: 0.6159 - val_loss: 0.6471 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9679 - sparse_categorical_accuracy: 0.6405 - val_loss: 0.6479 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9548 - sparse_categorical_accuracy: 0.6370 - val_loss: 0.6446 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9435 - sparse_categorical_accuracy: 0.6487 - val_loss: 0.6359 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8973 - sparse_categorical_accuracy: 0.6593 - val_loss: 0.6235 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8844 - sparse_categorical_accuracy: 0.6762 - val_loss: 0.6168 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9148 - sparse_categorical_accuracy: 0.6657 - val_loss: 0.6146 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8430 - sparse_categorical_accuracy: 0.6838 - val_loss: 0.6048 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8182 - sparse_categorical_accuracy: 0.7061 - val_loss: 0.6020 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8200 - sparse_categorical_accuracy: 0.6961 - val_loss: 0.5952 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7622 - sparse_categorical_accuracy: 0.7084 - val_loss: 0.5927 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7833 - sparse_categorical_accuracy: 0.6991 - val_loss: 0.5896 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7830 - sparse_categorical_accuracy: 0.7061 - val_loss: 0.5791 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7683 - sparse_categorical_accuracy: 0.7149 - val_loss: 0.5759 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7771 - sparse_categorical_accuracy: 0.7026 - val_loss: 0.5733 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7472 - sparse_categorical_accuracy: 0.7213 - val_loss: 0.5704 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7489 - sparse_categorical_accuracy: 0.7196 - val_loss: 0.5653 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7324 - sparse_categorical_accuracy: 0.7260 - val_loss: 0.5598 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7225 - sparse_categorical_accuracy: 0.7430 - val_loss: 0.5521 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7340 - sparse_categorical_accuracy: 0.7324 - val_loss: 0.5527 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7030 - sparse_categorical_accuracy: 0.7418 - val_loss: 0.5435 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7193 - sparse_categorical_accuracy: 0.7377 - val_loss: 0.5407 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6797 - sparse_categorical_accuracy: 0.7506 - val_loss: 0.5364 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6897 - sparse_categorical_accuracy: 0.7482 - val_loss: 0.5334 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6777 - sparse_categorical_accuracy: 0.7529 - val_loss: 0.5342 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6806 - sparse_categorical_accuracy: 0.7500 - val_loss: 0.5330 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6695 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.5233 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6762 - sparse_categorical_accuracy: 0.7477 - val_loss: 0.5219 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6679 - sparse_categorical_accuracy: 0.7459 - val_loss: 0.5194 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6462 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.5164 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6417 - sparse_categorical_accuracy: 0.7559 - val_loss: 0.5165 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6283 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5084 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6605 - sparse_categorical_accuracy: 0.7535 - val_loss: 0.5058 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6491 - sparse_categorical_accuracy: 0.7588 - val_loss: 0.5043 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6402 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5027 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6265 - sparse_categorical_accuracy: 0.7588 - val_loss: 0.4995 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6266 - sparse_categorical_accuracy: 0.7641 - val_loss: 0.4988 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6173 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.4961 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6191 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.5023 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5997 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.4902 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6126 - sparse_categorical_accuracy: 0.7787 - val_loss: 0.4892 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5863 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.4886 - val_sparse_categorical_accuracy: 0.8450 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6020 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.4809 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5861 - sparse_categorical_accuracy: 0.7851 - val_loss: 0.4870 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5878 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.4809 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5736 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.4842 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            " 84/107 [======================>.......] - ETA: 0s - loss: 0.5678 - sparse_categorical_accuracy: 0.7857\n",
            "Epoch 84: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5833 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.4808 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5919 - sparse_categorical_accuracy: 0.7769 - val_loss: 0.4762 - val_sparse_categorical_accuracy: 0.8410 - lr: 1.5000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5810 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.4739 - val_sparse_categorical_accuracy: 0.8420 - lr: 1.5000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6010 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.4745 - val_sparse_categorical_accuracy: 0.8470 - lr: 1.5000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5696 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4734 - val_sparse_categorical_accuracy: 0.8450 - lr: 1.5000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5803 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.4730 - val_sparse_categorical_accuracy: 0.8450 - lr: 1.5000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5745 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.4720 - val_sparse_categorical_accuracy: 0.8420 - lr: 1.5000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5503 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4722 - val_sparse_categorical_accuracy: 0.8460 - lr: 1.5000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5804 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.4712 - val_sparse_categorical_accuracy: 0.8420 - lr: 1.5000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5798 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.4710 - val_sparse_categorical_accuracy: 0.8450 - lr: 1.5000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5623 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.4703 - val_sparse_categorical_accuracy: 0.8460 - lr: 1.5000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.5902 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.4690 - val_sparse_categorical_accuracy: 0.8400 - lr: 1.5000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5743 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4699 - val_sparse_categorical_accuracy: 0.8450 - lr: 1.5000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5771 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4687 - val_sparse_categorical_accuracy: 0.8420 - lr: 1.5000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5990 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.4690 - val_sparse_categorical_accuracy: 0.8460 - lr: 1.5000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.5626 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4679 - val_sparse_categorical_accuracy: 0.8450 - lr: 1.5000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5596 - sparse_categorical_accuracy: 0.7963 - val_loss: 0.4663 - val_sparse_categorical_accuracy: 0.8470 - lr: 1.5000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 7, test #1, 2nd iteration: loss of 0.474511057138443; sparse_categorical_accuracy of 84.7000002861023%\n",
            "appending basis + selective feature vector data\n",
            "[1352, 1784, 1831, 659, 1127, 358, 648, 715, 2341, 965, 886, 584, 512, 518, 1577, 78, 1270, 2623, 660, 830, 440, 732, 1347, 2642, 1960, 2017, 2305, 1859, 1845, 2521, 852, 189, 2217, 656, 1707, 384, 2075, 528, 1585, 2256, 1391, 1749, 1432, 577, 1888, 2707, 1718, 2459, 99, 1358, 1404, 1504, 2250, 1973, 1147, 2371, 969, 306, 2172, 866, 297, 312, 1062, 1679, 2290, 2063, 385, 799, 1416, 1914, 2660, 1414, 2209, 826, 1238, 787, 1761, 414, 1314, 1256, 65, 1, 2193, 1324, 419, 738, 1491, 1087, 1195, 1216, 1363, 1925, 1615, 553, 2315, 2274, 2310, 2407, 300, 646, 1431, 1008, 466, 80, 727, 2477, 1365, 824, 1167, 0, 378, 1961, 505, 1640, 1792, 2339, 1326, 1231, 1834, 2262, 1780, 1959, 2045, 1455, 2220, 2653, 631, 898, 87, 1145, 956, 188, 2563, 2259, 27, 2033, 1651, 1320, 8, 507, 663, 843, 966, 856, 2203, 1233, 2004, 2288, 344, 1217, 2602, 919, 2353, 1159, 359, 2355, 1747, 2489, 1220, 1659, 1853, 1670, 662, 970, 2266, 1796, 1265, 132, 2159, 1222, 862, 1378, 850, 1578, 563, 1303, 1906, 1113, 2512, 1486, 2679, 2141, 360, 1336, 552, 2456, 535, 637, 691, 2218, 1178, 1102, 1809, 654, 2344, 1984, 1800, 404, 128, 2600, 1111, 481, 447, 1444, 2502, 923, 110, 1144, 271, 1164, 564, 2325, 1876, 2140, 1778, 1632, 704, 1209, 2419, 311, 983, 1807, 2372, 53, 615, 1642, 1474, 937, 2428, 1586, 1169, 415, 1283, 1436, 967, 1720, 1771, 1017, 5, 2579, 1538, 2480, 1032, 122, 233, 899, 2149, 2050, 2119, 2029, 1604, 335, 138, 598, 884, 209, 1500, 1426, 974, 1529, 370, 1348, 1692, 159, 730, 90, 1489, 2402, 2071, 2655, 2550, 986, 291, 1524, 797, 1228, 2632, 1084, 231, 1867, 1452, 58, 1610, 2520, 1133, 750, 1899, 2417, 599, 2647, 2018, 1983, 435, 581, 1884, 1668, 2275, 2082, 1760, 2072, 2246, 1457, 1739, 706, 2143, 1612, 612, 2066, 685, 278, 2375, 2155, 2354, 234, 1094, 2446, 262, 678, 1993, 945, 2431, 427, 1061, 1107, 299, 361, 227, 1873, 1653, 2139, 1465, 43, 889, 2576, 1862, 1191, 1518, 453, 2030, 1700, 2326, 2444, 2106, 2195, 1218, 486, 708, 2525, 1280, 2363, 1997, 1654, 100, 867, 1036, 499, 2443, 2049, 2158, 677, 321, 2239, 418, 2191, 1816, 1208, 1362, 1907, 711, 313, 2323, 2346, 245, 1662, 1450, 1468, 1935, 1354, 1293, 714, 2131, 1756, 1157, 1344, 1069, 1331, 1706, 493, 1944, 1267, 725, 1965, 1574, 2546, 1185, 562, 115, 1912, 165, 1394, 172, 753, 375, 1618, 1559, 1345, 1386, 865, 2557, 1029, 1963, 1932, 1379, 2387, 774, 1949, 836, 692, 1798, 2214, 13, 1072, 104, 2294, 1681, 1835, 1449, 2595, 2279, 1795, 2662, 1028, 1989, 495, 1016, 601, 2376, 149, 1081, 2360, 2533, 1261, 492, 1545, 225, 1368, 524, 2581, 2493, 2694, 94, 1136, 1494, 124, 2586, 647, 2044, 1571, 1746, 2621, 2374, 653, 1292, 1506, 1879, 2547, 2406, 540, 275, 1641, 2613, 1729, 838, 1701, 1397, 1889, 921, 261, 241, 2528, 2212, 1523, 1026, 1875, 203, 2554, 2364, 560, 1981, 2687, 1264, 2047, 2384, 2534, 2270, 400, 2630, 2122, 59, 1332, 160, 963, 2377, 634, 982, 1407, 2095, 2695, 1828, 932, 696, 253, 2465, 1403, 1053, 1006, 519, 1339, 537, 718, 534, 1333, 1398, 1012, 170, 1895, 421, 1953, 1092, 1541, 1806, 851, 2380, 1893, 314, 1629, 546, 1300, 1762, 1916, 1433, 2236, 1410, 2460, 772, 1674, 1357, 2627, 664, 2603, 593, 2500, 2691, 1512, 559, 322, 572, 667, 556, 2152, 1179, 2412, 1166, 1740, 2343, 1389, 336, 433, 1279, 2399, 845, 891, 2005, 2508, 1725, 2350, 2468, 213, 1025, 2175, 494, 1823, 1657, 1901, 1498, 2128, 1665, 630, 2479, 632, 1815, 2634, 1015, 905, 2681, 32, 1234, 1882, 2515, 1402, 2319, 1584, 1696, 2574, 705, 1180, 1395, 408, 2186, 439, 244, 2166, 143, 980, 2373, 841, 2161, 1877, 757, 775, 1976, 1969, 1709, 913, 482, 338, 2482, 2100, 279, 501, 1991, 29, 116, 1647, 1075, 2449, 2132, 990, 825, 670, 57, 218, 1067, 1548, 2226, 1572, 1799, 2680, 1878, 987, 2342, 1967, 145, 561, 1509, 1499, 173, 1168, 2242, 1269, 1210, 2179, 325, 2173, 2154, 1669, 610, 820, 2169, 2464, 2002, 2088, 522, 1312, 2471, 1341, 452, 1120, 1703, 713, 902, 319, 2649, 2240, 515, 2276, 2064, 796, 441, 2366, 216, 620, 1323, 627, 1313, 1088, 2296, 236, 837, 1007, 156, 1109, 2257, 1413, 2156, 1447, 256, 1583, 1866, 643, 2255, 1802, 911, 1198, 283, 2062, 352, 1972, 1549, 2040, 2135, 2618, 2556, 1235, 2622, 1479, 2247, 2145, 2187, 767, 1649, 2338, 462, 179, 1998, 168, 2260, 1297, 18, 1278, 2059, 1458, 821, 1384, 460, 1857, 1082, 863, 1417, 2599, 1009, 301, 1052, 940, 1958, 739, 2531, 1110, 147, 1887, 2510, 2656, 330, 1205, 1931, 1539, 182, 1172, 2297, 62, 878, 274, 1020, 289, 1909, 981, 2204, 2177, 766, 2229, 2400, 264, 1631, 1566, 1045, 2497, 2575, 1097, 2028, 1608, 468, 869, 485, 1832, 1684, 2137, 764, 1207, 2170, 376, 1723, 2277, 2388, 2385, 2481, 476, 298, 2441, 1716, 1090, 2633, 371, 469, 859, 2564, 1863, 272, 1050, 1272, 1223, 1712, 1639, 106, 2452, 861, 953, 251, 412, 2345, 742, 2611, 623, 947, 1183, 2605, 1467, 237, 1438, 2268, 1850, 1797, 54, 1776, 1904, 1192, 2052, 548, 1146, 736, 2077, 2495, 1686, 2409, 1135, 2541, 2442, 2254, 510, 2450, 260, 759, 930, 1519, 1315, 417, 1319, 480, 929, 1840, 1593, 2183, 214, 800, 353, 1564, 1918, 1334, 1225, 1869, 395, 1675, 410, 1552, 701, 2559, 1623, 1516, 2612, 2225, 151, 1841, 193, 1260, 1367, 1846, 2118, 1021, 834, 316, 549, 644, 1520, 2111, 2626, 328, 2253, 2461, 1383, 895, 1057, 24, 2107, 1408, 2688, 150, 2463, 720, 2472, 1540, 1013, 1089, 365, 1558, 1469, 1098, 2699, 1605, 320, 2222, 1138, 2038, 1377, 19, 369, 1661, 2232, 1077, 1942, 883, 250, 686, 2151, 1837, 2176, 808, 107, 1401, 392, 2289, 1289, 1360, 1385, 2301, 2058, 2184, 1374, 28, 431, 184, 1031, 1532, 1619, 1338, 167, 1396, 935, 290, 1947, 2379, 2549, 2065, 1448, 2598, 792, 1805, 304, 2027, 1496, 2086, 197, 285, 238, 894, 2530, 748, 1783, 2644, 961, 2083, 2561, 2285, 2631, 1644, 487, 1291, 668, 1655, 2675, 366, 1301]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 10.0287 - sparse_categorical_accuracy: 0.1809 - val_loss: 3.0931 - val_sparse_categorical_accuracy: 0.2840 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 8.7310 - sparse_categorical_accuracy: 0.2055 - val_loss: 2.7142 - val_sparse_categorical_accuracy: 0.3060 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.7648 - sparse_categorical_accuracy: 0.2037 - val_loss: 2.9675 - val_sparse_categorical_accuracy: 0.3020 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.4314 - sparse_categorical_accuracy: 0.2008 - val_loss: 2.3915 - val_sparse_categorical_accuracy: 0.3170 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 7.1202 - sparse_categorical_accuracy: 0.2119 - val_loss: 2.3690 - val_sparse_categorical_accuracy: 0.3240 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.6382 - sparse_categorical_accuracy: 0.2026 - val_loss: 1.9106 - val_sparse_categorical_accuracy: 0.3370 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.3842 - sparse_categorical_accuracy: 0.2166 - val_loss: 2.0696 - val_sparse_categorical_accuracy: 0.3450 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.7360 - sparse_categorical_accuracy: 0.2260 - val_loss: 1.9515 - val_sparse_categorical_accuracy: 0.3340 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.2937 - sparse_categorical_accuracy: 0.2494 - val_loss: 1.6429 - val_sparse_categorical_accuracy: 0.3800 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.2379 - sparse_categorical_accuracy: 0.2547 - val_loss: 1.7775 - val_sparse_categorical_accuracy: 0.3690 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.6666 - sparse_categorical_accuracy: 0.2547 - val_loss: 1.6396 - val_sparse_categorical_accuracy: 0.3860 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.3544 - sparse_categorical_accuracy: 0.2629 - val_loss: 1.4935 - val_sparse_categorical_accuracy: 0.4170 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.0638 - sparse_categorical_accuracy: 0.2740 - val_loss: 1.4231 - val_sparse_categorical_accuracy: 0.4370 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.9468 - sparse_categorical_accuracy: 0.2781 - val_loss: 1.2964 - val_sparse_categorical_accuracy: 0.4760 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5870 - sparse_categorical_accuracy: 0.3039 - val_loss: 1.1675 - val_sparse_categorical_accuracy: 0.5300 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.4545 - sparse_categorical_accuracy: 0.3080 - val_loss: 1.0920 - val_sparse_categorical_accuracy: 0.5750 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1570 - sparse_categorical_accuracy: 0.3238 - val_loss: 1.1213 - val_sparse_categorical_accuracy: 0.5410 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1293 - sparse_categorical_accuracy: 0.3126 - val_loss: 1.0718 - val_sparse_categorical_accuracy: 0.5700 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.9023 - sparse_categorical_accuracy: 0.3530 - val_loss: 1.0338 - val_sparse_categorical_accuracy: 0.5890 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.7625 - sparse_categorical_accuracy: 0.3378 - val_loss: 0.9823 - val_sparse_categorical_accuracy: 0.6290 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5271 - sparse_categorical_accuracy: 0.3718 - val_loss: 1.0053 - val_sparse_categorical_accuracy: 0.5970 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5150 - sparse_categorical_accuracy: 0.3519 - val_loss: 0.9524 - val_sparse_categorical_accuracy: 0.6250 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2234 - sparse_categorical_accuracy: 0.4046 - val_loss: 0.9022 - val_sparse_categorical_accuracy: 0.6620 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1358 - sparse_categorical_accuracy: 0.4081 - val_loss: 0.8885 - val_sparse_categorical_accuracy: 0.6670 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0161 - sparse_categorical_accuracy: 0.4297 - val_loss: 0.8672 - val_sparse_categorical_accuracy: 0.6570 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9859 - sparse_categorical_accuracy: 0.4327 - val_loss: 0.9047 - val_sparse_categorical_accuracy: 0.6290 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8346 - sparse_categorical_accuracy: 0.4526 - val_loss: 0.8285 - val_sparse_categorical_accuracy: 0.7010 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8848 - sparse_categorical_accuracy: 0.4426 - val_loss: 0.8229 - val_sparse_categorical_accuracy: 0.6920 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7404 - sparse_categorical_accuracy: 0.4608 - val_loss: 0.8272 - val_sparse_categorical_accuracy: 0.6760 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7114 - sparse_categorical_accuracy: 0.4625 - val_loss: 0.7972 - val_sparse_categorical_accuracy: 0.7080 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6010 - sparse_categorical_accuracy: 0.4848 - val_loss: 0.7847 - val_sparse_categorical_accuracy: 0.7060 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5693 - sparse_categorical_accuracy: 0.4965 - val_loss: 0.7605 - val_sparse_categorical_accuracy: 0.7350 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4932 - sparse_categorical_accuracy: 0.4924 - val_loss: 0.7445 - val_sparse_categorical_accuracy: 0.7480 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4978 - sparse_categorical_accuracy: 0.5018 - val_loss: 0.7393 - val_sparse_categorical_accuracy: 0.7520 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3501 - sparse_categorical_accuracy: 0.5492 - val_loss: 0.7367 - val_sparse_categorical_accuracy: 0.7430 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3413 - sparse_categorical_accuracy: 0.5433 - val_loss: 0.7169 - val_sparse_categorical_accuracy: 0.7670 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2675 - sparse_categorical_accuracy: 0.5603 - val_loss: 0.7146 - val_sparse_categorical_accuracy: 0.7590 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2586 - sparse_categorical_accuracy: 0.5632 - val_loss: 0.7028 - val_sparse_categorical_accuracy: 0.7670 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2124 - sparse_categorical_accuracy: 0.5591 - val_loss: 0.6915 - val_sparse_categorical_accuracy: 0.7850 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.1796 - sparse_categorical_accuracy: 0.5808 - val_loss: 0.6877 - val_sparse_categorical_accuracy: 0.7860 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1528 - sparse_categorical_accuracy: 0.5872 - val_loss: 0.6833 - val_sparse_categorical_accuracy: 0.7860 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1398 - sparse_categorical_accuracy: 0.5872 - val_loss: 0.6803 - val_sparse_categorical_accuracy: 0.7780 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.1087 - sparse_categorical_accuracy: 0.6025 - val_loss: 0.6655 - val_sparse_categorical_accuracy: 0.7930 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0817 - sparse_categorical_accuracy: 0.6165 - val_loss: 0.6581 - val_sparse_categorical_accuracy: 0.8010 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0709 - sparse_categorical_accuracy: 0.6230 - val_loss: 0.6539 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0488 - sparse_categorical_accuracy: 0.6183 - val_loss: 0.6439 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9388 - sparse_categorical_accuracy: 0.6557 - val_loss: 0.6479 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.9984 - sparse_categorical_accuracy: 0.6235 - val_loss: 0.6336 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9325 - sparse_categorical_accuracy: 0.6587 - val_loss: 0.6308 - val_sparse_categorical_accuracy: 0.8010 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9260 - sparse_categorical_accuracy: 0.6610 - val_loss: 0.6209 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8946 - sparse_categorical_accuracy: 0.6534 - val_loss: 0.6202 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8907 - sparse_categorical_accuracy: 0.6721 - val_loss: 0.6121 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8783 - sparse_categorical_accuracy: 0.6680 - val_loss: 0.6094 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.8493 - sparse_categorical_accuracy: 0.6961 - val_loss: 0.5977 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8531 - sparse_categorical_accuracy: 0.6903 - val_loss: 0.5927 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8367 - sparse_categorical_accuracy: 0.6792 - val_loss: 0.5936 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8471 - sparse_categorical_accuracy: 0.6815 - val_loss: 0.5926 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7881 - sparse_categorical_accuracy: 0.7090 - val_loss: 0.5795 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7948 - sparse_categorical_accuracy: 0.7032 - val_loss: 0.5769 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7831 - sparse_categorical_accuracy: 0.7131 - val_loss: 0.5699 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7629 - sparse_categorical_accuracy: 0.7131 - val_loss: 0.5667 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7697 - sparse_categorical_accuracy: 0.7102 - val_loss: 0.5660 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7354 - sparse_categorical_accuracy: 0.7289 - val_loss: 0.5579 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7488 - sparse_categorical_accuracy: 0.7125 - val_loss: 0.5536 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7324 - sparse_categorical_accuracy: 0.7160 - val_loss: 0.5569 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7177 - sparse_categorical_accuracy: 0.7319 - val_loss: 0.5528 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6992 - sparse_categorical_accuracy: 0.7506 - val_loss: 0.5447 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7213 - sparse_categorical_accuracy: 0.7307 - val_loss: 0.5410 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6932 - sparse_categorical_accuracy: 0.7395 - val_loss: 0.5347 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6829 - sparse_categorical_accuracy: 0.7535 - val_loss: 0.5378 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6832 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5297 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6856 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5255 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6737 - sparse_categorical_accuracy: 0.7441 - val_loss: 0.5209 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6794 - sparse_categorical_accuracy: 0.7436 - val_loss: 0.5184 - val_sparse_categorical_accuracy: 0.8460 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6413 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5218 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6496 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5174 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6438 - sparse_categorical_accuracy: 0.7564 - val_loss: 0.5112 - val_sparse_categorical_accuracy: 0.8450 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6291 - sparse_categorical_accuracy: 0.7705 - val_loss: 0.5121 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6341 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.5114 - val_sparse_categorical_accuracy: 0.8480 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6420 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.5054 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6358 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.5011 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6199 - sparse_categorical_accuracy: 0.7705 - val_loss: 0.5019 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6221 - sparse_categorical_accuracy: 0.7576 - val_loss: 0.5031 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6205 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.4996 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6185 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.4938 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6217 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.4899 - val_sparse_categorical_accuracy: 0.8440 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5881 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.4864 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6144 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.4870 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5914 - sparse_categorical_accuracy: 0.7758 - val_loss: 0.4824 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5883 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.4784 - val_sparse_categorical_accuracy: 0.8450 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5914 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.4823 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5927 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.4758 - val_sparse_categorical_accuracy: 0.8480 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5905 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.4764 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5854 - sparse_categorical_accuracy: 0.7939 - val_loss: 0.4744 - val_sparse_categorical_accuracy: 0.8460 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5704 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4771 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5753 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.4722 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5694 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4698 - val_sparse_categorical_accuracy: 0.8460 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5697 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4708 - val_sparse_categorical_accuracy: 0.8450 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5760 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.4656 - val_sparse_categorical_accuracy: 0.8470 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5828 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.4629 - val_sparse_categorical_accuracy: 0.8470 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 7, test #2, 2nd iteration: loss of 0.5113964080810547; sparse_categorical_accuracy of 84.79999899864197%\n",
            "appending basis + selective feature vector data\n",
            "[1352, 1784, 1831, 659, 1127, 358, 648, 715, 2341, 965, 886, 584, 512, 518, 1577, 78, 1270, 2623, 660, 830, 440, 732, 1347, 2642, 1960, 2017, 2305, 1859, 1845, 2521, 852, 189, 2217, 656, 1707, 384, 2075, 528, 1585, 2256, 1391, 1749, 1432, 577, 1888, 2707, 1718, 2459, 99, 1358, 1404, 1504, 2250, 1973, 1147, 2371, 969, 306, 2172, 866, 297, 312, 1062, 1679, 2290, 2063, 385, 799, 1416, 1914, 2660, 1414, 2209, 826, 1238, 787, 1761, 414, 1314, 1256, 65, 1, 2193, 1324, 419, 738, 1491, 1087, 1195, 1216, 1363, 1925, 1615, 553, 2315, 2274, 2310, 2407, 300, 646, 1431, 1008, 466, 80, 727, 2477, 1365, 824, 1167, 0, 378, 1961, 505, 1640, 1792, 2339, 1326, 1231, 1834, 2262, 1780, 1959, 2045, 1455, 2220, 2653, 631, 898, 87, 1145, 956, 188, 2563, 2259, 27, 2033, 1651, 1320, 8, 507, 663, 843, 966, 856, 2203, 1233, 2004, 2288, 344, 1217, 2602, 919, 2353, 1159, 359, 2355, 1747, 2489, 1220, 1659, 1853, 1670, 662, 970, 2266, 1796, 1265, 132, 2159, 1222, 862, 1378, 850, 1578, 563, 1303, 1906, 1113, 2512, 1486, 2679, 2141, 360, 1336, 552, 2456, 535, 637, 691, 2218, 1178, 1102, 1809, 654, 2344, 1984, 1800, 404, 128, 2600, 1111, 481, 447, 1444, 2502, 923, 110, 1144, 271, 1164, 564, 2325, 1876, 2140, 1778, 1632, 704, 1209, 2419, 311, 983, 1807, 2372, 53, 615, 1642, 1474, 937, 2428, 1586, 1169, 415, 1283, 1436, 967, 1720, 1771, 1017, 5, 2579, 1538, 2480, 1032, 122, 233, 899, 2149, 2050, 2119, 2029, 1604, 335, 138, 598, 884, 209, 1500, 1426, 974, 1529, 370, 1348, 1692, 159, 730, 90, 1489, 2402, 2071, 2655, 2550, 986, 291, 1524, 797, 1228, 2632, 1084, 231, 1867, 1452, 58, 1610, 2520, 1133, 750, 1899, 2417, 599, 2647, 2018, 1983, 435, 581, 1884, 1668, 2275, 2082, 1760, 2072, 2246, 1457, 1739, 706, 2143, 1612, 612, 2066, 685, 278, 2375, 2155, 2354, 234, 1094, 2446, 262, 678, 1993, 945, 2431, 427, 1061, 1107, 299, 361, 227, 1873, 1653, 2139, 1465, 43, 889, 2576, 1862, 1191, 1518, 453, 2030, 1700, 2326, 2444, 2106, 2195, 1218, 486, 708, 2525, 1280, 2363, 1997, 1654, 100, 867, 1036, 499, 2443, 2049, 2158, 677, 321, 2239, 418, 2191, 1816, 1208, 1362, 1907, 711, 313, 2323, 2346, 245, 1662, 1450, 1468, 1935, 1354, 1293, 714, 2131, 1756, 1157, 1344, 1069, 1331, 1706, 493, 1944, 1267, 725, 1965, 1574, 2546, 1185, 562, 115, 1912, 165, 1394, 172, 753, 375, 1618, 1559, 1345, 1386, 865, 2557, 1029, 1963, 1932, 1379, 2387, 774, 1949, 836, 692, 1798, 2214, 13, 1072, 104, 2294, 1681, 1835, 1449, 2595, 2279, 1795, 2662, 1028, 1989, 495, 1016, 601, 2376, 149, 1081, 2360, 2533, 1261, 492, 1545, 225, 1368, 524, 2581, 2493, 2694, 94, 1136, 1494, 124, 2586, 647, 2044, 1571, 1746, 2621, 2374, 653, 1292, 1506, 1879, 2547, 2406, 540, 275, 1641, 2613, 1729, 838, 1701, 1397, 1889, 921, 261, 241, 2528, 2212, 1523, 1026, 1875, 203, 2554, 2364, 560, 1981, 2687, 1264, 2047, 2384, 2534, 2270, 400, 2630, 2122, 59, 1332, 160, 963, 2377, 634, 982, 1407, 2095, 2695, 1828, 932, 696, 253, 2465, 1403, 1053, 1006, 519, 1339, 537, 718, 534, 1333, 1398, 1012, 170, 1895, 421, 1953, 1092, 1541, 1806, 851, 2380, 1893, 314, 1629, 546, 1300, 1762, 1916, 1433, 2236, 1410, 2460, 772, 1674, 1357, 2627, 664, 2603, 593, 2500, 2691, 1512, 559, 322, 572, 667, 556, 2152, 1179, 2412, 1166, 1740, 2343, 1389, 336, 433, 1279, 2399, 845, 891, 2005, 2508, 1725, 2350, 2468, 213, 1025, 2175, 494, 1823, 1657, 1901, 1498, 2128, 1665, 630, 2479, 632, 1815, 2634, 1015, 905, 2681, 32, 1234, 1882, 2515, 1402, 2319, 1584, 1696, 2574, 705, 1180, 1395, 408, 2186, 439, 244, 2166, 143, 980, 2373, 841, 2161, 1877, 757, 775, 1976, 1969, 1709, 913, 482, 338, 2482, 2100, 279, 501, 1991, 29, 116, 1647, 1075, 2449, 2132, 990, 825, 670, 57, 218, 1067, 1548, 2226, 1572, 1799, 2680, 1878, 987, 2342, 1967, 145, 561, 1509, 1499, 173, 1168, 2242, 1269, 1210, 2179, 325, 2173, 2154, 1669, 610, 820, 2169, 2464, 2002, 2088, 522, 1312, 2471, 1341, 452, 1120, 1703, 713, 902, 319, 2649, 2240, 515, 2276, 2064, 796, 441, 2366, 216, 620, 1323, 627, 1313, 1088, 2296, 236, 837, 1007, 156, 1109, 2257, 1413, 2156, 1447, 256, 1583, 1866, 643, 2255, 1802, 911, 1198, 283, 2062, 352, 1972, 1549, 2040, 2135, 2618, 2556, 1235, 2622, 1479, 2247, 2145, 2187, 767, 1649, 2338, 462, 179, 1998, 168, 2260, 1297, 18, 1278, 2059, 1458, 821, 1384, 460, 1857, 1082, 863, 1417, 2599, 1009, 301, 1052, 940, 1958, 739, 2531, 1110, 147, 1887, 2510, 2656, 330, 1205, 1931, 1539, 182, 1172, 2297, 62, 878, 274, 1020, 289, 1909, 981, 2204, 2177, 766, 2229, 2400, 264, 1631, 1566, 1045, 2497, 2575, 1097, 2028, 1608, 468, 869, 485, 1832, 1684, 2137, 764, 1207, 2170, 376, 1723, 2277, 2388, 2385, 2481, 476, 298, 2441, 1716, 1090, 2633, 371, 469, 859, 2564, 1863, 272, 1050, 1272, 1223, 1712, 1639, 106, 2452, 861, 953, 251, 412, 2345, 742, 2611, 623, 947, 1183, 2605, 1467, 237, 1438, 2268, 1850, 1797, 54, 1776, 1904, 1192, 2052, 548, 1146, 736, 2077, 2495, 1686, 2409, 1135, 2541, 2442, 2254, 510, 2450, 260, 759, 930, 1519, 1315, 417, 1319, 480, 929, 1840, 1593, 2183, 214, 800, 353, 1564, 1918, 1334, 1225, 1869, 395, 1675, 410, 1552, 701, 2559, 1623, 1516, 2612, 2225, 151, 1841, 193, 1260, 1367, 1846, 2118, 1021, 834, 316, 549, 644, 1520, 2111, 2626, 328, 2253, 2461, 1383, 895, 1057, 24, 2107, 1408, 2688, 150, 2463, 720, 2472, 1540, 1013, 1089, 365, 1558, 1469, 1098, 2699, 1605, 320, 2222, 1138, 2038, 1377, 19, 369, 1661, 2232, 1077, 1942, 883, 250, 686, 2151, 1837, 2176, 808, 107, 1401, 392, 2289, 1289, 1360, 1385, 2301, 2058, 2184, 1374, 28, 431, 184, 1031, 1532, 1619, 1338, 167, 1396, 935, 290, 1947, 2379, 2549, 2065, 1448, 2598, 792, 1805, 304, 2027, 1496, 2086, 197, 285, 238, 894, 2530, 748, 1783, 2644, 961, 2083, 2561, 2285, 2631, 1644, 487, 1291, 668, 1655, 2675, 366, 1301]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 9.7881 - sparse_categorical_accuracy: 0.1493 - val_loss: 2.8989 - val_sparse_categorical_accuracy: 0.2820 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 8.0665 - sparse_categorical_accuracy: 0.1762 - val_loss: 2.7494 - val_sparse_categorical_accuracy: 0.2950 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.7409 - sparse_categorical_accuracy: 0.1669 - val_loss: 2.9781 - val_sparse_categorical_accuracy: 0.2950 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.5893 - sparse_categorical_accuracy: 0.2172 - val_loss: 2.2976 - val_sparse_categorical_accuracy: 0.3140 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.3747 - sparse_categorical_accuracy: 0.1973 - val_loss: 2.0808 - val_sparse_categorical_accuracy: 0.3260 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.0248 - sparse_categorical_accuracy: 0.2055 - val_loss: 2.2629 - val_sparse_categorical_accuracy: 0.3350 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.5451 - sparse_categorical_accuracy: 0.2043 - val_loss: 2.1862 - val_sparse_categorical_accuracy: 0.3450 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.2991 - sparse_categorical_accuracy: 0.2131 - val_loss: 2.1271 - val_sparse_categorical_accuracy: 0.3390 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.8997 - sparse_categorical_accuracy: 0.2453 - val_loss: 1.7187 - val_sparse_categorical_accuracy: 0.3910 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.6194 - sparse_categorical_accuracy: 0.2313 - val_loss: 1.5348 - val_sparse_categorical_accuracy: 0.4240 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.3574 - sparse_categorical_accuracy: 0.2477 - val_loss: 1.5129 - val_sparse_categorical_accuracy: 0.4370 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.7868 - sparse_categorical_accuracy: 0.2670 - val_loss: 1.4151 - val_sparse_categorical_accuracy: 0.4450 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5758 - sparse_categorical_accuracy: 0.2898 - val_loss: 1.3148 - val_sparse_categorical_accuracy: 0.4870 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.3556 - sparse_categorical_accuracy: 0.2845 - val_loss: 1.4420 - val_sparse_categorical_accuracy: 0.4530 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1817 - sparse_categorical_accuracy: 0.3173 - val_loss: 1.2996 - val_sparse_categorical_accuracy: 0.4780 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.0051 - sparse_categorical_accuracy: 0.3132 - val_loss: 1.1569 - val_sparse_categorical_accuracy: 0.5590 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8691 - sparse_categorical_accuracy: 0.3267 - val_loss: 1.1170 - val_sparse_categorical_accuracy: 0.5740 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.6530 - sparse_categorical_accuracy: 0.3443 - val_loss: 1.1488 - val_sparse_categorical_accuracy: 0.5400 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4839 - sparse_categorical_accuracy: 0.3677 - val_loss: 0.9810 - val_sparse_categorical_accuracy: 0.6600 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3576 - sparse_categorical_accuracy: 0.3753 - val_loss: 1.0314 - val_sparse_categorical_accuracy: 0.5960 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1941 - sparse_categorical_accuracy: 0.3882 - val_loss: 0.9544 - val_sparse_categorical_accuracy: 0.6630 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2360 - sparse_categorical_accuracy: 0.3823 - val_loss: 0.8892 - val_sparse_categorical_accuracy: 0.7080 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0518 - sparse_categorical_accuracy: 0.4052 - val_loss: 0.8801 - val_sparse_categorical_accuracy: 0.7010 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8833 - sparse_categorical_accuracy: 0.4479 - val_loss: 0.8791 - val_sparse_categorical_accuracy: 0.6960 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.9121 - sparse_categorical_accuracy: 0.4379 - val_loss: 0.8595 - val_sparse_categorical_accuracy: 0.7090 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8062 - sparse_categorical_accuracy: 0.4514 - val_loss: 0.8203 - val_sparse_categorical_accuracy: 0.7320 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6959 - sparse_categorical_accuracy: 0.4643 - val_loss: 0.8062 - val_sparse_categorical_accuracy: 0.7420 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5588 - sparse_categorical_accuracy: 0.4895 - val_loss: 0.7863 - val_sparse_categorical_accuracy: 0.7420 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5134 - sparse_categorical_accuracy: 0.5082 - val_loss: 0.7902 - val_sparse_categorical_accuracy: 0.7330 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4468 - sparse_categorical_accuracy: 0.5211 - val_loss: 0.7634 - val_sparse_categorical_accuracy: 0.7540 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4350 - sparse_categorical_accuracy: 0.5152 - val_loss: 0.7406 - val_sparse_categorical_accuracy: 0.7680 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3712 - sparse_categorical_accuracy: 0.5281 - val_loss: 0.7414 - val_sparse_categorical_accuracy: 0.7510 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3095 - sparse_categorical_accuracy: 0.5568 - val_loss: 0.7200 - val_sparse_categorical_accuracy: 0.7670 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3169 - sparse_categorical_accuracy: 0.5597 - val_loss: 0.7089 - val_sparse_categorical_accuracy: 0.7680 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2717 - sparse_categorical_accuracy: 0.5615 - val_loss: 0.6947 - val_sparse_categorical_accuracy: 0.7820 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1791 - sparse_categorical_accuracy: 0.5732 - val_loss: 0.6921 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1521 - sparse_categorical_accuracy: 0.5960 - val_loss: 0.6799 - val_sparse_categorical_accuracy: 0.7840 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1101 - sparse_categorical_accuracy: 0.5972 - val_loss: 0.6695 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1383 - sparse_categorical_accuracy: 0.5984 - val_loss: 0.6677 - val_sparse_categorical_accuracy: 0.7820 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0714 - sparse_categorical_accuracy: 0.6230 - val_loss: 0.6574 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0076 - sparse_categorical_accuracy: 0.6317 - val_loss: 0.6656 - val_sparse_categorical_accuracy: 0.7790 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0169 - sparse_categorical_accuracy: 0.6317 - val_loss: 0.6448 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0176 - sparse_categorical_accuracy: 0.6300 - val_loss: 0.6373 - val_sparse_categorical_accuracy: 0.7920 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9531 - sparse_categorical_accuracy: 0.6604 - val_loss: 0.6312 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9578 - sparse_categorical_accuracy: 0.6511 - val_loss: 0.6316 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9042 - sparse_categorical_accuracy: 0.6669 - val_loss: 0.6184 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9091 - sparse_categorical_accuracy: 0.6680 - val_loss: 0.6104 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8965 - sparse_categorical_accuracy: 0.6715 - val_loss: 0.6130 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8606 - sparse_categorical_accuracy: 0.6862 - val_loss: 0.6065 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8907 - sparse_categorical_accuracy: 0.6686 - val_loss: 0.5978 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8323 - sparse_categorical_accuracy: 0.6903 - val_loss: 0.6051 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7933 - sparse_categorical_accuracy: 0.6909 - val_loss: 0.6072 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8622 - sparse_categorical_accuracy: 0.6897 - val_loss: 0.5838 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8045 - sparse_categorical_accuracy: 0.7014 - val_loss: 0.5790 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.7942 - sparse_categorical_accuracy: 0.7055 - val_loss: 0.5743 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7849 - sparse_categorical_accuracy: 0.7166 - val_loss: 0.5845 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7476 - sparse_categorical_accuracy: 0.7207 - val_loss: 0.5685 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7635 - sparse_categorical_accuracy: 0.7119 - val_loss: 0.5677 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7460 - sparse_categorical_accuracy: 0.7289 - val_loss: 0.5606 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7440 - sparse_categorical_accuracy: 0.7254 - val_loss: 0.5571 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7488 - sparse_categorical_accuracy: 0.7213 - val_loss: 0.5549 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7255 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5481 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7306 - sparse_categorical_accuracy: 0.7307 - val_loss: 0.5462 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7136 - sparse_categorical_accuracy: 0.7330 - val_loss: 0.5419 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6945 - sparse_categorical_accuracy: 0.7371 - val_loss: 0.5416 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7099 - sparse_categorical_accuracy: 0.7400 - val_loss: 0.5425 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.7068 - sparse_categorical_accuracy: 0.7330 - val_loss: 0.5298 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6838 - sparse_categorical_accuracy: 0.7459 - val_loss: 0.5275 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6652 - sparse_categorical_accuracy: 0.7523 - val_loss: 0.5308 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6681 - sparse_categorical_accuracy: 0.7523 - val_loss: 0.5240 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6725 - sparse_categorical_accuracy: 0.7412 - val_loss: 0.5164 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6652 - sparse_categorical_accuracy: 0.7535 - val_loss: 0.5184 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6513 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.5146 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6536 - sparse_categorical_accuracy: 0.7641 - val_loss: 0.5130 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6442 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.5087 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6249 - sparse_categorical_accuracy: 0.7728 - val_loss: 0.5088 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6278 - sparse_categorical_accuracy: 0.7576 - val_loss: 0.5049 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6141 - sparse_categorical_accuracy: 0.7693 - val_loss: 0.5041 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6100 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.4994 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6160 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.4994 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6159 - sparse_categorical_accuracy: 0.7705 - val_loss: 0.4973 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6332 - sparse_categorical_accuracy: 0.7629 - val_loss: 0.4940 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6179 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.4906 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6238 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.4885 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6003 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.4843 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6141 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.4844 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5702 - sparse_categorical_accuracy: 0.7904 - val_loss: 0.4825 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6066 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.4835 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5893 - sparse_categorical_accuracy: 0.7851 - val_loss: 0.4818 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5892 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.4772 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5978 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.4826 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5764 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4741 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5567 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4702 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5803 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.4710 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5777 - sparse_categorical_accuracy: 0.7693 - val_loss: 0.4705 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5559 - sparse_categorical_accuracy: 0.7881 - val_loss: 0.4682 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5937 - sparse_categorical_accuracy: 0.7728 - val_loss: 0.4704 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5605 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.4643 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5718 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4618 - val_sparse_categorical_accuracy: 0.8440 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5537 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.4633 - val_sparse_categorical_accuracy: 0.8480 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 7, test #3, 2nd iteration: loss of 0.4632578194141388; sparse_categorical_accuracy of 84.79999899864197%\n",
            "appending basis + selective feature vector data\n",
            "[1352, 1784, 1831, 659, 1127, 358, 648, 715, 2341, 965, 886, 584, 512, 518, 1577, 78, 1270, 2623, 660, 830, 440, 732, 1347, 2642, 1960, 2017, 2305, 1859, 1845, 2521, 852, 189, 2217, 656, 1707, 384, 2075, 528, 1585, 2256, 1391, 1749, 1432, 577, 1888, 2707, 1718, 2459, 99, 1358, 1404, 1504, 2250, 1973, 1147, 2371, 969, 306, 2172, 866, 297, 312, 1062, 1679, 2290, 2063, 385, 799, 1416, 1914, 2660, 1414, 2209, 826, 1238, 787, 1761, 414, 1314, 1256, 65, 1, 2193, 1324, 419, 738, 1491, 1087, 1195, 1216, 1363, 1925, 1615, 553, 2315, 2274, 2310, 2407, 300, 646, 1431, 1008, 466, 80, 727, 2477, 1365, 824, 1167, 0, 378, 1961, 505, 1640, 1792, 2339, 1326, 1231, 1834, 2262, 1780, 1959, 2045, 1455, 2220, 2653, 631, 898, 87, 1145, 956, 188, 2563, 2259, 27, 2033, 1651, 1320, 8, 507, 663, 843, 966, 856, 2203, 1233, 2004, 2288, 344, 1217, 2602, 919, 2353, 1159, 359, 2355, 1747, 2489, 1220, 1659, 1853, 1670, 662, 970, 2266, 1796, 1265, 132, 2159, 1222, 862, 1378, 850, 1578, 563, 1303, 1906, 1113, 2512, 1486, 2679, 2141, 360, 1336, 552, 2456, 535, 637, 691, 2218, 1178, 1102, 1809, 654, 2344, 1984, 1800, 404, 128, 2600, 1111, 481, 447, 1444, 2502, 923, 110, 1144, 271, 1164, 564, 2325, 1876, 2140, 1778, 1632, 704, 1209, 2419, 311, 983, 1807, 2372, 53, 615, 1642, 1474, 937, 2428, 1586, 1169, 415, 1283, 1436, 967, 1720, 1771, 1017, 5, 2579, 1538, 2480, 1032, 122, 233, 899, 2149, 2050, 2119, 2029, 1604, 335, 138, 598, 884, 209, 1500, 1426, 974, 1529, 370, 1348, 1692, 159, 730, 90, 1489, 2402, 2071, 2655, 2550, 986, 291, 1524, 797, 1228, 2632, 1084, 231, 1867, 1452, 58, 1610, 2520, 1133, 750, 1899, 2417, 599, 2647, 2018, 1983, 435, 581, 1884, 1668, 2275, 2082, 1760, 2072, 2246, 1457, 1739, 706, 2143, 1612, 612, 2066, 685, 278, 2375, 2155, 2354, 234, 1094, 2446, 262, 678, 1993, 945, 2431, 427, 1061, 1107, 299, 361, 227, 1873, 1653, 2139, 1465, 43, 889, 2576, 1862, 1191, 1518, 453, 2030, 1700, 2326, 2444, 2106, 2195, 1218, 486, 708, 2525, 1280, 2363, 1997, 1654, 100, 867, 1036, 499, 2443, 2049, 2158, 677, 321, 2239, 418, 2191, 1816, 1208, 1362, 1907, 711, 313, 2323, 2346, 245, 1662, 1450, 1468, 1935, 1354, 1293, 714, 2131, 1756, 1157, 1344, 1069, 1331, 1706, 493, 1944, 1267, 725, 1965, 1574, 2546, 1185, 562, 115, 1912, 165, 1394, 172, 753, 375, 1618, 1559, 1345, 1386, 865, 2557, 1029, 1963, 1932, 1379, 2387, 774, 1949, 836, 692, 1798, 2214, 13, 1072, 104, 2294, 1681, 1835, 1449, 2595, 2279, 1795, 2662, 1028, 1989, 495, 1016, 601, 2376, 149, 1081, 2360, 2533, 1261, 492, 1545, 225, 1368, 524, 2581, 2493, 2694, 94, 1136, 1494, 124, 2586, 647, 2044, 1571, 1746, 2621, 2374, 653, 1292, 1506, 1879, 2547, 2406, 540, 275, 1641, 2613, 1729, 838, 1701, 1397, 1889, 921, 261, 241, 2528, 2212, 1523, 1026, 1875, 203, 2554, 2364, 560, 1981, 2687, 1264, 2047, 2384, 2534, 2270, 400, 2630, 2122, 59, 1332, 160, 963, 2377, 634, 982, 1407, 2095, 2695, 1828, 932, 696, 253, 2465, 1403, 1053, 1006, 519, 1339, 537, 718, 534, 1333, 1398, 1012, 170, 1895, 421, 1953, 1092, 1541, 1806, 851, 2380, 1893, 314, 1629, 546, 1300, 1762, 1916, 1433, 2236, 1410, 2460, 772, 1674, 1357, 2627, 664, 2603, 593, 2500, 2691, 1512, 559, 322, 572, 667, 556, 2152, 1179, 2412, 1166, 1740, 2343, 1389, 336, 433, 1279, 2399, 845, 891, 2005, 2508, 1725, 2350, 2468, 213, 1025, 2175, 494, 1823, 1657, 1901, 1498, 2128, 1665, 630, 2479, 632, 1815, 2634, 1015, 905, 2681, 32, 1234, 1882, 2515, 1402, 2319, 1584, 1696, 2574, 705, 1180, 1395, 408, 2186, 439, 244, 2166, 143, 980, 2373, 841, 2161, 1877, 757, 775, 1976, 1969, 1709, 913, 482, 338, 2482, 2100, 279, 501, 1991, 29, 116, 1647, 1075, 2449, 2132, 990, 825, 670, 57, 218, 1067, 1548, 2226, 1572, 1799, 2680, 1878, 987, 2342, 1967, 145, 561, 1509, 1499, 173, 1168, 2242, 1269, 1210, 2179, 325, 2173, 2154, 1669, 610, 820, 2169, 2464, 2002, 2088, 522, 1312, 2471, 1341, 452, 1120, 1703, 713, 902, 319, 2649, 2240, 515, 2276, 2064, 796, 441, 2366, 216, 620, 1323, 627, 1313, 1088, 2296, 236, 837, 1007, 156, 1109, 2257, 1413, 2156, 1447, 256, 1583, 1866, 643, 2255, 1802, 911, 1198, 283, 2062, 352, 1972, 1549, 2040, 2135, 2618, 2556, 1235, 2622, 1479, 2247, 2145, 2187, 767, 1649, 2338, 462, 179, 1998, 168, 2260, 1297, 18, 1278, 2059, 1458, 821, 1384, 460, 1857, 1082, 863, 1417, 2599, 1009, 301, 1052, 940, 1958, 739, 2531, 1110, 147, 1887, 2510, 2656, 330, 1205, 1931, 1539, 182, 1172, 2297, 62, 878, 274, 1020, 289, 1909, 981, 2204, 2177, 766, 2229, 2400, 264, 1631, 1566, 1045, 2497, 2575, 1097, 2028, 1608, 468, 869, 485, 1832, 1684, 2137, 764, 1207, 2170, 376, 1723, 2277, 2388, 2385, 2481, 476, 298, 2441, 1716, 1090, 2633, 371, 469, 859, 2564, 1863, 272, 1050, 1272, 1223, 1712, 1639, 106, 2452, 861, 953, 251, 412, 2345, 742, 2611, 623, 947, 1183, 2605, 1467, 237, 1438, 2268, 1850, 1797, 54, 1776, 1904, 1192, 2052, 548, 1146, 736, 2077, 2495, 1686, 2409, 1135, 2541, 2442, 2254, 510, 2450, 260, 759, 930, 1519, 1315, 417, 1319, 480, 929, 1840, 1593, 2183, 214, 800, 353, 1564, 1918, 1334, 1225, 1869, 395, 1675, 410, 1552, 701, 2559, 1623, 1516, 2612, 2225, 151, 1841, 193, 1260, 1367, 1846, 2118, 1021, 834, 316, 549, 644, 1520, 2111, 2626, 328, 2253, 2461, 1383, 895, 1057, 24, 2107, 1408, 2688, 150, 2463, 720, 2472, 1540, 1013, 1089, 365, 1558, 1469, 1098, 2699, 1605, 320, 2222, 1138, 2038, 1377, 19, 369, 1661, 2232, 1077, 1942, 883, 250, 686, 2151, 1837, 2176, 808, 107, 1401, 392, 2289, 1289, 1360, 1385, 2301, 2058, 2184, 1374, 28, 431, 184, 1031, 1532, 1619, 1338, 167, 1396, 935, 290, 1947, 2379, 2549, 2065, 1448, 2598, 792, 1805, 304, 2027, 1496, 2086, 197, 285, 238, 894, 2530, 748, 1783, 2644, 961, 2083, 2561, 2285, 2631, 1644, 487, 1291, 668, 1655, 2675, 366, 1301]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 9.3519 - sparse_categorical_accuracy: 0.1920 - val_loss: 3.6476 - val_sparse_categorical_accuracy: 0.2940 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.4359 - sparse_categorical_accuracy: 0.1938 - val_loss: 2.7930 - val_sparse_categorical_accuracy: 0.2970 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.4284 - sparse_categorical_accuracy: 0.2032 - val_loss: 2.6138 - val_sparse_categorical_accuracy: 0.2950 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.2885 - sparse_categorical_accuracy: 0.1985 - val_loss: 2.4347 - val_sparse_categorical_accuracy: 0.2950 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.0337 - sparse_categorical_accuracy: 0.1903 - val_loss: 2.1497 - val_sparse_categorical_accuracy: 0.3100 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.5933 - sparse_categorical_accuracy: 0.2002 - val_loss: 1.9090 - val_sparse_categorical_accuracy: 0.3530 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.8832 - sparse_categorical_accuracy: 0.2330 - val_loss: 1.8130 - val_sparse_categorical_accuracy: 0.3570 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 4.5987 - sparse_categorical_accuracy: 0.2377 - val_loss: 1.9189 - val_sparse_categorical_accuracy: 0.3210 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.3223 - sparse_categorical_accuracy: 0.2629 - val_loss: 1.6816 - val_sparse_categorical_accuracy: 0.3990 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.9812 - sparse_categorical_accuracy: 0.2664 - val_loss: 1.6725 - val_sparse_categorical_accuracy: 0.3790 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.9125 - sparse_categorical_accuracy: 0.2734 - val_loss: 1.5087 - val_sparse_categorical_accuracy: 0.4600 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5636 - sparse_categorical_accuracy: 0.2892 - val_loss: 1.4893 - val_sparse_categorical_accuracy: 0.4470 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 3.4349 - sparse_categorical_accuracy: 0.2945 - val_loss: 1.2961 - val_sparse_categorical_accuracy: 0.5330 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1660 - sparse_categorical_accuracy: 0.3004 - val_loss: 1.2800 - val_sparse_categorical_accuracy: 0.5300 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.9460 - sparse_categorical_accuracy: 0.3296 - val_loss: 1.2540 - val_sparse_categorical_accuracy: 0.5500 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.6797 - sparse_categorical_accuracy: 0.3355 - val_loss: 1.1760 - val_sparse_categorical_accuracy: 0.6120 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5579 - sparse_categorical_accuracy: 0.3460 - val_loss: 1.1069 - val_sparse_categorical_accuracy: 0.5880 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5497 - sparse_categorical_accuracy: 0.3589 - val_loss: 1.0802 - val_sparse_categorical_accuracy: 0.6410 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3468 - sparse_categorical_accuracy: 0.3677 - val_loss: 1.0314 - val_sparse_categorical_accuracy: 0.6120 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3072 - sparse_categorical_accuracy: 0.3730 - val_loss: 1.0308 - val_sparse_categorical_accuracy: 0.6220 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 2.1707 - sparse_categorical_accuracy: 0.3835 - val_loss: 0.9730 - val_sparse_categorical_accuracy: 0.6480 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0718 - sparse_categorical_accuracy: 0.4028 - val_loss: 0.9425 - val_sparse_categorical_accuracy: 0.6570 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9563 - sparse_categorical_accuracy: 0.4116 - val_loss: 0.9055 - val_sparse_categorical_accuracy: 0.6670 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8169 - sparse_categorical_accuracy: 0.4280 - val_loss: 0.8764 - val_sparse_categorical_accuracy: 0.7050 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8123 - sparse_categorical_accuracy: 0.4420 - val_loss: 0.8721 - val_sparse_categorical_accuracy: 0.6800 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7157 - sparse_categorical_accuracy: 0.4532 - val_loss: 0.8480 - val_sparse_categorical_accuracy: 0.7090 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6270 - sparse_categorical_accuracy: 0.4707 - val_loss: 0.8229 - val_sparse_categorical_accuracy: 0.7160 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5389 - sparse_categorical_accuracy: 0.4959 - val_loss: 0.8106 - val_sparse_categorical_accuracy: 0.7210 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4829 - sparse_categorical_accuracy: 0.5082 - val_loss: 0.7932 - val_sparse_categorical_accuracy: 0.7180 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3861 - sparse_categorical_accuracy: 0.5281 - val_loss: 0.7767 - val_sparse_categorical_accuracy: 0.7310 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4006 - sparse_categorical_accuracy: 0.5398 - val_loss: 0.7884 - val_sparse_categorical_accuracy: 0.7130 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3229 - sparse_categorical_accuracy: 0.5451 - val_loss: 0.7625 - val_sparse_categorical_accuracy: 0.7350 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3033 - sparse_categorical_accuracy: 0.5556 - val_loss: 0.7483 - val_sparse_categorical_accuracy: 0.7490 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2332 - sparse_categorical_accuracy: 0.5679 - val_loss: 0.7398 - val_sparse_categorical_accuracy: 0.7480 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1890 - sparse_categorical_accuracy: 0.5749 - val_loss: 0.7160 - val_sparse_categorical_accuracy: 0.7490 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1637 - sparse_categorical_accuracy: 0.5872 - val_loss: 0.7074 - val_sparse_categorical_accuracy: 0.7710 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1371 - sparse_categorical_accuracy: 0.5738 - val_loss: 0.6939 - val_sparse_categorical_accuracy: 0.7720 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1028 - sparse_categorical_accuracy: 0.5913 - val_loss: 0.6829 - val_sparse_categorical_accuracy: 0.7840 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0483 - sparse_categorical_accuracy: 0.6347 - val_loss: 0.6817 - val_sparse_categorical_accuracy: 0.7710 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0666 - sparse_categorical_accuracy: 0.6124 - val_loss: 0.6675 - val_sparse_categorical_accuracy: 0.7800 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0054 - sparse_categorical_accuracy: 0.6253 - val_loss: 0.6600 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9835 - sparse_categorical_accuracy: 0.6388 - val_loss: 0.6477 - val_sparse_categorical_accuracy: 0.7950 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9883 - sparse_categorical_accuracy: 0.6464 - val_loss: 0.6431 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9727 - sparse_categorical_accuracy: 0.6446 - val_loss: 0.6376 - val_sparse_categorical_accuracy: 0.8040 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9394 - sparse_categorical_accuracy: 0.6499 - val_loss: 0.6299 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8983 - sparse_categorical_accuracy: 0.6897 - val_loss: 0.6258 - val_sparse_categorical_accuracy: 0.8010 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8782 - sparse_categorical_accuracy: 0.6727 - val_loss: 0.6237 - val_sparse_categorical_accuracy: 0.8010 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8757 - sparse_categorical_accuracy: 0.6780 - val_loss: 0.6133 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8567 - sparse_categorical_accuracy: 0.6833 - val_loss: 0.6060 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8411 - sparse_categorical_accuracy: 0.6891 - val_loss: 0.6019 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8427 - sparse_categorical_accuracy: 0.6862 - val_loss: 0.5953 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8048 - sparse_categorical_accuracy: 0.7002 - val_loss: 0.5913 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8148 - sparse_categorical_accuracy: 0.6991 - val_loss: 0.5844 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7919 - sparse_categorical_accuracy: 0.7102 - val_loss: 0.5833 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7681 - sparse_categorical_accuracy: 0.7260 - val_loss: 0.5780 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7590 - sparse_categorical_accuracy: 0.7037 - val_loss: 0.5702 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7491 - sparse_categorical_accuracy: 0.7266 - val_loss: 0.5686 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.7254 - sparse_categorical_accuracy: 0.7441 - val_loss: 0.5640 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.7334 - sparse_categorical_accuracy: 0.7319 - val_loss: 0.5570 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7373 - sparse_categorical_accuracy: 0.7231 - val_loss: 0.5562 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7258 - sparse_categorical_accuracy: 0.7219 - val_loss: 0.5492 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7104 - sparse_categorical_accuracy: 0.7488 - val_loss: 0.5418 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7112 - sparse_categorical_accuracy: 0.7307 - val_loss: 0.5396 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7178 - sparse_categorical_accuracy: 0.7330 - val_loss: 0.5397 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6800 - sparse_categorical_accuracy: 0.7488 - val_loss: 0.5329 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6656 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.5286 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6826 - sparse_categorical_accuracy: 0.7465 - val_loss: 0.5254 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6791 - sparse_categorical_accuracy: 0.7424 - val_loss: 0.5342 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6632 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.5200 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6341 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.5173 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6556 - sparse_categorical_accuracy: 0.7518 - val_loss: 0.5187 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6394 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.5142 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6170 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.5075 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6364 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5033 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6140 - sparse_categorical_accuracy: 0.7822 - val_loss: 0.5011 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6165 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.5008 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6422 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.4967 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6028 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.4949 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6123 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.4920 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6327 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.4898 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6287 - sparse_categorical_accuracy: 0.7617 - val_loss: 0.4846 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6109 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.4856 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5806 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4825 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.5901 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.4826 - val_sparse_categorical_accuracy: 0.8440 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5921 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.4783 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5806 - sparse_categorical_accuracy: 0.7787 - val_loss: 0.4769 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5815 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4755 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5809 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.4703 - val_sparse_categorical_accuracy: 0.8450 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5990 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.4716 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5801 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.4710 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5488 - sparse_categorical_accuracy: 0.7986 - val_loss: 0.4738 - val_sparse_categorical_accuracy: 0.8430 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5762 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.4639 - val_sparse_categorical_accuracy: 0.8480 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5721 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.4610 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5674 - sparse_categorical_accuracy: 0.7916 - val_loss: 0.4590 - val_sparse_categorical_accuracy: 0.8510 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5522 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4596 - val_sparse_categorical_accuracy: 0.8450 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5563 - sparse_categorical_accuracy: 0.7916 - val_loss: 0.4562 - val_sparse_categorical_accuracy: 0.8490 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5684 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4535 - val_sparse_categorical_accuracy: 0.8490 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5473 - sparse_categorical_accuracy: 0.7986 - val_loss: 0.4532 - val_sparse_categorical_accuracy: 0.8500 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.5499 - sparse_categorical_accuracy: 0.8015 - val_loss: 0.4553 - val_sparse_categorical_accuracy: 0.8460 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5750 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.4524 - val_sparse_categorical_accuracy: 0.8510 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 7, test #4, 2nd iteration: loss of 0.4589885473251343; sparse_categorical_accuracy of 85.10000109672546%\n",
            "appending basis + selective feature vector data\n",
            "[1929, 2232, 225, 909, 1451, 2707, 1048, 2701, 39, 907, 224, 659, 2675, 2288, 2637, 1544, 73, 804, 1417, 631, 547, 1039, 916, 2336, 1585, 1724, 120, 300, 1872, 1580, 2216, 2456, 2155, 1014, 396, 1551, 1521, 397, 262, 1480, 764, 789, 2300, 208, 1323, 2468, 2062, 643, 796, 1262, 1285, 2059, 2343, 2128, 2425, 2100, 2364, 861, 1113, 1690, 1990, 135, 1924, 2023, 2692, 942, 331, 2625, 1825, 3, 923, 1070, 217, 109, 1987, 2586, 818, 2131, 1173, 1118, 41, 1408, 773, 2212, 1664, 800, 1592, 54, 892, 1944, 782, 594, 2092, 2454, 378, 2367, 2646, 1766, 2687, 450, 2044, 536, 1989, 1941, 1972, 531, 1484, 1058, 330, 2, 1354, 263, 2034, 1236, 2099, 927, 2332, 996, 1100, 260, 1940, 2447, 2349, 1073, 2580, 2371, 857, 116, 670, 299, 1668, 1816, 1116, 2133, 658, 264, 2287, 753, 2662, 9, 2327, 1880, 1057, 1334, 268, 1024, 844, 1810, 2645, 417, 1159, 1887, 231, 2115, 85, 2110, 1893, 1968, 2067, 189, 1775, 971, 835, 2266, 453, 920, 1978, 1557, 421, 2379, 1170, 472, 1299, 2020, 1602, 2071, 2186, 139, 1007, 336, 2444, 46, 2588, 524, 2411, 854, 1289, 1743, 24, 791, 2552, 583, 1111, 134, 1002, 2021, 2699, 283, 334, 380, 2661, 127, 2591, 1397, 1707, 481, 1550, 698, 1121, 1764, 2163, 2121, 291, 2321, 1650, 2198, 922, 2197, 104, 790, 2011, 1635, 398, 2599, 49, 834, 222, 2426, 166, 1997, 672, 1760, 362, 200, 985, 558, 341, 1206, 2421, 140, 1715, 879, 1362, 2245, 1244, 1767, 2463, 661, 115, 2682, 2590, 1099, 510, 1620, 2217, 2521, 2414, 913, 1407, 982, 1235, 2028, 433, 646, 288, 2365, 2533, 1981, 1304, 1470, 1325, 733, 606, 695, 394, 346, 1754, 2272, 1063, 1508, 2688, 2265, 680, 1370, 478, 2286, 1811, 701, 2638, 1172, 2328, 2311, 1992, 739, 725, 853, 69, 2452, 322, 90, 560, 1086, 287, 2010, 1177, 2000, 1894, 215, 413, 205, 2222, 172, 2553, 2169, 887, 1595, 874, 2392, 687, 445, 2639, 2068, 737, 2037, 2460, 429, 585, 2502, 210, 1182, 1824, 364, 1089, 2090, 2359, 1658, 1238, 1319, 2106, 882, 2512, 138, 591, 1644, 664, 2677, 2617, 2135, 1341, 236, 1581, 1790, 1642, 2293, 609, 1316, 1393, 1763, 1360, 1171, 788, 304, 2200, 17, 1911, 2213, 126, 1628, 406, 2539, 60, 2567, 2433, 1485, 2048, 2209, 105, 1846, 269, 657, 1482, 1952, 1797, 22, 1688, 2193, 666, 939, 94, 1657, 1785, 467, 997, 2144, 207, 250, 1257, 1574, 494, 1914, 1270, 415, 2290, 188, 1283, 1986, 141, 367, 2238, 2471, 214, 2451, 707, 1095, 1364, 1963, 1308, 1566, 353, 1955, 885, 1036, 1840, 459, 738, 525, 277, 1903, 1835, 1529, 865, 599, 1091, 2348, 723, 405, 411, 1369, 1813, 777, 1201, 148, 201, 2223, 1502, 1152, 1434, 2150, 1146, 1780, 1326, 902, 545, 755, 2472, 1452, 1792, 846, 1358, 691, 2383, 1190, 940, 1457, 2394, 1009, 637, 1572, 2019, 1844, 2546, 2412, 234, 2029, 2431, 815, 1402, 1998, 540, 387, 557, 1197, 1166, 886, 2636, 592, 1455, 1683, 513, 1020, 2175, 2390, 89, 2526, 2542, 38, 1878, 2203, 1181, 1492, 721, 1167, 2005, 1988, 383, 778, 2652, 1337, 539, 361, 1853, 979, 2130, 1659, 1639, 765, 2061, 86, 759, 2096, 293, 2226, 2347, 641, 193, 1453, 2263, 1460, 529, 1208, 894, 2003, 1004, 382, 516, 964, 2536, 486, 1468, 571, 1318, 18, 357, 55, 1908, 163, 2314, 1848, 697, 2386, 627, 991, 32, 1539, 1420, 2418, 2490, 2070, 1106, 175, 1346, 952, 1411, 528, 2576, 1519, 251, 15, 678, 2303, 1549, 1119, 911, 2615, 2647, 1961, 1178, 2604, 602, 1377, 2382, 1984, 806, 2587, 798, 1094, 821, 78, 1947, 460, 2225, 1904, 482, 2107, 1708, 437, 1645, 458, 1108, 2399, 2579, 491, 1536, 1192, 2656, 1222, 1801, 13, 1897, 7, 2227, 213, 1534, 1456, 650, 1186, 2334, 1112, 1219, 2098, 2503, 1306, 349, 1867, 1507, 1915, 2537, 1392, 1765, 564, 1345, 2244, 414, 180, 376, 1500, 1488, 1242, 1907, 968, 649, 1818, 1588, 1343, 1666, 176, 436, 211, 703, 1827, 410, 2635, 973, 2557, 192, 2424, 1161, 642, 2632, 1098, 1221, 407, 1385, 318, 1053, 709, 469, 988, 31, 183, 371, 780, 1964, 1246, 2199, 829, 2007, 1082, 1349, 1101, 1232, 1906, 595, 2523, 1991, 1430, 2127, 2257, 2095, 233, 1750, 1064, 92, 2356, 1699, 681, 385, 1568, 248, 83, 345, 95, 613, 1720, 831, 1771, 2541, 858, 28, 498, 936, 742, 1438, 10, 1137, 966, 354, 1050, 1905, 2084, 2627, 103, 686, 1748, 702, 1011, 2221, 2295, 2369, 2400, 1901, 2680, 1447, 875, 1355, 2422, 1117, 2497, 1292, 441, 2437, 196, 1735, 904, 493, 1671, 419, 1541, 2289, 1796, 1406, 2195, 2362, 1721, 779, 872, 527, 1237, 914, 648, 2529, 424, 2513, 1351, 128, 1487, 344, 2449, 1218, 1001, 941, 2330, 1475, 1950, 2611, 1884, 2001, 1684, 282, 652, 792, 1522, 2038, 2629, 1034, 618, 719, 243, 2219, 2282, 1597, 1509, 1033, 2211, 2013, 2389, 2703, 1305, 2101, 895, 1662, 91, 422, 447, 1798, 2105, 663, 1233, 1017, 2378, 535, 2049, 1670, 1147, 1015, 944, 159, 1327, 218, 1088, 1976, 1667, 80, 1311, 1295, 2508, 1309, 2047, 864, 1037, 891, 1847, 590, 1569, 29, 1413, 1373, 2500, 1239, 1394, 1400, 848, 2120, 2194, 477, 1561, 1078, 468, 1769, 107, 358, 1231, 216, 647, 1731, 256, 544, 900, 2052, 1102, 1174, 259, 1155, 393, 2243, 2596, 1066, 1314, 492, 1422, 2478, 1255, 581, 2118, 2177, 1938, 1936, 258, 1081, 1352, 958, 2373, 2350, 628, 2455, 630, 1614, 1794, 117, 1134, 2031, 1535, 167, 1012, 2017, 416, 315, 573, 1686, 1436, 2415, 1895, 635, 1258, 1085, 998, 674, 611, 184, 1466, 959, 634, 2117, 232, 2256, 2081, 244, 1860, 1554, 2697, 2388, 125, 292, 1497, 1087, 1646, 1795, 1261, 170, 1788, 2457, 1655, 113, 2409, 1705, 1532, 2439, 730, 2270, 1805, 935, 2165, 1974, 1604, 1396, 1127, 302, 2260, 824, 2157, 1067, 2250, 1531, 1652, 746, 2441, 1934, 897, 1510, 677, 1332, 2484, 1454, 1478, 2361, 2241, 1640, 278, 620, 1203, 1149, 2275, 1441, 2104, 160, 593, 1052, 1610, 2628, 2145, 1489, 1142, 2522, 2146, 2598, 2323, 2240, 2046, 1198, 520, 321, 2413, 66, 1097, 1205, 735, 26, 1245, 290, 1449]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 9.2431 - sparse_categorical_accuracy: 0.1534 - val_loss: 2.7304 - val_sparse_categorical_accuracy: 0.2880 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 7.0076 - sparse_categorical_accuracy: 0.1862 - val_loss: 2.6130 - val_sparse_categorical_accuracy: 0.2950 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.2622 - sparse_categorical_accuracy: 0.2114 - val_loss: 2.3637 - val_sparse_categorical_accuracy: 0.3010 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.8531 - sparse_categorical_accuracy: 0.2231 - val_loss: 2.3606 - val_sparse_categorical_accuracy: 0.3220 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.7346 - sparse_categorical_accuracy: 0.2196 - val_loss: 2.1165 - val_sparse_categorical_accuracy: 0.3380 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.0931 - sparse_categorical_accuracy: 0.2272 - val_loss: 2.0739 - val_sparse_categorical_accuracy: 0.3240 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.7332 - sparse_categorical_accuracy: 0.2553 - val_loss: 1.7618 - val_sparse_categorical_accuracy: 0.3660 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.2741 - sparse_categorical_accuracy: 0.2500 - val_loss: 1.7798 - val_sparse_categorical_accuracy: 0.3490 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.9621 - sparse_categorical_accuracy: 0.2740 - val_loss: 1.7468 - val_sparse_categorical_accuracy: 0.3630 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.7502 - sparse_categorical_accuracy: 0.2799 - val_loss: 1.4997 - val_sparse_categorical_accuracy: 0.4210 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5290 - sparse_categorical_accuracy: 0.2857 - val_loss: 1.5100 - val_sparse_categorical_accuracy: 0.4190 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1220 - sparse_categorical_accuracy: 0.3033 - val_loss: 1.3572 - val_sparse_categorical_accuracy: 0.4780 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.0591 - sparse_categorical_accuracy: 0.2986 - val_loss: 1.2613 - val_sparse_categorical_accuracy: 0.5080 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8225 - sparse_categorical_accuracy: 0.3484 - val_loss: 1.2698 - val_sparse_categorical_accuracy: 0.4800 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5815 - sparse_categorical_accuracy: 0.3530 - val_loss: 1.1718 - val_sparse_categorical_accuracy: 0.5430 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5213 - sparse_categorical_accuracy: 0.3589 - val_loss: 1.1261 - val_sparse_categorical_accuracy: 0.5610 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3219 - sparse_categorical_accuracy: 0.3782 - val_loss: 1.0689 - val_sparse_categorical_accuracy: 0.6080 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2345 - sparse_categorical_accuracy: 0.3841 - val_loss: 1.0105 - val_sparse_categorical_accuracy: 0.6380 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0656 - sparse_categorical_accuracy: 0.4005 - val_loss: 1.0002 - val_sparse_categorical_accuracy: 0.6410 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9612 - sparse_categorical_accuracy: 0.4052 - val_loss: 0.9314 - val_sparse_categorical_accuracy: 0.6770 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7965 - sparse_categorical_accuracy: 0.4379 - val_loss: 0.9318 - val_sparse_categorical_accuracy: 0.6610 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7611 - sparse_categorical_accuracy: 0.4397 - val_loss: 0.8938 - val_sparse_categorical_accuracy: 0.6900 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6723 - sparse_categorical_accuracy: 0.4602 - val_loss: 0.8870 - val_sparse_categorical_accuracy: 0.6800 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5514 - sparse_categorical_accuracy: 0.4783 - val_loss: 0.8363 - val_sparse_categorical_accuracy: 0.7230 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5634 - sparse_categorical_accuracy: 0.4795 - val_loss: 0.8341 - val_sparse_categorical_accuracy: 0.7210 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4933 - sparse_categorical_accuracy: 0.5100 - val_loss: 0.7941 - val_sparse_categorical_accuracy: 0.7390 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3862 - sparse_categorical_accuracy: 0.5269 - val_loss: 0.8061 - val_sparse_categorical_accuracy: 0.7210 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3269 - sparse_categorical_accuracy: 0.5345 - val_loss: 0.7824 - val_sparse_categorical_accuracy: 0.7400 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2589 - sparse_categorical_accuracy: 0.5509 - val_loss: 0.7679 - val_sparse_categorical_accuracy: 0.7490 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2070 - sparse_categorical_accuracy: 0.5732 - val_loss: 0.7502 - val_sparse_categorical_accuracy: 0.7540 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1803 - sparse_categorical_accuracy: 0.5814 - val_loss: 0.7461 - val_sparse_categorical_accuracy: 0.7530 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1590 - sparse_categorical_accuracy: 0.5749 - val_loss: 0.7285 - val_sparse_categorical_accuracy: 0.7690 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0819 - sparse_categorical_accuracy: 0.6136 - val_loss: 0.7249 - val_sparse_categorical_accuracy: 0.7600 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0756 - sparse_categorical_accuracy: 0.6060 - val_loss: 0.7076 - val_sparse_categorical_accuracy: 0.7740 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0494 - sparse_categorical_accuracy: 0.6206 - val_loss: 0.7093 - val_sparse_categorical_accuracy: 0.7730 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0280 - sparse_categorical_accuracy: 0.6370 - val_loss: 0.6881 - val_sparse_categorical_accuracy: 0.7870 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9933 - sparse_categorical_accuracy: 0.6411 - val_loss: 0.6760 - val_sparse_categorical_accuracy: 0.7860 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9876 - sparse_categorical_accuracy: 0.6452 - val_loss: 0.6715 - val_sparse_categorical_accuracy: 0.7880 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9373 - sparse_categorical_accuracy: 0.6598 - val_loss: 0.6593 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9282 - sparse_categorical_accuracy: 0.6692 - val_loss: 0.6545 - val_sparse_categorical_accuracy: 0.7930 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8805 - sparse_categorical_accuracy: 0.6727 - val_loss: 0.6469 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8502 - sparse_categorical_accuracy: 0.6956 - val_loss: 0.6459 - val_sparse_categorical_accuracy: 0.7930 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8824 - sparse_categorical_accuracy: 0.6815 - val_loss: 0.6332 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8530 - sparse_categorical_accuracy: 0.6809 - val_loss: 0.6324 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8504 - sparse_categorical_accuracy: 0.6844 - val_loss: 0.6244 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8500 - sparse_categorical_accuracy: 0.6967 - val_loss: 0.6195 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7958 - sparse_categorical_accuracy: 0.7108 - val_loss: 0.6110 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7954 - sparse_categorical_accuracy: 0.7049 - val_loss: 0.6024 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7877 - sparse_categorical_accuracy: 0.7043 - val_loss: 0.5980 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7729 - sparse_categorical_accuracy: 0.7155 - val_loss: 0.5942 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7426 - sparse_categorical_accuracy: 0.7406 - val_loss: 0.5904 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7583 - sparse_categorical_accuracy: 0.7254 - val_loss: 0.5835 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7382 - sparse_categorical_accuracy: 0.7330 - val_loss: 0.5789 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7371 - sparse_categorical_accuracy: 0.7272 - val_loss: 0.5761 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7111 - sparse_categorical_accuracy: 0.7559 - val_loss: 0.5691 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6998 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.5666 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7016 - sparse_categorical_accuracy: 0.7447 - val_loss: 0.5674 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6896 - sparse_categorical_accuracy: 0.7553 - val_loss: 0.5570 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6881 - sparse_categorical_accuracy: 0.7412 - val_loss: 0.5542 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6601 - sparse_categorical_accuracy: 0.7477 - val_loss: 0.5494 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6648 - sparse_categorical_accuracy: 0.7600 - val_loss: 0.5449 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6623 - sparse_categorical_accuracy: 0.7600 - val_loss: 0.5443 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6610 - sparse_categorical_accuracy: 0.7553 - val_loss: 0.5414 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6542 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5377 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6460 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.5353 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6359 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.5358 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6464 - sparse_categorical_accuracy: 0.7641 - val_loss: 0.5272 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6385 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.5276 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6226 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.5209 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6317 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.5177 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6343 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.5153 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6106 - sparse_categorical_accuracy: 0.7629 - val_loss: 0.5121 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5916 - sparse_categorical_accuracy: 0.7922 - val_loss: 0.5127 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6084 - sparse_categorical_accuracy: 0.7804 - val_loss: 0.5079 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6014 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.5054 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5947 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.5055 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5996 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.5028 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5932 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.4980 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6142 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.4958 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5991 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4945 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5720 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.4888 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5852 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4874 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5761 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.4863 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5950 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.4855 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5718 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.4825 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5722 - sparse_categorical_accuracy: 0.7922 - val_loss: 0.4806 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5603 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.4823 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5641 - sparse_categorical_accuracy: 0.7916 - val_loss: 0.4799 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5314 - sparse_categorical_accuracy: 0.7992 - val_loss: 0.4760 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5448 - sparse_categorical_accuracy: 0.7992 - val_loss: 0.4766 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5630 - sparse_categorical_accuracy: 0.7922 - val_loss: 0.4718 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5655 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.4730 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5550 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.4679 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5506 - sparse_categorical_accuracy: 0.7998 - val_loss: 0.4706 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5521 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.4641 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5439 - sparse_categorical_accuracy: 0.8044 - val_loss: 0.4644 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5472 - sparse_categorical_accuracy: 0.8009 - val_loss: 0.4624 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5226 - sparse_categorical_accuracy: 0.8062 - val_loss: 0.4660 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5439 - sparse_categorical_accuracy: 0.8039 - val_loss: 0.4634 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5472 - sparse_categorical_accuracy: 0.7963 - val_loss: 0.4610 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 8, test #0, 2nd iteration: loss of 0.4644469618797302; sparse_categorical_accuracy of 84.10000205039978%\n",
            "appending basis + selective feature vector data\n",
            "[1929, 2232, 225, 909, 1451, 2707, 1048, 2701, 39, 907, 224, 659, 2675, 2288, 2637, 1544, 73, 804, 1417, 631, 547, 1039, 916, 2336, 1585, 1724, 120, 300, 1872, 1580, 2216, 2456, 2155, 1014, 396, 1551, 1521, 397, 262, 1480, 764, 789, 2300, 208, 1323, 2468, 2062, 643, 796, 1262, 1285, 2059, 2343, 2128, 2425, 2100, 2364, 861, 1113, 1690, 1990, 135, 1924, 2023, 2692, 942, 331, 2625, 1825, 3, 923, 1070, 217, 109, 1987, 2586, 818, 2131, 1173, 1118, 41, 1408, 773, 2212, 1664, 800, 1592, 54, 892, 1944, 782, 594, 2092, 2454, 378, 2367, 2646, 1766, 2687, 450, 2044, 536, 1989, 1941, 1972, 531, 1484, 1058, 330, 2, 1354, 263, 2034, 1236, 2099, 927, 2332, 996, 1100, 260, 1940, 2447, 2349, 1073, 2580, 2371, 857, 116, 670, 299, 1668, 1816, 1116, 2133, 658, 264, 2287, 753, 2662, 9, 2327, 1880, 1057, 1334, 268, 1024, 844, 1810, 2645, 417, 1159, 1887, 231, 2115, 85, 2110, 1893, 1968, 2067, 189, 1775, 971, 835, 2266, 453, 920, 1978, 1557, 421, 2379, 1170, 472, 1299, 2020, 1602, 2071, 2186, 139, 1007, 336, 2444, 46, 2588, 524, 2411, 854, 1289, 1743, 24, 791, 2552, 583, 1111, 134, 1002, 2021, 2699, 283, 334, 380, 2661, 127, 2591, 1397, 1707, 481, 1550, 698, 1121, 1764, 2163, 2121, 291, 2321, 1650, 2198, 922, 2197, 104, 790, 2011, 1635, 398, 2599, 49, 834, 222, 2426, 166, 1997, 672, 1760, 362, 200, 985, 558, 341, 1206, 2421, 140, 1715, 879, 1362, 2245, 1244, 1767, 2463, 661, 115, 2682, 2590, 1099, 510, 1620, 2217, 2521, 2414, 913, 1407, 982, 1235, 2028, 433, 646, 288, 2365, 2533, 1981, 1304, 1470, 1325, 733, 606, 695, 394, 346, 1754, 2272, 1063, 1508, 2688, 2265, 680, 1370, 478, 2286, 1811, 701, 2638, 1172, 2328, 2311, 1992, 739, 725, 853, 69, 2452, 322, 90, 560, 1086, 287, 2010, 1177, 2000, 1894, 215, 413, 205, 2222, 172, 2553, 2169, 887, 1595, 874, 2392, 687, 445, 2639, 2068, 737, 2037, 2460, 429, 585, 2502, 210, 1182, 1824, 364, 1089, 2090, 2359, 1658, 1238, 1319, 2106, 882, 2512, 138, 591, 1644, 664, 2677, 2617, 2135, 1341, 236, 1581, 1790, 1642, 2293, 609, 1316, 1393, 1763, 1360, 1171, 788, 304, 2200, 17, 1911, 2213, 126, 1628, 406, 2539, 60, 2567, 2433, 1485, 2048, 2209, 105, 1846, 269, 657, 1482, 1952, 1797, 22, 1688, 2193, 666, 939, 94, 1657, 1785, 467, 997, 2144, 207, 250, 1257, 1574, 494, 1914, 1270, 415, 2290, 188, 1283, 1986, 141, 367, 2238, 2471, 214, 2451, 707, 1095, 1364, 1963, 1308, 1566, 353, 1955, 885, 1036, 1840, 459, 738, 525, 277, 1903, 1835, 1529, 865, 599, 1091, 2348, 723, 405, 411, 1369, 1813, 777, 1201, 148, 201, 2223, 1502, 1152, 1434, 2150, 1146, 1780, 1326, 902, 545, 755, 2472, 1452, 1792, 846, 1358, 691, 2383, 1190, 940, 1457, 2394, 1009, 637, 1572, 2019, 1844, 2546, 2412, 234, 2029, 2431, 815, 1402, 1998, 540, 387, 557, 1197, 1166, 886, 2636, 592, 1455, 1683, 513, 1020, 2175, 2390, 89, 2526, 2542, 38, 1878, 2203, 1181, 1492, 721, 1167, 2005, 1988, 383, 778, 2652, 1337, 539, 361, 1853, 979, 2130, 1659, 1639, 765, 2061, 86, 759, 2096, 293, 2226, 2347, 641, 193, 1453, 2263, 1460, 529, 1208, 894, 2003, 1004, 382, 516, 964, 2536, 486, 1468, 571, 1318, 18, 357, 55, 1908, 163, 2314, 1848, 697, 2386, 627, 991, 32, 1539, 1420, 2418, 2490, 2070, 1106, 175, 1346, 952, 1411, 528, 2576, 1519, 251, 15, 678, 2303, 1549, 1119, 911, 2615, 2647, 1961, 1178, 2604, 602, 1377, 2382, 1984, 806, 2587, 798, 1094, 821, 78, 1947, 460, 2225, 1904, 482, 2107, 1708, 437, 1645, 458, 1108, 2399, 2579, 491, 1536, 1192, 2656, 1222, 1801, 13, 1897, 7, 2227, 213, 1534, 1456, 650, 1186, 2334, 1112, 1219, 2098, 2503, 1306, 349, 1867, 1507, 1915, 2537, 1392, 1765, 564, 1345, 2244, 414, 180, 376, 1500, 1488, 1242, 1907, 968, 649, 1818, 1588, 1343, 1666, 176, 436, 211, 703, 1827, 410, 2635, 973, 2557, 192, 2424, 1161, 642, 2632, 1098, 1221, 407, 1385, 318, 1053, 709, 469, 988, 31, 183, 371, 780, 1964, 1246, 2199, 829, 2007, 1082, 1349, 1101, 1232, 1906, 595, 2523, 1991, 1430, 2127, 2257, 2095, 233, 1750, 1064, 92, 2356, 1699, 681, 385, 1568, 248, 83, 345, 95, 613, 1720, 831, 1771, 2541, 858, 28, 498, 936, 742, 1438, 10, 1137, 966, 354, 1050, 1905, 2084, 2627, 103, 686, 1748, 702, 1011, 2221, 2295, 2369, 2400, 1901, 2680, 1447, 875, 1355, 2422, 1117, 2497, 1292, 441, 2437, 196, 1735, 904, 493, 1671, 419, 1541, 2289, 1796, 1406, 2195, 2362, 1721, 779, 872, 527, 1237, 914, 648, 2529, 424, 2513, 1351, 128, 1487, 344, 2449, 1218, 1001, 941, 2330, 1475, 1950, 2611, 1884, 2001, 1684, 282, 652, 792, 1522, 2038, 2629, 1034, 618, 719, 243, 2219, 2282, 1597, 1509, 1033, 2211, 2013, 2389, 2703, 1305, 2101, 895, 1662, 91, 422, 447, 1798, 2105, 663, 1233, 1017, 2378, 535, 2049, 1670, 1147, 1015, 944, 159, 1327, 218, 1088, 1976, 1667, 80, 1311, 1295, 2508, 1309, 2047, 864, 1037, 891, 1847, 590, 1569, 29, 1413, 1373, 2500, 1239, 1394, 1400, 848, 2120, 2194, 477, 1561, 1078, 468, 1769, 107, 358, 1231, 216, 647, 1731, 256, 544, 900, 2052, 1102, 1174, 259, 1155, 393, 2243, 2596, 1066, 1314, 492, 1422, 2478, 1255, 581, 2118, 2177, 1938, 1936, 258, 1081, 1352, 958, 2373, 2350, 628, 2455, 630, 1614, 1794, 117, 1134, 2031, 1535, 167, 1012, 2017, 416, 315, 573, 1686, 1436, 2415, 1895, 635, 1258, 1085, 998, 674, 611, 184, 1466, 959, 634, 2117, 232, 2256, 2081, 244, 1860, 1554, 2697, 2388, 125, 292, 1497, 1087, 1646, 1795, 1261, 170, 1788, 2457, 1655, 113, 2409, 1705, 1532, 2439, 730, 2270, 1805, 935, 2165, 1974, 1604, 1396, 1127, 302, 2260, 824, 2157, 1067, 2250, 1531, 1652, 746, 2441, 1934, 897, 1510, 677, 1332, 2484, 1454, 1478, 2361, 2241, 1640, 278, 620, 1203, 1149, 2275, 1441, 2104, 160, 593, 1052, 1610, 2628, 2145, 1489, 1142, 2522, 2146, 2598, 2323, 2240, 2046, 1198, 520, 321, 2413, 66, 1097, 1205, 735, 26, 1245, 290, 1449]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 10.5245 - sparse_categorical_accuracy: 0.1464 - val_loss: 3.6748 - val_sparse_categorical_accuracy: 0.2450 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.7651 - sparse_categorical_accuracy: 0.1868 - val_loss: 3.1062 - val_sparse_categorical_accuracy: 0.2780 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.3435 - sparse_categorical_accuracy: 0.1809 - val_loss: 2.5054 - val_sparse_categorical_accuracy: 0.2770 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.9410 - sparse_categorical_accuracy: 0.1973 - val_loss: 2.5229 - val_sparse_categorical_accuracy: 0.2920 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.3293 - sparse_categorical_accuracy: 0.2108 - val_loss: 2.0728 - val_sparse_categorical_accuracy: 0.2970 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.8584 - sparse_categorical_accuracy: 0.2289 - val_loss: 2.0630 - val_sparse_categorical_accuracy: 0.3170 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.6062 - sparse_categorical_accuracy: 0.2137 - val_loss: 1.7635 - val_sparse_categorical_accuracy: 0.3500 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.1635 - sparse_categorical_accuracy: 0.2354 - val_loss: 1.6896 - val_sparse_categorical_accuracy: 0.3610 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.7423 - sparse_categorical_accuracy: 0.2529 - val_loss: 1.5337 - val_sparse_categorical_accuracy: 0.4150 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5074 - sparse_categorical_accuracy: 0.2740 - val_loss: 1.5123 - val_sparse_categorical_accuracy: 0.4110 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.2214 - sparse_categorical_accuracy: 0.2910 - val_loss: 1.4515 - val_sparse_categorical_accuracy: 0.4120 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.9810 - sparse_categorical_accuracy: 0.3115 - val_loss: 1.2821 - val_sparse_categorical_accuracy: 0.5120 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8330 - sparse_categorical_accuracy: 0.3296 - val_loss: 1.2973 - val_sparse_categorical_accuracy: 0.4770 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.6819 - sparse_categorical_accuracy: 0.3349 - val_loss: 1.1795 - val_sparse_categorical_accuracy: 0.5590 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4145 - sparse_categorical_accuracy: 0.3577 - val_loss: 1.1369 - val_sparse_categorical_accuracy: 0.5740 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2052 - sparse_categorical_accuracy: 0.3981 - val_loss: 1.0716 - val_sparse_categorical_accuracy: 0.5930 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0587 - sparse_categorical_accuracy: 0.3970 - val_loss: 1.0052 - val_sparse_categorical_accuracy: 0.6410 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9368 - sparse_categorical_accuracy: 0.4227 - val_loss: 0.9913 - val_sparse_categorical_accuracy: 0.6350 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8558 - sparse_categorical_accuracy: 0.4379 - val_loss: 0.9571 - val_sparse_categorical_accuracy: 0.6440 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7108 - sparse_categorical_accuracy: 0.4549 - val_loss: 0.9631 - val_sparse_categorical_accuracy: 0.6440 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6623 - sparse_categorical_accuracy: 0.4660 - val_loss: 0.9015 - val_sparse_categorical_accuracy: 0.6770 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5591 - sparse_categorical_accuracy: 0.4865 - val_loss: 0.8737 - val_sparse_categorical_accuracy: 0.6880 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5172 - sparse_categorical_accuracy: 0.4941 - val_loss: 0.8388 - val_sparse_categorical_accuracy: 0.7160 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4536 - sparse_categorical_accuracy: 0.5059 - val_loss: 0.8193 - val_sparse_categorical_accuracy: 0.7160 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3797 - sparse_categorical_accuracy: 0.5211 - val_loss: 0.8122 - val_sparse_categorical_accuracy: 0.7090 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3428 - sparse_categorical_accuracy: 0.5252 - val_loss: 0.7980 - val_sparse_categorical_accuracy: 0.7220 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3002 - sparse_categorical_accuracy: 0.5427 - val_loss: 0.7844 - val_sparse_categorical_accuracy: 0.7240 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2142 - sparse_categorical_accuracy: 0.5638 - val_loss: 0.7807 - val_sparse_categorical_accuracy: 0.7310 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1803 - sparse_categorical_accuracy: 0.5779 - val_loss: 0.7574 - val_sparse_categorical_accuracy: 0.7370 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1608 - sparse_categorical_accuracy: 0.5802 - val_loss: 0.7429 - val_sparse_categorical_accuracy: 0.7500 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0706 - sparse_categorical_accuracy: 0.6136 - val_loss: 0.7322 - val_sparse_categorical_accuracy: 0.7500 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0713 - sparse_categorical_accuracy: 0.6048 - val_loss: 0.7130 - val_sparse_categorical_accuracy: 0.7560 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0338 - sparse_categorical_accuracy: 0.6276 - val_loss: 0.7181 - val_sparse_categorical_accuracy: 0.7590 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0364 - sparse_categorical_accuracy: 0.6241 - val_loss: 0.6974 - val_sparse_categorical_accuracy: 0.7630 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9635 - sparse_categorical_accuracy: 0.6528 - val_loss: 0.6916 - val_sparse_categorical_accuracy: 0.7670 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9440 - sparse_categorical_accuracy: 0.6499 - val_loss: 0.6865 - val_sparse_categorical_accuracy: 0.7670 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9283 - sparse_categorical_accuracy: 0.6622 - val_loss: 0.6719 - val_sparse_categorical_accuracy: 0.7720 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9058 - sparse_categorical_accuracy: 0.6633 - val_loss: 0.6671 - val_sparse_categorical_accuracy: 0.7780 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9172 - sparse_categorical_accuracy: 0.6552 - val_loss: 0.6600 - val_sparse_categorical_accuracy: 0.7800 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8590 - sparse_categorical_accuracy: 0.6715 - val_loss: 0.6523 - val_sparse_categorical_accuracy: 0.7700 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8446 - sparse_categorical_accuracy: 0.6909 - val_loss: 0.6390 - val_sparse_categorical_accuracy: 0.7840 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8533 - sparse_categorical_accuracy: 0.6850 - val_loss: 0.6324 - val_sparse_categorical_accuracy: 0.7850 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8436 - sparse_categorical_accuracy: 0.6991 - val_loss: 0.6275 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7943 - sparse_categorical_accuracy: 0.6979 - val_loss: 0.6275 - val_sparse_categorical_accuracy: 0.7840 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8096 - sparse_categorical_accuracy: 0.7055 - val_loss: 0.6123 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7921 - sparse_categorical_accuracy: 0.7055 - val_loss: 0.6099 - val_sparse_categorical_accuracy: 0.7930 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7669 - sparse_categorical_accuracy: 0.7166 - val_loss: 0.6052 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7402 - sparse_categorical_accuracy: 0.7301 - val_loss: 0.6051 - val_sparse_categorical_accuracy: 0.7870 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7605 - sparse_categorical_accuracy: 0.7219 - val_loss: 0.6003 - val_sparse_categorical_accuracy: 0.7920 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7373 - sparse_categorical_accuracy: 0.7301 - val_loss: 0.5878 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7481 - sparse_categorical_accuracy: 0.7354 - val_loss: 0.5869 - val_sparse_categorical_accuracy: 0.7950 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7371 - sparse_categorical_accuracy: 0.7330 - val_loss: 0.5797 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7341 - sparse_categorical_accuracy: 0.7365 - val_loss: 0.5741 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7243 - sparse_categorical_accuracy: 0.7324 - val_loss: 0.5714 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6862 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5672 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6822 - sparse_categorical_accuracy: 0.7453 - val_loss: 0.5648 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6779 - sparse_categorical_accuracy: 0.7465 - val_loss: 0.5597 - val_sparse_categorical_accuracy: 0.8040 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6953 - sparse_categorical_accuracy: 0.7383 - val_loss: 0.5563 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6792 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.5511 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6774 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.5478 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6463 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.5437 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6754 - sparse_categorical_accuracy: 0.7529 - val_loss: 0.5422 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6382 - sparse_categorical_accuracy: 0.7635 - val_loss: 0.5456 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6446 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.5356 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6406 - sparse_categorical_accuracy: 0.7559 - val_loss: 0.5336 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6346 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.5281 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6120 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.5280 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6150 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.5219 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6160 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.5226 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6152 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.5178 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6211 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.5148 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6147 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.5125 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6197 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.5064 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5994 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.5111 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5966 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.5028 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5825 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.5041 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6005 - sparse_categorical_accuracy: 0.7728 - val_loss: 0.4976 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5912 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.4953 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5869 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4960 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5688 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.4933 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5799 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4884 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5879 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4860 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5816 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4857 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5649 - sparse_categorical_accuracy: 0.8009 - val_loss: 0.4833 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5720 - sparse_categorical_accuracy: 0.7892 - val_loss: 0.4805 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5690 - sparse_categorical_accuracy: 0.7998 - val_loss: 0.4840 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5670 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.4812 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5605 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.4810 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            " 85/107 [======================>.......] - ETA: 0s - loss: 0.5696 - sparse_categorical_accuracy: 0.7897\n",
            "Epoch 89: ReduceLROnPlateau reducing learning rate to 1.4999999621068127e-05.\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5714 - sparse_categorical_accuracy: 0.7910 - val_loss: 0.4809 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5597 - sparse_categorical_accuracy: 0.7986 - val_loss: 0.4754 - val_sparse_categorical_accuracy: 0.8400 - lr: 1.5000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5684 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4745 - val_sparse_categorical_accuracy: 0.8420 - lr: 1.5000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5650 - sparse_categorical_accuracy: 0.7910 - val_loss: 0.4733 - val_sparse_categorical_accuracy: 0.8380 - lr: 1.5000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5599 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4735 - val_sparse_categorical_accuracy: 0.8370 - lr: 1.5000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5504 - sparse_categorical_accuracy: 0.7974 - val_loss: 0.4728 - val_sparse_categorical_accuracy: 0.8420 - lr: 1.5000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5535 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4707 - val_sparse_categorical_accuracy: 0.8430 - lr: 1.5000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5540 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.4720 - val_sparse_categorical_accuracy: 0.8400 - lr: 1.5000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5566 - sparse_categorical_accuracy: 0.7957 - val_loss: 0.4713 - val_sparse_categorical_accuracy: 0.8420 - lr: 1.5000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5374 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.4692 - val_sparse_categorical_accuracy: 0.8390 - lr: 1.5000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5590 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4703 - val_sparse_categorical_accuracy: 0.8410 - lr: 1.5000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5375 - sparse_categorical_accuracy: 0.8050 - val_loss: 0.4695 - val_sparse_categorical_accuracy: 0.8400 - lr: 1.5000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 8, test #1, 2nd iteration: loss of 0.4706781208515167; sparse_categorical_accuracy of 84.29999947547913%\n",
            "appending basis + selective feature vector data\n",
            "[1929, 2232, 225, 909, 1451, 2707, 1048, 2701, 39, 907, 224, 659, 2675, 2288, 2637, 1544, 73, 804, 1417, 631, 547, 1039, 916, 2336, 1585, 1724, 120, 300, 1872, 1580, 2216, 2456, 2155, 1014, 396, 1551, 1521, 397, 262, 1480, 764, 789, 2300, 208, 1323, 2468, 2062, 643, 796, 1262, 1285, 2059, 2343, 2128, 2425, 2100, 2364, 861, 1113, 1690, 1990, 135, 1924, 2023, 2692, 942, 331, 2625, 1825, 3, 923, 1070, 217, 109, 1987, 2586, 818, 2131, 1173, 1118, 41, 1408, 773, 2212, 1664, 800, 1592, 54, 892, 1944, 782, 594, 2092, 2454, 378, 2367, 2646, 1766, 2687, 450, 2044, 536, 1989, 1941, 1972, 531, 1484, 1058, 330, 2, 1354, 263, 2034, 1236, 2099, 927, 2332, 996, 1100, 260, 1940, 2447, 2349, 1073, 2580, 2371, 857, 116, 670, 299, 1668, 1816, 1116, 2133, 658, 264, 2287, 753, 2662, 9, 2327, 1880, 1057, 1334, 268, 1024, 844, 1810, 2645, 417, 1159, 1887, 231, 2115, 85, 2110, 1893, 1968, 2067, 189, 1775, 971, 835, 2266, 453, 920, 1978, 1557, 421, 2379, 1170, 472, 1299, 2020, 1602, 2071, 2186, 139, 1007, 336, 2444, 46, 2588, 524, 2411, 854, 1289, 1743, 24, 791, 2552, 583, 1111, 134, 1002, 2021, 2699, 283, 334, 380, 2661, 127, 2591, 1397, 1707, 481, 1550, 698, 1121, 1764, 2163, 2121, 291, 2321, 1650, 2198, 922, 2197, 104, 790, 2011, 1635, 398, 2599, 49, 834, 222, 2426, 166, 1997, 672, 1760, 362, 200, 985, 558, 341, 1206, 2421, 140, 1715, 879, 1362, 2245, 1244, 1767, 2463, 661, 115, 2682, 2590, 1099, 510, 1620, 2217, 2521, 2414, 913, 1407, 982, 1235, 2028, 433, 646, 288, 2365, 2533, 1981, 1304, 1470, 1325, 733, 606, 695, 394, 346, 1754, 2272, 1063, 1508, 2688, 2265, 680, 1370, 478, 2286, 1811, 701, 2638, 1172, 2328, 2311, 1992, 739, 725, 853, 69, 2452, 322, 90, 560, 1086, 287, 2010, 1177, 2000, 1894, 215, 413, 205, 2222, 172, 2553, 2169, 887, 1595, 874, 2392, 687, 445, 2639, 2068, 737, 2037, 2460, 429, 585, 2502, 210, 1182, 1824, 364, 1089, 2090, 2359, 1658, 1238, 1319, 2106, 882, 2512, 138, 591, 1644, 664, 2677, 2617, 2135, 1341, 236, 1581, 1790, 1642, 2293, 609, 1316, 1393, 1763, 1360, 1171, 788, 304, 2200, 17, 1911, 2213, 126, 1628, 406, 2539, 60, 2567, 2433, 1485, 2048, 2209, 105, 1846, 269, 657, 1482, 1952, 1797, 22, 1688, 2193, 666, 939, 94, 1657, 1785, 467, 997, 2144, 207, 250, 1257, 1574, 494, 1914, 1270, 415, 2290, 188, 1283, 1986, 141, 367, 2238, 2471, 214, 2451, 707, 1095, 1364, 1963, 1308, 1566, 353, 1955, 885, 1036, 1840, 459, 738, 525, 277, 1903, 1835, 1529, 865, 599, 1091, 2348, 723, 405, 411, 1369, 1813, 777, 1201, 148, 201, 2223, 1502, 1152, 1434, 2150, 1146, 1780, 1326, 902, 545, 755, 2472, 1452, 1792, 846, 1358, 691, 2383, 1190, 940, 1457, 2394, 1009, 637, 1572, 2019, 1844, 2546, 2412, 234, 2029, 2431, 815, 1402, 1998, 540, 387, 557, 1197, 1166, 886, 2636, 592, 1455, 1683, 513, 1020, 2175, 2390, 89, 2526, 2542, 38, 1878, 2203, 1181, 1492, 721, 1167, 2005, 1988, 383, 778, 2652, 1337, 539, 361, 1853, 979, 2130, 1659, 1639, 765, 2061, 86, 759, 2096, 293, 2226, 2347, 641, 193, 1453, 2263, 1460, 529, 1208, 894, 2003, 1004, 382, 516, 964, 2536, 486, 1468, 571, 1318, 18, 357, 55, 1908, 163, 2314, 1848, 697, 2386, 627, 991, 32, 1539, 1420, 2418, 2490, 2070, 1106, 175, 1346, 952, 1411, 528, 2576, 1519, 251, 15, 678, 2303, 1549, 1119, 911, 2615, 2647, 1961, 1178, 2604, 602, 1377, 2382, 1984, 806, 2587, 798, 1094, 821, 78, 1947, 460, 2225, 1904, 482, 2107, 1708, 437, 1645, 458, 1108, 2399, 2579, 491, 1536, 1192, 2656, 1222, 1801, 13, 1897, 7, 2227, 213, 1534, 1456, 650, 1186, 2334, 1112, 1219, 2098, 2503, 1306, 349, 1867, 1507, 1915, 2537, 1392, 1765, 564, 1345, 2244, 414, 180, 376, 1500, 1488, 1242, 1907, 968, 649, 1818, 1588, 1343, 1666, 176, 436, 211, 703, 1827, 410, 2635, 973, 2557, 192, 2424, 1161, 642, 2632, 1098, 1221, 407, 1385, 318, 1053, 709, 469, 988, 31, 183, 371, 780, 1964, 1246, 2199, 829, 2007, 1082, 1349, 1101, 1232, 1906, 595, 2523, 1991, 1430, 2127, 2257, 2095, 233, 1750, 1064, 92, 2356, 1699, 681, 385, 1568, 248, 83, 345, 95, 613, 1720, 831, 1771, 2541, 858, 28, 498, 936, 742, 1438, 10, 1137, 966, 354, 1050, 1905, 2084, 2627, 103, 686, 1748, 702, 1011, 2221, 2295, 2369, 2400, 1901, 2680, 1447, 875, 1355, 2422, 1117, 2497, 1292, 441, 2437, 196, 1735, 904, 493, 1671, 419, 1541, 2289, 1796, 1406, 2195, 2362, 1721, 779, 872, 527, 1237, 914, 648, 2529, 424, 2513, 1351, 128, 1487, 344, 2449, 1218, 1001, 941, 2330, 1475, 1950, 2611, 1884, 2001, 1684, 282, 652, 792, 1522, 2038, 2629, 1034, 618, 719, 243, 2219, 2282, 1597, 1509, 1033, 2211, 2013, 2389, 2703, 1305, 2101, 895, 1662, 91, 422, 447, 1798, 2105, 663, 1233, 1017, 2378, 535, 2049, 1670, 1147, 1015, 944, 159, 1327, 218, 1088, 1976, 1667, 80, 1311, 1295, 2508, 1309, 2047, 864, 1037, 891, 1847, 590, 1569, 29, 1413, 1373, 2500, 1239, 1394, 1400, 848, 2120, 2194, 477, 1561, 1078, 468, 1769, 107, 358, 1231, 216, 647, 1731, 256, 544, 900, 2052, 1102, 1174, 259, 1155, 393, 2243, 2596, 1066, 1314, 492, 1422, 2478, 1255, 581, 2118, 2177, 1938, 1936, 258, 1081, 1352, 958, 2373, 2350, 628, 2455, 630, 1614, 1794, 117, 1134, 2031, 1535, 167, 1012, 2017, 416, 315, 573, 1686, 1436, 2415, 1895, 635, 1258, 1085, 998, 674, 611, 184, 1466, 959, 634, 2117, 232, 2256, 2081, 244, 1860, 1554, 2697, 2388, 125, 292, 1497, 1087, 1646, 1795, 1261, 170, 1788, 2457, 1655, 113, 2409, 1705, 1532, 2439, 730, 2270, 1805, 935, 2165, 1974, 1604, 1396, 1127, 302, 2260, 824, 2157, 1067, 2250, 1531, 1652, 746, 2441, 1934, 897, 1510, 677, 1332, 2484, 1454, 1478, 2361, 2241, 1640, 278, 620, 1203, 1149, 2275, 1441, 2104, 160, 593, 1052, 1610, 2628, 2145, 1489, 1142, 2522, 2146, 2598, 2323, 2240, 2046, 1198, 520, 321, 2413, 66, 1097, 1205, 735, 26, 1245, 290, 1449]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 12.0963 - sparse_categorical_accuracy: 0.1552 - val_loss: 4.0560 - val_sparse_categorical_accuracy: 0.2890 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 8.9082 - sparse_categorical_accuracy: 0.1856 - val_loss: 3.4385 - val_sparse_categorical_accuracy: 0.2920 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 8.2938 - sparse_categorical_accuracy: 0.1850 - val_loss: 2.8063 - val_sparse_categorical_accuracy: 0.2950 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 8.0194 - sparse_categorical_accuracy: 0.1856 - val_loss: 3.1924 - val_sparse_categorical_accuracy: 0.2940 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.1190 - sparse_categorical_accuracy: 0.1950 - val_loss: 2.7649 - val_sparse_categorical_accuracy: 0.2940 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.9070 - sparse_categorical_accuracy: 0.2002 - val_loss: 2.7500 - val_sparse_categorical_accuracy: 0.2940 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 6.3338 - sparse_categorical_accuracy: 0.2073 - val_loss: 2.0772 - val_sparse_categorical_accuracy: 0.3420 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.6918 - sparse_categorical_accuracy: 0.2196 - val_loss: 2.3889 - val_sparse_categorical_accuracy: 0.3080 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.3431 - sparse_categorical_accuracy: 0.2336 - val_loss: 1.9896 - val_sparse_categorical_accuracy: 0.3400 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.2442 - sparse_categorical_accuracy: 0.2207 - val_loss: 1.9481 - val_sparse_categorical_accuracy: 0.3230 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.8063 - sparse_categorical_accuracy: 0.2465 - val_loss: 1.7539 - val_sparse_categorical_accuracy: 0.3700 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.5928 - sparse_categorical_accuracy: 0.2330 - val_loss: 1.6820 - val_sparse_categorical_accuracy: 0.3610 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.3794 - sparse_categorical_accuracy: 0.2348 - val_loss: 1.5818 - val_sparse_categorical_accuracy: 0.3710 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.0222 - sparse_categorical_accuracy: 0.2529 - val_loss: 1.5710 - val_sparse_categorical_accuracy: 0.3950 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.8030 - sparse_categorical_accuracy: 0.2869 - val_loss: 1.4654 - val_sparse_categorical_accuracy: 0.4130 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5241 - sparse_categorical_accuracy: 0.2635 - val_loss: 1.3197 - val_sparse_categorical_accuracy: 0.4710 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.2908 - sparse_categorical_accuracy: 0.3021 - val_loss: 1.3993 - val_sparse_categorical_accuracy: 0.4550 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1668 - sparse_categorical_accuracy: 0.2939 - val_loss: 1.1919 - val_sparse_categorical_accuracy: 0.5100 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8982 - sparse_categorical_accuracy: 0.3191 - val_loss: 1.2060 - val_sparse_categorical_accuracy: 0.5180 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.7534 - sparse_categorical_accuracy: 0.3285 - val_loss: 1.1974 - val_sparse_categorical_accuracy: 0.5130 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 2.6518 - sparse_categorical_accuracy: 0.3326 - val_loss: 1.0858 - val_sparse_categorical_accuracy: 0.5720 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4790 - sparse_categorical_accuracy: 0.3747 - val_loss: 1.1023 - val_sparse_categorical_accuracy: 0.5440 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4777 - sparse_categorical_accuracy: 0.3454 - val_loss: 1.0364 - val_sparse_categorical_accuracy: 0.6070 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3298 - sparse_categorical_accuracy: 0.3636 - val_loss: 1.0326 - val_sparse_categorical_accuracy: 0.5770 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1882 - sparse_categorical_accuracy: 0.3724 - val_loss: 1.0127 - val_sparse_categorical_accuracy: 0.5830 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1043 - sparse_categorical_accuracy: 0.3952 - val_loss: 0.9952 - val_sparse_categorical_accuracy: 0.5940 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0153 - sparse_categorical_accuracy: 0.3876 - val_loss: 0.9367 - val_sparse_categorical_accuracy: 0.6560 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8850 - sparse_categorical_accuracy: 0.4292 - val_loss: 0.9211 - val_sparse_categorical_accuracy: 0.6640 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8856 - sparse_categorical_accuracy: 0.4093 - val_loss: 0.8766 - val_sparse_categorical_accuracy: 0.7170 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7464 - sparse_categorical_accuracy: 0.4502 - val_loss: 0.9177 - val_sparse_categorical_accuracy: 0.6340 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6310 - sparse_categorical_accuracy: 0.4543 - val_loss: 0.8789 - val_sparse_categorical_accuracy: 0.6790 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6673 - sparse_categorical_accuracy: 0.4666 - val_loss: 0.8628 - val_sparse_categorical_accuracy: 0.6950 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5971 - sparse_categorical_accuracy: 0.4719 - val_loss: 0.8198 - val_sparse_categorical_accuracy: 0.7220 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5371 - sparse_categorical_accuracy: 0.4760 - val_loss: 0.8244 - val_sparse_categorical_accuracy: 0.7130 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4444 - sparse_categorical_accuracy: 0.5105 - val_loss: 0.8022 - val_sparse_categorical_accuracy: 0.7410 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3648 - sparse_categorical_accuracy: 0.5105 - val_loss: 0.8011 - val_sparse_categorical_accuracy: 0.7320 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3999 - sparse_categorical_accuracy: 0.5064 - val_loss: 0.7738 - val_sparse_categorical_accuracy: 0.7520 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3390 - sparse_categorical_accuracy: 0.5527 - val_loss: 0.7759 - val_sparse_categorical_accuracy: 0.7470 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2910 - sparse_categorical_accuracy: 0.5404 - val_loss: 0.7554 - val_sparse_categorical_accuracy: 0.7530 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2666 - sparse_categorical_accuracy: 0.5410 - val_loss: 0.7571 - val_sparse_categorical_accuracy: 0.7640 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2569 - sparse_categorical_accuracy: 0.5433 - val_loss: 0.7351 - val_sparse_categorical_accuracy: 0.7580 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1735 - sparse_categorical_accuracy: 0.5855 - val_loss: 0.7470 - val_sparse_categorical_accuracy: 0.7560 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1911 - sparse_categorical_accuracy: 0.5615 - val_loss: 0.7198 - val_sparse_categorical_accuracy: 0.7700 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1277 - sparse_categorical_accuracy: 0.5796 - val_loss: 0.7172 - val_sparse_categorical_accuracy: 0.7750 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1108 - sparse_categorical_accuracy: 0.5972 - val_loss: 0.7052 - val_sparse_categorical_accuracy: 0.7710 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0672 - sparse_categorical_accuracy: 0.6042 - val_loss: 0.6970 - val_sparse_categorical_accuracy: 0.7740 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0170 - sparse_categorical_accuracy: 0.6364 - val_loss: 0.6908 - val_sparse_categorical_accuracy: 0.7750 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0412 - sparse_categorical_accuracy: 0.6118 - val_loss: 0.6899 - val_sparse_categorical_accuracy: 0.7720 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9892 - sparse_categorical_accuracy: 0.6294 - val_loss: 0.6830 - val_sparse_categorical_accuracy: 0.7810 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9799 - sparse_categorical_accuracy: 0.6259 - val_loss: 0.6736 - val_sparse_categorical_accuracy: 0.7790 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9505 - sparse_categorical_accuracy: 0.6364 - val_loss: 0.6646 - val_sparse_categorical_accuracy: 0.7790 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9085 - sparse_categorical_accuracy: 0.6563 - val_loss: 0.6546 - val_sparse_categorical_accuracy: 0.7890 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9474 - sparse_categorical_accuracy: 0.6481 - val_loss: 0.6507 - val_sparse_categorical_accuracy: 0.7850 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.8902 - sparse_categorical_accuracy: 0.6628 - val_loss: 0.6445 - val_sparse_categorical_accuracy: 0.7820 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9109 - sparse_categorical_accuracy: 0.6575 - val_loss: 0.6377 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8648 - sparse_categorical_accuracy: 0.6786 - val_loss: 0.6426 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8421 - sparse_categorical_accuracy: 0.6780 - val_loss: 0.6282 - val_sparse_categorical_accuracy: 0.7920 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8884 - sparse_categorical_accuracy: 0.6786 - val_loss: 0.6245 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8104 - sparse_categorical_accuracy: 0.6915 - val_loss: 0.6216 - val_sparse_categorical_accuracy: 0.7950 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8187 - sparse_categorical_accuracy: 0.6961 - val_loss: 0.6162 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8087 - sparse_categorical_accuracy: 0.6961 - val_loss: 0.6056 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7794 - sparse_categorical_accuracy: 0.7143 - val_loss: 0.6033 - val_sparse_categorical_accuracy: 0.7920 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7883 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.5991 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7551 - sparse_categorical_accuracy: 0.7114 - val_loss: 0.5911 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7588 - sparse_categorical_accuracy: 0.7166 - val_loss: 0.5882 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7376 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.5862 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7490 - sparse_categorical_accuracy: 0.7196 - val_loss: 0.5805 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7342 - sparse_categorical_accuracy: 0.7319 - val_loss: 0.5784 - val_sparse_categorical_accuracy: 0.8010 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7308 - sparse_categorical_accuracy: 0.7359 - val_loss: 0.5800 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7300 - sparse_categorical_accuracy: 0.7278 - val_loss: 0.5694 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7038 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.5643 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6978 - sparse_categorical_accuracy: 0.7494 - val_loss: 0.5600 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7069 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.5578 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6856 - sparse_categorical_accuracy: 0.7506 - val_loss: 0.5539 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6917 - sparse_categorical_accuracy: 0.7477 - val_loss: 0.5525 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6820 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.5491 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6714 - sparse_categorical_accuracy: 0.7570 - val_loss: 0.5456 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6492 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.5409 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6329 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.5409 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6492 - sparse_categorical_accuracy: 0.7500 - val_loss: 0.5330 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6491 - sparse_categorical_accuracy: 0.7611 - val_loss: 0.5311 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6558 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.5269 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6371 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.5244 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6437 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.5217 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6030 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.5234 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6273 - sparse_categorical_accuracy: 0.7611 - val_loss: 0.5219 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6133 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.5134 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6260 - sparse_categorical_accuracy: 0.7588 - val_loss: 0.5106 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6001 - sparse_categorical_accuracy: 0.7851 - val_loss: 0.5101 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6047 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.5077 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5915 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.5052 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5812 - sparse_categorical_accuracy: 0.7904 - val_loss: 0.5011 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5864 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.5020 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5922 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.4973 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5877 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.4970 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5866 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.4942 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.5767 - sparse_categorical_accuracy: 0.7904 - val_loss: 0.4905 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5862 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4853 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5882 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.4847 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5738 - sparse_categorical_accuracy: 0.7963 - val_loss: 0.4845 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 8, test #2, 2nd iteration: loss of 0.48529180884361267; sparse_categorical_accuracy of 83.99999737739563%\n",
            "appending basis + selective feature vector data\n",
            "[1929, 2232, 225, 909, 1451, 2707, 1048, 2701, 39, 907, 224, 659, 2675, 2288, 2637, 1544, 73, 804, 1417, 631, 547, 1039, 916, 2336, 1585, 1724, 120, 300, 1872, 1580, 2216, 2456, 2155, 1014, 396, 1551, 1521, 397, 262, 1480, 764, 789, 2300, 208, 1323, 2468, 2062, 643, 796, 1262, 1285, 2059, 2343, 2128, 2425, 2100, 2364, 861, 1113, 1690, 1990, 135, 1924, 2023, 2692, 942, 331, 2625, 1825, 3, 923, 1070, 217, 109, 1987, 2586, 818, 2131, 1173, 1118, 41, 1408, 773, 2212, 1664, 800, 1592, 54, 892, 1944, 782, 594, 2092, 2454, 378, 2367, 2646, 1766, 2687, 450, 2044, 536, 1989, 1941, 1972, 531, 1484, 1058, 330, 2, 1354, 263, 2034, 1236, 2099, 927, 2332, 996, 1100, 260, 1940, 2447, 2349, 1073, 2580, 2371, 857, 116, 670, 299, 1668, 1816, 1116, 2133, 658, 264, 2287, 753, 2662, 9, 2327, 1880, 1057, 1334, 268, 1024, 844, 1810, 2645, 417, 1159, 1887, 231, 2115, 85, 2110, 1893, 1968, 2067, 189, 1775, 971, 835, 2266, 453, 920, 1978, 1557, 421, 2379, 1170, 472, 1299, 2020, 1602, 2071, 2186, 139, 1007, 336, 2444, 46, 2588, 524, 2411, 854, 1289, 1743, 24, 791, 2552, 583, 1111, 134, 1002, 2021, 2699, 283, 334, 380, 2661, 127, 2591, 1397, 1707, 481, 1550, 698, 1121, 1764, 2163, 2121, 291, 2321, 1650, 2198, 922, 2197, 104, 790, 2011, 1635, 398, 2599, 49, 834, 222, 2426, 166, 1997, 672, 1760, 362, 200, 985, 558, 341, 1206, 2421, 140, 1715, 879, 1362, 2245, 1244, 1767, 2463, 661, 115, 2682, 2590, 1099, 510, 1620, 2217, 2521, 2414, 913, 1407, 982, 1235, 2028, 433, 646, 288, 2365, 2533, 1981, 1304, 1470, 1325, 733, 606, 695, 394, 346, 1754, 2272, 1063, 1508, 2688, 2265, 680, 1370, 478, 2286, 1811, 701, 2638, 1172, 2328, 2311, 1992, 739, 725, 853, 69, 2452, 322, 90, 560, 1086, 287, 2010, 1177, 2000, 1894, 215, 413, 205, 2222, 172, 2553, 2169, 887, 1595, 874, 2392, 687, 445, 2639, 2068, 737, 2037, 2460, 429, 585, 2502, 210, 1182, 1824, 364, 1089, 2090, 2359, 1658, 1238, 1319, 2106, 882, 2512, 138, 591, 1644, 664, 2677, 2617, 2135, 1341, 236, 1581, 1790, 1642, 2293, 609, 1316, 1393, 1763, 1360, 1171, 788, 304, 2200, 17, 1911, 2213, 126, 1628, 406, 2539, 60, 2567, 2433, 1485, 2048, 2209, 105, 1846, 269, 657, 1482, 1952, 1797, 22, 1688, 2193, 666, 939, 94, 1657, 1785, 467, 997, 2144, 207, 250, 1257, 1574, 494, 1914, 1270, 415, 2290, 188, 1283, 1986, 141, 367, 2238, 2471, 214, 2451, 707, 1095, 1364, 1963, 1308, 1566, 353, 1955, 885, 1036, 1840, 459, 738, 525, 277, 1903, 1835, 1529, 865, 599, 1091, 2348, 723, 405, 411, 1369, 1813, 777, 1201, 148, 201, 2223, 1502, 1152, 1434, 2150, 1146, 1780, 1326, 902, 545, 755, 2472, 1452, 1792, 846, 1358, 691, 2383, 1190, 940, 1457, 2394, 1009, 637, 1572, 2019, 1844, 2546, 2412, 234, 2029, 2431, 815, 1402, 1998, 540, 387, 557, 1197, 1166, 886, 2636, 592, 1455, 1683, 513, 1020, 2175, 2390, 89, 2526, 2542, 38, 1878, 2203, 1181, 1492, 721, 1167, 2005, 1988, 383, 778, 2652, 1337, 539, 361, 1853, 979, 2130, 1659, 1639, 765, 2061, 86, 759, 2096, 293, 2226, 2347, 641, 193, 1453, 2263, 1460, 529, 1208, 894, 2003, 1004, 382, 516, 964, 2536, 486, 1468, 571, 1318, 18, 357, 55, 1908, 163, 2314, 1848, 697, 2386, 627, 991, 32, 1539, 1420, 2418, 2490, 2070, 1106, 175, 1346, 952, 1411, 528, 2576, 1519, 251, 15, 678, 2303, 1549, 1119, 911, 2615, 2647, 1961, 1178, 2604, 602, 1377, 2382, 1984, 806, 2587, 798, 1094, 821, 78, 1947, 460, 2225, 1904, 482, 2107, 1708, 437, 1645, 458, 1108, 2399, 2579, 491, 1536, 1192, 2656, 1222, 1801, 13, 1897, 7, 2227, 213, 1534, 1456, 650, 1186, 2334, 1112, 1219, 2098, 2503, 1306, 349, 1867, 1507, 1915, 2537, 1392, 1765, 564, 1345, 2244, 414, 180, 376, 1500, 1488, 1242, 1907, 968, 649, 1818, 1588, 1343, 1666, 176, 436, 211, 703, 1827, 410, 2635, 973, 2557, 192, 2424, 1161, 642, 2632, 1098, 1221, 407, 1385, 318, 1053, 709, 469, 988, 31, 183, 371, 780, 1964, 1246, 2199, 829, 2007, 1082, 1349, 1101, 1232, 1906, 595, 2523, 1991, 1430, 2127, 2257, 2095, 233, 1750, 1064, 92, 2356, 1699, 681, 385, 1568, 248, 83, 345, 95, 613, 1720, 831, 1771, 2541, 858, 28, 498, 936, 742, 1438, 10, 1137, 966, 354, 1050, 1905, 2084, 2627, 103, 686, 1748, 702, 1011, 2221, 2295, 2369, 2400, 1901, 2680, 1447, 875, 1355, 2422, 1117, 2497, 1292, 441, 2437, 196, 1735, 904, 493, 1671, 419, 1541, 2289, 1796, 1406, 2195, 2362, 1721, 779, 872, 527, 1237, 914, 648, 2529, 424, 2513, 1351, 128, 1487, 344, 2449, 1218, 1001, 941, 2330, 1475, 1950, 2611, 1884, 2001, 1684, 282, 652, 792, 1522, 2038, 2629, 1034, 618, 719, 243, 2219, 2282, 1597, 1509, 1033, 2211, 2013, 2389, 2703, 1305, 2101, 895, 1662, 91, 422, 447, 1798, 2105, 663, 1233, 1017, 2378, 535, 2049, 1670, 1147, 1015, 944, 159, 1327, 218, 1088, 1976, 1667, 80, 1311, 1295, 2508, 1309, 2047, 864, 1037, 891, 1847, 590, 1569, 29, 1413, 1373, 2500, 1239, 1394, 1400, 848, 2120, 2194, 477, 1561, 1078, 468, 1769, 107, 358, 1231, 216, 647, 1731, 256, 544, 900, 2052, 1102, 1174, 259, 1155, 393, 2243, 2596, 1066, 1314, 492, 1422, 2478, 1255, 581, 2118, 2177, 1938, 1936, 258, 1081, 1352, 958, 2373, 2350, 628, 2455, 630, 1614, 1794, 117, 1134, 2031, 1535, 167, 1012, 2017, 416, 315, 573, 1686, 1436, 2415, 1895, 635, 1258, 1085, 998, 674, 611, 184, 1466, 959, 634, 2117, 232, 2256, 2081, 244, 1860, 1554, 2697, 2388, 125, 292, 1497, 1087, 1646, 1795, 1261, 170, 1788, 2457, 1655, 113, 2409, 1705, 1532, 2439, 730, 2270, 1805, 935, 2165, 1974, 1604, 1396, 1127, 302, 2260, 824, 2157, 1067, 2250, 1531, 1652, 746, 2441, 1934, 897, 1510, 677, 1332, 2484, 1454, 1478, 2361, 2241, 1640, 278, 620, 1203, 1149, 2275, 1441, 2104, 160, 593, 1052, 1610, 2628, 2145, 1489, 1142, 2522, 2146, 2598, 2323, 2240, 2046, 1198, 520, 321, 2413, 66, 1097, 1205, 735, 26, 1245, 290, 1449]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 11.2623 - sparse_categorical_accuracy: 0.1241 - val_loss: 2.5620 - val_sparse_categorical_accuracy: 0.2840 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 8.1417 - sparse_categorical_accuracy: 0.1809 - val_loss: 2.6932 - val_sparse_categorical_accuracy: 0.3040 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.2505 - sparse_categorical_accuracy: 0.2055 - val_loss: 2.4381 - val_sparse_categorical_accuracy: 0.3190 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.9281 - sparse_categorical_accuracy: 0.2160 - val_loss: 2.0821 - val_sparse_categorical_accuracy: 0.3150 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.5100 - sparse_categorical_accuracy: 0.1879 - val_loss: 2.0335 - val_sparse_categorical_accuracy: 0.3660 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.0604 - sparse_categorical_accuracy: 0.2055 - val_loss: 2.2180 - val_sparse_categorical_accuracy: 0.3290 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.7668 - sparse_categorical_accuracy: 0.2131 - val_loss: 1.9763 - val_sparse_categorical_accuracy: 0.3580 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.3204 - sparse_categorical_accuracy: 0.2178 - val_loss: 1.8379 - val_sparse_categorical_accuracy: 0.3610 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.9392 - sparse_categorical_accuracy: 0.2494 - val_loss: 1.6111 - val_sparse_categorical_accuracy: 0.4350 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.5299 - sparse_categorical_accuracy: 0.2436 - val_loss: 1.5493 - val_sparse_categorical_accuracy: 0.4420 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.2901 - sparse_categorical_accuracy: 0.2676 - val_loss: 1.4954 - val_sparse_categorical_accuracy: 0.4820 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.8759 - sparse_categorical_accuracy: 0.2769 - val_loss: 1.4117 - val_sparse_categorical_accuracy: 0.5060 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.7010 - sparse_categorical_accuracy: 0.2775 - val_loss: 1.3208 - val_sparse_categorical_accuracy: 0.5390 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.4658 - sparse_categorical_accuracy: 0.2968 - val_loss: 1.3257 - val_sparse_categorical_accuracy: 0.5430 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1236 - sparse_categorical_accuracy: 0.3203 - val_loss: 1.2585 - val_sparse_categorical_accuracy: 0.5490 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1039 - sparse_categorical_accuracy: 0.3050 - val_loss: 1.2076 - val_sparse_categorical_accuracy: 0.5850 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.0074 - sparse_categorical_accuracy: 0.3308 - val_loss: 1.1809 - val_sparse_categorical_accuracy: 0.5630 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.7072 - sparse_categorical_accuracy: 0.3425 - val_loss: 1.1252 - val_sparse_categorical_accuracy: 0.6010 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5630 - sparse_categorical_accuracy: 0.3454 - val_loss: 1.1180 - val_sparse_categorical_accuracy: 0.5630 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4425 - sparse_categorical_accuracy: 0.3507 - val_loss: 1.0618 - val_sparse_categorical_accuracy: 0.6020 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2891 - sparse_categorical_accuracy: 0.3694 - val_loss: 1.0262 - val_sparse_categorical_accuracy: 0.6230 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2366 - sparse_categorical_accuracy: 0.3794 - val_loss: 1.0079 - val_sparse_categorical_accuracy: 0.6220 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0502 - sparse_categorical_accuracy: 0.4151 - val_loss: 0.9640 - val_sparse_categorical_accuracy: 0.6480 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9608 - sparse_categorical_accuracy: 0.4321 - val_loss: 0.9408 - val_sparse_categorical_accuracy: 0.6510 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.8926 - sparse_categorical_accuracy: 0.4297 - val_loss: 0.9336 - val_sparse_categorical_accuracy: 0.6550 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9498 - sparse_categorical_accuracy: 0.4338 - val_loss: 0.9051 - val_sparse_categorical_accuracy: 0.6720 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7551 - sparse_categorical_accuracy: 0.4567 - val_loss: 0.8805 - val_sparse_categorical_accuracy: 0.6880 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6048 - sparse_categorical_accuracy: 0.4766 - val_loss: 0.8501 - val_sparse_categorical_accuracy: 0.6970 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5841 - sparse_categorical_accuracy: 0.5006 - val_loss: 0.8231 - val_sparse_categorical_accuracy: 0.7260 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5627 - sparse_categorical_accuracy: 0.4766 - val_loss: 0.8249 - val_sparse_categorical_accuracy: 0.7190 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4779 - sparse_categorical_accuracy: 0.5059 - val_loss: 0.8123 - val_sparse_categorical_accuracy: 0.7190 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4121 - sparse_categorical_accuracy: 0.5158 - val_loss: 0.8080 - val_sparse_categorical_accuracy: 0.7100 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3865 - sparse_categorical_accuracy: 0.5217 - val_loss: 0.7865 - val_sparse_categorical_accuracy: 0.7240 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2953 - sparse_categorical_accuracy: 0.5480 - val_loss: 0.7695 - val_sparse_categorical_accuracy: 0.7400 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3068 - sparse_categorical_accuracy: 0.5468 - val_loss: 0.7506 - val_sparse_categorical_accuracy: 0.7500 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2242 - sparse_categorical_accuracy: 0.5679 - val_loss: 0.7403 - val_sparse_categorical_accuracy: 0.7520 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1981 - sparse_categorical_accuracy: 0.5703 - val_loss: 0.7381 - val_sparse_categorical_accuracy: 0.7490 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1197 - sparse_categorical_accuracy: 0.5948 - val_loss: 0.7257 - val_sparse_categorical_accuracy: 0.7510 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1007 - sparse_categorical_accuracy: 0.5954 - val_loss: 0.7102 - val_sparse_categorical_accuracy: 0.7700 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1287 - sparse_categorical_accuracy: 0.5919 - val_loss: 0.7161 - val_sparse_categorical_accuracy: 0.7530 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1112 - sparse_categorical_accuracy: 0.5849 - val_loss: 0.6951 - val_sparse_categorical_accuracy: 0.7770 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0418 - sparse_categorical_accuracy: 0.6142 - val_loss: 0.6847 - val_sparse_categorical_accuracy: 0.7710 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0652 - sparse_categorical_accuracy: 0.6142 - val_loss: 0.6761 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0034 - sparse_categorical_accuracy: 0.6388 - val_loss: 0.6711 - val_sparse_categorical_accuracy: 0.7870 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9185 - sparse_categorical_accuracy: 0.6487 - val_loss: 0.6604 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9562 - sparse_categorical_accuracy: 0.6458 - val_loss: 0.6545 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9074 - sparse_categorical_accuracy: 0.6557 - val_loss: 0.6494 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.9332 - sparse_categorical_accuracy: 0.6493 - val_loss: 0.6416 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8874 - sparse_categorical_accuracy: 0.6780 - val_loss: 0.6394 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8575 - sparse_categorical_accuracy: 0.6827 - val_loss: 0.6272 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8388 - sparse_categorical_accuracy: 0.6827 - val_loss: 0.6257 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8611 - sparse_categorical_accuracy: 0.6768 - val_loss: 0.6241 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8487 - sparse_categorical_accuracy: 0.6926 - val_loss: 0.6165 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8299 - sparse_categorical_accuracy: 0.7032 - val_loss: 0.6093 - val_sparse_categorical_accuracy: 0.8040 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7960 - sparse_categorical_accuracy: 0.7078 - val_loss: 0.6050 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8264 - sparse_categorical_accuracy: 0.6938 - val_loss: 0.5984 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8032 - sparse_categorical_accuracy: 0.7155 - val_loss: 0.5933 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7905 - sparse_categorical_accuracy: 0.7055 - val_loss: 0.5936 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7543 - sparse_categorical_accuracy: 0.7178 - val_loss: 0.5833 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7702 - sparse_categorical_accuracy: 0.7184 - val_loss: 0.5809 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7556 - sparse_categorical_accuracy: 0.7213 - val_loss: 0.5753 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7415 - sparse_categorical_accuracy: 0.7213 - val_loss: 0.5716 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7300 - sparse_categorical_accuracy: 0.7324 - val_loss: 0.5705 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7012 - sparse_categorical_accuracy: 0.7512 - val_loss: 0.5614 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6978 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.5576 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7104 - sparse_categorical_accuracy: 0.7383 - val_loss: 0.5538 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6991 - sparse_categorical_accuracy: 0.7359 - val_loss: 0.5473 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6909 - sparse_categorical_accuracy: 0.7430 - val_loss: 0.5481 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6916 - sparse_categorical_accuracy: 0.7459 - val_loss: 0.5436 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6827 - sparse_categorical_accuracy: 0.7482 - val_loss: 0.5394 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6751 - sparse_categorical_accuracy: 0.7459 - val_loss: 0.5351 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6669 - sparse_categorical_accuracy: 0.7459 - val_loss: 0.5396 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6435 - sparse_categorical_accuracy: 0.7629 - val_loss: 0.5290 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6331 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.5287 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6346 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.5255 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6637 - sparse_categorical_accuracy: 0.7547 - val_loss: 0.5211 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6423 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.5186 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6440 - sparse_categorical_accuracy: 0.7641 - val_loss: 0.5174 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6291 - sparse_categorical_accuracy: 0.7646 - val_loss: 0.5143 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6357 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.5144 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6352 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.5098 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6118 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.5087 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6371 - sparse_categorical_accuracy: 0.7664 - val_loss: 0.5037 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6228 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.5053 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5985 - sparse_categorical_accuracy: 0.7728 - val_loss: 0.4985 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6041 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.4995 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5890 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4977 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5919 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.4936 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5776 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4922 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5891 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.4885 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5746 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.4873 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5659 - sparse_categorical_accuracy: 0.7892 - val_loss: 0.4845 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5905 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.4843 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5719 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.4816 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5819 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.4771 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5910 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.4798 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5885 - sparse_categorical_accuracy: 0.7822 - val_loss: 0.4772 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5847 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.4732 - val_sparse_categorical_accuracy: 0.8450 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5830 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4715 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5826 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.4744 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 8, test #3, 2nd iteration: loss of 0.4731837511062622; sparse_categorical_accuracy of 84.50000286102295%\n",
            "appending basis + selective feature vector data\n",
            "[1929, 2232, 225, 909, 1451, 2707, 1048, 2701, 39, 907, 224, 659, 2675, 2288, 2637, 1544, 73, 804, 1417, 631, 547, 1039, 916, 2336, 1585, 1724, 120, 300, 1872, 1580, 2216, 2456, 2155, 1014, 396, 1551, 1521, 397, 262, 1480, 764, 789, 2300, 208, 1323, 2468, 2062, 643, 796, 1262, 1285, 2059, 2343, 2128, 2425, 2100, 2364, 861, 1113, 1690, 1990, 135, 1924, 2023, 2692, 942, 331, 2625, 1825, 3, 923, 1070, 217, 109, 1987, 2586, 818, 2131, 1173, 1118, 41, 1408, 773, 2212, 1664, 800, 1592, 54, 892, 1944, 782, 594, 2092, 2454, 378, 2367, 2646, 1766, 2687, 450, 2044, 536, 1989, 1941, 1972, 531, 1484, 1058, 330, 2, 1354, 263, 2034, 1236, 2099, 927, 2332, 996, 1100, 260, 1940, 2447, 2349, 1073, 2580, 2371, 857, 116, 670, 299, 1668, 1816, 1116, 2133, 658, 264, 2287, 753, 2662, 9, 2327, 1880, 1057, 1334, 268, 1024, 844, 1810, 2645, 417, 1159, 1887, 231, 2115, 85, 2110, 1893, 1968, 2067, 189, 1775, 971, 835, 2266, 453, 920, 1978, 1557, 421, 2379, 1170, 472, 1299, 2020, 1602, 2071, 2186, 139, 1007, 336, 2444, 46, 2588, 524, 2411, 854, 1289, 1743, 24, 791, 2552, 583, 1111, 134, 1002, 2021, 2699, 283, 334, 380, 2661, 127, 2591, 1397, 1707, 481, 1550, 698, 1121, 1764, 2163, 2121, 291, 2321, 1650, 2198, 922, 2197, 104, 790, 2011, 1635, 398, 2599, 49, 834, 222, 2426, 166, 1997, 672, 1760, 362, 200, 985, 558, 341, 1206, 2421, 140, 1715, 879, 1362, 2245, 1244, 1767, 2463, 661, 115, 2682, 2590, 1099, 510, 1620, 2217, 2521, 2414, 913, 1407, 982, 1235, 2028, 433, 646, 288, 2365, 2533, 1981, 1304, 1470, 1325, 733, 606, 695, 394, 346, 1754, 2272, 1063, 1508, 2688, 2265, 680, 1370, 478, 2286, 1811, 701, 2638, 1172, 2328, 2311, 1992, 739, 725, 853, 69, 2452, 322, 90, 560, 1086, 287, 2010, 1177, 2000, 1894, 215, 413, 205, 2222, 172, 2553, 2169, 887, 1595, 874, 2392, 687, 445, 2639, 2068, 737, 2037, 2460, 429, 585, 2502, 210, 1182, 1824, 364, 1089, 2090, 2359, 1658, 1238, 1319, 2106, 882, 2512, 138, 591, 1644, 664, 2677, 2617, 2135, 1341, 236, 1581, 1790, 1642, 2293, 609, 1316, 1393, 1763, 1360, 1171, 788, 304, 2200, 17, 1911, 2213, 126, 1628, 406, 2539, 60, 2567, 2433, 1485, 2048, 2209, 105, 1846, 269, 657, 1482, 1952, 1797, 22, 1688, 2193, 666, 939, 94, 1657, 1785, 467, 997, 2144, 207, 250, 1257, 1574, 494, 1914, 1270, 415, 2290, 188, 1283, 1986, 141, 367, 2238, 2471, 214, 2451, 707, 1095, 1364, 1963, 1308, 1566, 353, 1955, 885, 1036, 1840, 459, 738, 525, 277, 1903, 1835, 1529, 865, 599, 1091, 2348, 723, 405, 411, 1369, 1813, 777, 1201, 148, 201, 2223, 1502, 1152, 1434, 2150, 1146, 1780, 1326, 902, 545, 755, 2472, 1452, 1792, 846, 1358, 691, 2383, 1190, 940, 1457, 2394, 1009, 637, 1572, 2019, 1844, 2546, 2412, 234, 2029, 2431, 815, 1402, 1998, 540, 387, 557, 1197, 1166, 886, 2636, 592, 1455, 1683, 513, 1020, 2175, 2390, 89, 2526, 2542, 38, 1878, 2203, 1181, 1492, 721, 1167, 2005, 1988, 383, 778, 2652, 1337, 539, 361, 1853, 979, 2130, 1659, 1639, 765, 2061, 86, 759, 2096, 293, 2226, 2347, 641, 193, 1453, 2263, 1460, 529, 1208, 894, 2003, 1004, 382, 516, 964, 2536, 486, 1468, 571, 1318, 18, 357, 55, 1908, 163, 2314, 1848, 697, 2386, 627, 991, 32, 1539, 1420, 2418, 2490, 2070, 1106, 175, 1346, 952, 1411, 528, 2576, 1519, 251, 15, 678, 2303, 1549, 1119, 911, 2615, 2647, 1961, 1178, 2604, 602, 1377, 2382, 1984, 806, 2587, 798, 1094, 821, 78, 1947, 460, 2225, 1904, 482, 2107, 1708, 437, 1645, 458, 1108, 2399, 2579, 491, 1536, 1192, 2656, 1222, 1801, 13, 1897, 7, 2227, 213, 1534, 1456, 650, 1186, 2334, 1112, 1219, 2098, 2503, 1306, 349, 1867, 1507, 1915, 2537, 1392, 1765, 564, 1345, 2244, 414, 180, 376, 1500, 1488, 1242, 1907, 968, 649, 1818, 1588, 1343, 1666, 176, 436, 211, 703, 1827, 410, 2635, 973, 2557, 192, 2424, 1161, 642, 2632, 1098, 1221, 407, 1385, 318, 1053, 709, 469, 988, 31, 183, 371, 780, 1964, 1246, 2199, 829, 2007, 1082, 1349, 1101, 1232, 1906, 595, 2523, 1991, 1430, 2127, 2257, 2095, 233, 1750, 1064, 92, 2356, 1699, 681, 385, 1568, 248, 83, 345, 95, 613, 1720, 831, 1771, 2541, 858, 28, 498, 936, 742, 1438, 10, 1137, 966, 354, 1050, 1905, 2084, 2627, 103, 686, 1748, 702, 1011, 2221, 2295, 2369, 2400, 1901, 2680, 1447, 875, 1355, 2422, 1117, 2497, 1292, 441, 2437, 196, 1735, 904, 493, 1671, 419, 1541, 2289, 1796, 1406, 2195, 2362, 1721, 779, 872, 527, 1237, 914, 648, 2529, 424, 2513, 1351, 128, 1487, 344, 2449, 1218, 1001, 941, 2330, 1475, 1950, 2611, 1884, 2001, 1684, 282, 652, 792, 1522, 2038, 2629, 1034, 618, 719, 243, 2219, 2282, 1597, 1509, 1033, 2211, 2013, 2389, 2703, 1305, 2101, 895, 1662, 91, 422, 447, 1798, 2105, 663, 1233, 1017, 2378, 535, 2049, 1670, 1147, 1015, 944, 159, 1327, 218, 1088, 1976, 1667, 80, 1311, 1295, 2508, 1309, 2047, 864, 1037, 891, 1847, 590, 1569, 29, 1413, 1373, 2500, 1239, 1394, 1400, 848, 2120, 2194, 477, 1561, 1078, 468, 1769, 107, 358, 1231, 216, 647, 1731, 256, 544, 900, 2052, 1102, 1174, 259, 1155, 393, 2243, 2596, 1066, 1314, 492, 1422, 2478, 1255, 581, 2118, 2177, 1938, 1936, 258, 1081, 1352, 958, 2373, 2350, 628, 2455, 630, 1614, 1794, 117, 1134, 2031, 1535, 167, 1012, 2017, 416, 315, 573, 1686, 1436, 2415, 1895, 635, 1258, 1085, 998, 674, 611, 184, 1466, 959, 634, 2117, 232, 2256, 2081, 244, 1860, 1554, 2697, 2388, 125, 292, 1497, 1087, 1646, 1795, 1261, 170, 1788, 2457, 1655, 113, 2409, 1705, 1532, 2439, 730, 2270, 1805, 935, 2165, 1974, 1604, 1396, 1127, 302, 2260, 824, 2157, 1067, 2250, 1531, 1652, 746, 2441, 1934, 897, 1510, 677, 1332, 2484, 1454, 1478, 2361, 2241, 1640, 278, 620, 1203, 1149, 2275, 1441, 2104, 160, 593, 1052, 1610, 2628, 2145, 1489, 1142, 2522, 2146, 2598, 2323, 2240, 2046, 1198, 520, 321, 2413, 66, 1097, 1205, 735, 26, 1245, 290, 1449]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 11.3502 - sparse_categorical_accuracy: 0.1095 - val_loss: 2.6614 - val_sparse_categorical_accuracy: 0.3050 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.3774 - sparse_categorical_accuracy: 0.1862 - val_loss: 2.4731 - val_sparse_categorical_accuracy: 0.3160 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.9734 - sparse_categorical_accuracy: 0.2108 - val_loss: 2.4460 - val_sparse_categorical_accuracy: 0.3210 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.7813 - sparse_categorical_accuracy: 0.2043 - val_loss: 2.1479 - val_sparse_categorical_accuracy: 0.3340 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.4549 - sparse_categorical_accuracy: 0.2160 - val_loss: 2.2058 - val_sparse_categorical_accuracy: 0.3380 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.9431 - sparse_categorical_accuracy: 0.2260 - val_loss: 1.8402 - val_sparse_categorical_accuracy: 0.3690 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.4814 - sparse_categorical_accuracy: 0.2412 - val_loss: 1.8480 - val_sparse_categorical_accuracy: 0.3730 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.2091 - sparse_categorical_accuracy: 0.2594 - val_loss: 1.6092 - val_sparse_categorical_accuracy: 0.3940 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.8477 - sparse_categorical_accuracy: 0.2681 - val_loss: 1.4932 - val_sparse_categorical_accuracy: 0.4440 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.7253 - sparse_categorical_accuracy: 0.2863 - val_loss: 1.5002 - val_sparse_categorical_accuracy: 0.4350 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.4329 - sparse_categorical_accuracy: 0.2986 - val_loss: 1.4285 - val_sparse_categorical_accuracy: 0.4390 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1470 - sparse_categorical_accuracy: 0.3167 - val_loss: 1.3063 - val_sparse_categorical_accuracy: 0.5130 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.9983 - sparse_categorical_accuracy: 0.3167 - val_loss: 1.3295 - val_sparse_categorical_accuracy: 0.4950 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8396 - sparse_categorical_accuracy: 0.3255 - val_loss: 1.2121 - val_sparse_categorical_accuracy: 0.5450 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.6490 - sparse_categorical_accuracy: 0.3624 - val_loss: 1.1625 - val_sparse_categorical_accuracy: 0.5560 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5674 - sparse_categorical_accuracy: 0.3507 - val_loss: 1.1070 - val_sparse_categorical_accuracy: 0.5940 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3276 - sparse_categorical_accuracy: 0.3811 - val_loss: 1.0394 - val_sparse_categorical_accuracy: 0.6290 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2459 - sparse_categorical_accuracy: 0.3899 - val_loss: 1.0125 - val_sparse_categorical_accuracy: 0.6270 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0928 - sparse_categorical_accuracy: 0.4110 - val_loss: 0.9460 - val_sparse_categorical_accuracy: 0.6730 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.9377 - sparse_categorical_accuracy: 0.4122 - val_loss: 0.9292 - val_sparse_categorical_accuracy: 0.6740 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9022 - sparse_categorical_accuracy: 0.4362 - val_loss: 0.9278 - val_sparse_categorical_accuracy: 0.6540 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8037 - sparse_categorical_accuracy: 0.4584 - val_loss: 0.9007 - val_sparse_categorical_accuracy: 0.6630 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6613 - sparse_categorical_accuracy: 0.4555 - val_loss: 0.8892 - val_sparse_categorical_accuracy: 0.6860 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6406 - sparse_categorical_accuracy: 0.4660 - val_loss: 0.8381 - val_sparse_categorical_accuracy: 0.6960 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5713 - sparse_categorical_accuracy: 0.4824 - val_loss: 0.8238 - val_sparse_categorical_accuracy: 0.7090 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4322 - sparse_categorical_accuracy: 0.5176 - val_loss: 0.8124 - val_sparse_categorical_accuracy: 0.7060 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4681 - sparse_categorical_accuracy: 0.5076 - val_loss: 0.7921 - val_sparse_categorical_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4014 - sparse_categorical_accuracy: 0.5152 - val_loss: 0.7675 - val_sparse_categorical_accuracy: 0.7400 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3483 - sparse_categorical_accuracy: 0.5381 - val_loss: 0.7559 - val_sparse_categorical_accuracy: 0.7390 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2389 - sparse_categorical_accuracy: 0.5656 - val_loss: 0.7472 - val_sparse_categorical_accuracy: 0.7410 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2290 - sparse_categorical_accuracy: 0.5755 - val_loss: 0.7278 - val_sparse_categorical_accuracy: 0.7500 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2125 - sparse_categorical_accuracy: 0.5831 - val_loss: 0.7180 - val_sparse_categorical_accuracy: 0.7610 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1199 - sparse_categorical_accuracy: 0.5989 - val_loss: 0.7048 - val_sparse_categorical_accuracy: 0.7680 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1495 - sparse_categorical_accuracy: 0.5861 - val_loss: 0.6980 - val_sparse_categorical_accuracy: 0.7610 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1069 - sparse_categorical_accuracy: 0.5948 - val_loss: 0.6816 - val_sparse_categorical_accuracy: 0.7800 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0510 - sparse_categorical_accuracy: 0.6247 - val_loss: 0.6787 - val_sparse_categorical_accuracy: 0.7740 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0291 - sparse_categorical_accuracy: 0.6282 - val_loss: 0.6747 - val_sparse_categorical_accuracy: 0.7800 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0280 - sparse_categorical_accuracy: 0.6393 - val_loss: 0.6598 - val_sparse_categorical_accuracy: 0.7820 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9956 - sparse_categorical_accuracy: 0.6364 - val_loss: 0.6513 - val_sparse_categorical_accuracy: 0.7870 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9432 - sparse_categorical_accuracy: 0.6633 - val_loss: 0.6418 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9157 - sparse_categorical_accuracy: 0.6616 - val_loss: 0.6384 - val_sparse_categorical_accuracy: 0.7810 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9017 - sparse_categorical_accuracy: 0.6645 - val_loss: 0.6436 - val_sparse_categorical_accuracy: 0.7760 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9056 - sparse_categorical_accuracy: 0.6809 - val_loss: 0.6233 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8654 - sparse_categorical_accuracy: 0.6786 - val_loss: 0.6196 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8568 - sparse_categorical_accuracy: 0.6768 - val_loss: 0.6115 - val_sparse_categorical_accuracy: 0.7920 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8330 - sparse_categorical_accuracy: 0.7037 - val_loss: 0.6029 - val_sparse_categorical_accuracy: 0.8010 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8372 - sparse_categorical_accuracy: 0.6721 - val_loss: 0.5960 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8127 - sparse_categorical_accuracy: 0.7178 - val_loss: 0.5947 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8048 - sparse_categorical_accuracy: 0.7037 - val_loss: 0.5887 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7757 - sparse_categorical_accuracy: 0.7119 - val_loss: 0.5830 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7804 - sparse_categorical_accuracy: 0.7125 - val_loss: 0.5736 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7605 - sparse_categorical_accuracy: 0.7254 - val_loss: 0.5765 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7494 - sparse_categorical_accuracy: 0.7143 - val_loss: 0.5724 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7401 - sparse_categorical_accuracy: 0.7248 - val_loss: 0.5631 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7185 - sparse_categorical_accuracy: 0.7330 - val_loss: 0.5615 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7185 - sparse_categorical_accuracy: 0.7354 - val_loss: 0.5672 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7098 - sparse_categorical_accuracy: 0.7441 - val_loss: 0.5502 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6929 - sparse_categorical_accuracy: 0.7553 - val_loss: 0.5469 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7076 - sparse_categorical_accuracy: 0.7354 - val_loss: 0.5443 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6866 - sparse_categorical_accuracy: 0.7406 - val_loss: 0.5405 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6849 - sparse_categorical_accuracy: 0.7465 - val_loss: 0.5345 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6849 - sparse_categorical_accuracy: 0.7459 - val_loss: 0.5326 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6854 - sparse_categorical_accuracy: 0.7494 - val_loss: 0.5310 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6515 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.5256 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6704 - sparse_categorical_accuracy: 0.7623 - val_loss: 0.5208 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6351 - sparse_categorical_accuracy: 0.7629 - val_loss: 0.5201 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6455 - sparse_categorical_accuracy: 0.7594 - val_loss: 0.5146 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6339 - sparse_categorical_accuracy: 0.7705 - val_loss: 0.5121 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6390 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.5099 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6345 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.5096 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6196 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.5037 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6071 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.5024 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6112 - sparse_categorical_accuracy: 0.7564 - val_loss: 0.4988 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6346 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.4992 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6101 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.4978 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6138 - sparse_categorical_accuracy: 0.7641 - val_loss: 0.4948 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5998 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.4932 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.5919 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.4883 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5939 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.4862 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5889 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.4822 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5924 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4813 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5974 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.4827 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5915 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.4750 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5746 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4736 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5838 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4740 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5851 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.4694 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5698 - sparse_categorical_accuracy: 0.7916 - val_loss: 0.4700 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5677 - sparse_categorical_accuracy: 0.7881 - val_loss: 0.4693 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5665 - sparse_categorical_accuracy: 0.7904 - val_loss: 0.4687 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5650 - sparse_categorical_accuracy: 0.7980 - val_loss: 0.4652 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5534 - sparse_categorical_accuracy: 0.7951 - val_loss: 0.4636 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5573 - sparse_categorical_accuracy: 0.7986 - val_loss: 0.4634 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5366 - sparse_categorical_accuracy: 0.7963 - val_loss: 0.4622 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5661 - sparse_categorical_accuracy: 0.7910 - val_loss: 0.4574 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5622 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.4587 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5468 - sparse_categorical_accuracy: 0.7916 - val_loss: 0.4555 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5597 - sparse_categorical_accuracy: 0.7963 - val_loss: 0.4561 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5483 - sparse_categorical_accuracy: 0.7951 - val_loss: 0.4540 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5404 - sparse_categorical_accuracy: 0.7998 - val_loss: 0.4528 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5490 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4568 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 8, test #4, 2nd iteration: loss of 0.465200275182724; sparse_categorical_accuracy of 84.20000076293945%\n",
            "appending basis + selective feature vector data\n",
            "[1994, 2290, 1874, 268, 511, 1086, 491, 98, 1785, 2195, 1530, 2509, 1251, 1705, 344, 2662, 281, 2484, 2497, 865, 359, 1819, 2173, 911, 100, 882, 1857, 1415, 239, 86, 2107, 2584, 2292, 1329, 2201, 1744, 522, 2576, 1539, 343, 628, 588, 138, 2355, 826, 2514, 106, 2060, 1664, 858, 1498, 1067, 1967, 135, 885, 1937, 1823, 1915, 2074, 292, 2285, 1866, 992, 713, 2109, 2190, 1100, 1662, 1204, 1396, 173, 970, 1725, 2088, 290, 277, 270, 1144, 843, 543, 927, 1406, 2063, 2454, 1789, 1687, 2521, 450, 1998, 2629, 818, 2702, 602, 495, 783, 426, 2477, 1134, 526, 1371, 1354, 2054, 1078, 1671, 2354, 685, 943, 1159, 1597, 1163, 2689, 1271, 1437, 421, 435, 31, 2661, 1198, 1342, 260, 156, 134, 240, 2447, 1641, 802, 89, 1448, 499, 1337, 1246, 2250, 2392, 2181, 2257, 507, 490, 2253, 2041, 1876, 468, 2359, 617, 1044, 233, 37, 1694, 1810, 53, 1581, 1334, 1768, 821, 1057, 207, 2527, 199, 976, 2369, 1167, 358, 286, 2322, 399, 1129, 1667, 2478, 609, 2495, 425, 664, 1854, 2615, 1654, 2307, 812, 479, 1965, 1631, 2094, 217, 254, 878, 68, 2681, 1422, 228, 657, 2583, 2160, 236, 1451, 500, 2602, 190, 754, 1576, 123, 857, 1733, 101, 2009, 2411, 2427, 1065, 989, 1880, 652, 42, 776, 1376, 2239, 587, 2541, 226, 1070, 346, 920, 647, 1732, 1397, 1464, 2042, 704, 2128, 218, 940, 353, 514, 148, 2441, 2086, 1055, 1410, 2597, 632, 1979, 572, 717, 1752, 1736, 1843, 2450, 1331, 2431, 2665, 2002, 159, 2336, 693, 1605, 1828, 419, 1421, 774, 1181, 1327, 330, 2085, 1683, 171, 1066, 643, 1074, 2612, 274, 1940, 815, 1249, 1096, 707, 206, 2287, 165, 1642, 113, 1412, 0, 1972, 1946, 1678, 2288, 1191, 2421, 558, 1992, 567, 1717, 934, 1578, 848, 579, 1969, 2619, 275, 1475, 1824, 782, 2433, 1128, 192, 2601, 904, 2522, 1447, 1024, 83, 2240, 1345, 1478, 1775, 1172, 645, 1080, 952, 2464, 56, 1681, 757, 820, 1321, 2306, 396, 1684, 1928, 930, 1629, 30, 1298, 1943, 1510, 1332, 1194, 698, 755, 2071, 2609, 1009, 2039, 1726, 1302, 20, 965, 1818, 1402, 2607, 518, 751, 2486, 875, 1123, 551, 795, 1431, 1185, 2564, 541, 2592, 1287, 393, 352, 836, 2549, 1936, 964, 170, 175, 1151, 905, 1427, 853, 444, 137, 1093, 1036, 1634, 1283, 405, 748, 1650, 1216, 143, 345, 420, 1627, 1111, 2364, 1485, 659, 2415, 2289, 1661, 1533, 1280, 1137, 367, 1368, 997, 768, 1574, 2593, 1092, 252, 505, 529, 582, 2508, 2672, 141, 651, 1790, 906, 1976, 2580, 1330, 1846, 2114, 953, 1845, 583, 2204, 926, 811, 109, 2261, 2247, 695, 1753, 303, 886, 1378, 2269, 921, 2703, 936, 1916, 1390, 1784, 2642, 2382, 954, 197, 1982, 1722, 607, 1221, 2295, 577, 1193, 852, 1492, 653, 1166, 2647, 646, 2596, 1319, 2407, 2131, 612, 1821, 2594, 1521, 2457, 599, 2262, 622, 40, 2698, 2545, 1575, 8, 2375, 2624, 868, 1105, 1847, 1112, 92, 2356, 525, 1927, 1908, 978, 1497, 1622, 1190, 174, 1010, 1462, 981, 2334, 1157, 321, 2127, 2548, 1727, 63, 1663, 1179, 1517, 1075, 411, 1981, 209, 384, 1707, 1779, 1557, 966, 2425, 2245, 2581, 1887, 2631, 238, 2338, 181, 2271, 324, 716, 1039, 1016, 1632, 2655, 2083, 2586, 2033, 1848, 1199, 299, 1900, 555, 357, 660, 1306, 66, 2529, 676, 1090, 1208, 234, 722, 2205, 1225, 2032, 1858, 1579, 2663, 447, 2120, 120, 1987, 1741, 2694, 1803, 1814, 1723, 2263, 1951, 2111, 2673, 304, 2153, 1465, 1822, 2333, 1942, 1955, 1468, 93, 2641, 51, 414, 1586, 1718, 1323, 814, 775, 2168, 473, 1379, 1017, 2704, 2423, 1919, 2006, 76, 1734, 1636, 566, 2398, 2418, 1230, 2660, 2274, 1366, 1472, 2341, 198, 250, 769, 319, 413, 1288, 2070, 1188, 2535, 1783, 1505, 2386, 1480, 1115, 1418, 1192, 271, 803, 2023, 283, 1685, 408, 2505, 2019, 1553, 1308, 1155, 1320, 149, 2507, 2110, 2565, 1984, 2349, 917, 1833, 929, 2053, 1322, 699, 1966, 832, 1121, 606, 2223, 1849, 1991, 2158, 837, 1022, 1688, 614, 2164, 1289, 845, 2197, 16, 1207, 1162, 1262, 1551, 736, 1423, 2283, 2363, 1351, 1136, 2012, 2516, 2511, 1160, 1248, 1713, 2001, 933, 1026, 2259, 262, 47, 1006, 601, 1056, 388, 727, 1742, 1083, 1501, 2443, 963, 1279, 916, 1934, 1904, 883, 907, 2301, 2372, 105, 1309, 689, 90, 102, 1310, 1766, 571, 2152, 1046, 2391, 705, 2177, 1122, 1689, 2328, 2401, 760, 730, 1264, 1455, 67, 731, 682, 1508, 2224, 1277, 1518, 855, 711, 191, 1590, 2360, 448, 1881, 2133, 1363, 1961, 2460, 2073, 193, 1293, 2492, 2235, 398, 1053, 1938, 834, 1138, 2150, 1515, 1747, 1615, 247, 2113, 935, 279, 502, 15, 735, 944, 2408, 2455, 2537, 189, 1537, 765, 2065, 690, 1616, 1062, 1232, 1381, 1963, 2126, 136, 1220, 738, 157, 1176, 323, 990, 2543, 2294, 1393, 2487, 472, 2256, 2534, 2620, 959, 888, 1829, 1691, 824, 520, 2134, 1417, 1324, 2573, 2482, 80, 1276, 1639, 1791, 1610, 1906, 1068, 937, 1284, 2305, 1486, 2044, 2368, 1350, 1419, 683, 7, 1834, 2136, 1399, 2618, 2025, 654, 118, 2621, 838, 741, 1012, 807, 1656, 884, 2144, 1404, 1269, 1401, 2686, 1607, 877, 2646, 1898, 1132, 27, 661, 2196, 2174, 2286, 427, 2059, 671, 2102, 471, 1620, 817, 2707, 2657, 2366, 2161, 2142, 2377, 1467, 2254, 1939, 1686, 23, 1416, 1295, 70, 1373, 82, 115, 1903, 1815, 293, 2213, 1059, 2649, 85, 1695, 527, 2608, 1265, 1300, 870, 792, 931, 2129, 2696, 2388, 823, 861, 2216, 813, 1336, 1614, 409, 1143, 1812, 627, 1787, 1020, 806, 1142, 1825, 1503, 375, 1569, 1282, 2176, 1347, 1680, 1878, 2532, 2297, 245, 129, 2034, 79, 1147, 2118, 1835, 1897, 476, 1885, 2246, 164, 467, 1658, 482, 36, 1535, 2605, 1651, 2406, 1905, 1041, 1403, 2078, 322, 152, 1349, 487, 610, 64, 2384, 1802, 465, 556, 2329, 761, 2599, 320, 81, 1069, 2680, 1988, 354, 590, 932, 489, 1930, 1079, 1592, 2659, 1580, 973, 284, 107, 2299, 2687, 2439, 1702, 1211, 611, 2311, 43, 697, 1820, 1077, 2590, 2480, 742, 2149, 146, 1646, 2588, 825, 1548, 1907, 394, 1529, 1700, 2562, 844, 2547, 1980, 1601, 2140, 272, 575, 25, 899, 317, 1247]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 10.8417 - sparse_categorical_accuracy: 0.1780 - val_loss: 3.5685 - val_sparse_categorical_accuracy: 0.3090 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.1333 - sparse_categorical_accuracy: 0.1903 - val_loss: 2.8415 - val_sparse_categorical_accuracy: 0.2920 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.5574 - sparse_categorical_accuracy: 0.2155 - val_loss: 1.9106 - val_sparse_categorical_accuracy: 0.3260 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.0570 - sparse_categorical_accuracy: 0.2143 - val_loss: 1.9164 - val_sparse_categorical_accuracy: 0.3390 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.5478 - sparse_categorical_accuracy: 0.2254 - val_loss: 1.8315 - val_sparse_categorical_accuracy: 0.3500 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.0160 - sparse_categorical_accuracy: 0.2482 - val_loss: 1.6712 - val_sparse_categorical_accuracy: 0.3700 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.7353 - sparse_categorical_accuracy: 0.2523 - val_loss: 1.6447 - val_sparse_categorical_accuracy: 0.3890 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 4.4671 - sparse_categorical_accuracy: 0.2553 - val_loss: 1.5088 - val_sparse_categorical_accuracy: 0.4240 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.2081 - sparse_categorical_accuracy: 0.2845 - val_loss: 1.3767 - val_sparse_categorical_accuracy: 0.4800 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.8527 - sparse_categorical_accuracy: 0.2927 - val_loss: 1.3551 - val_sparse_categorical_accuracy: 0.4620 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.6497 - sparse_categorical_accuracy: 0.2845 - val_loss: 1.2861 - val_sparse_categorical_accuracy: 0.4780 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.4554 - sparse_categorical_accuracy: 0.3062 - val_loss: 1.2255 - val_sparse_categorical_accuracy: 0.5320 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1640 - sparse_categorical_accuracy: 0.3167 - val_loss: 1.1315 - val_sparse_categorical_accuracy: 0.5650 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8943 - sparse_categorical_accuracy: 0.3501 - val_loss: 1.0815 - val_sparse_categorical_accuracy: 0.5980 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.6697 - sparse_categorical_accuracy: 0.3665 - val_loss: 1.0329 - val_sparse_categorical_accuracy: 0.6170 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5996 - sparse_categorical_accuracy: 0.3513 - val_loss: 0.9921 - val_sparse_categorical_accuracy: 0.6270 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3999 - sparse_categorical_accuracy: 0.3882 - val_loss: 0.9578 - val_sparse_categorical_accuracy: 0.6590 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2950 - sparse_categorical_accuracy: 0.3841 - val_loss: 0.9284 - val_sparse_categorical_accuracy: 0.6640 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1689 - sparse_categorical_accuracy: 0.4356 - val_loss: 0.9140 - val_sparse_categorical_accuracy: 0.6620 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0796 - sparse_categorical_accuracy: 0.4110 - val_loss: 0.8736 - val_sparse_categorical_accuracy: 0.6840 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9414 - sparse_categorical_accuracy: 0.4473 - val_loss: 0.8682 - val_sparse_categorical_accuracy: 0.7000 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9136 - sparse_categorical_accuracy: 0.4415 - val_loss: 0.8498 - val_sparse_categorical_accuracy: 0.6960 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8003 - sparse_categorical_accuracy: 0.4602 - val_loss: 0.8146 - val_sparse_categorical_accuracy: 0.7160 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7083 - sparse_categorical_accuracy: 0.4754 - val_loss: 0.8048 - val_sparse_categorical_accuracy: 0.7190 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6273 - sparse_categorical_accuracy: 0.5029 - val_loss: 0.7877 - val_sparse_categorical_accuracy: 0.7120 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6041 - sparse_categorical_accuracy: 0.4988 - val_loss: 0.7819 - val_sparse_categorical_accuracy: 0.7230 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4944 - sparse_categorical_accuracy: 0.5181 - val_loss: 0.7851 - val_sparse_categorical_accuracy: 0.7170 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4128 - sparse_categorical_accuracy: 0.5369 - val_loss: 0.7483 - val_sparse_categorical_accuracy: 0.7410 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4073 - sparse_categorical_accuracy: 0.5422 - val_loss: 0.7294 - val_sparse_categorical_accuracy: 0.7440 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2828 - sparse_categorical_accuracy: 0.5474 - val_loss: 0.7311 - val_sparse_categorical_accuracy: 0.7390 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2902 - sparse_categorical_accuracy: 0.5580 - val_loss: 0.7137 - val_sparse_categorical_accuracy: 0.7570 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2234 - sparse_categorical_accuracy: 0.5638 - val_loss: 0.7055 - val_sparse_categorical_accuracy: 0.7630 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2059 - sparse_categorical_accuracy: 0.5831 - val_loss: 0.6922 - val_sparse_categorical_accuracy: 0.7630 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1237 - sparse_categorical_accuracy: 0.6025 - val_loss: 0.6825 - val_sparse_categorical_accuracy: 0.7630 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0986 - sparse_categorical_accuracy: 0.6165 - val_loss: 0.6797 - val_sparse_categorical_accuracy: 0.7770 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1269 - sparse_categorical_accuracy: 0.6019 - val_loss: 0.6754 - val_sparse_categorical_accuracy: 0.7710 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.0568 - sparse_categorical_accuracy: 0.6282 - val_loss: 0.6661 - val_sparse_categorical_accuracy: 0.7840 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0102 - sparse_categorical_accuracy: 0.6364 - val_loss: 0.6550 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9831 - sparse_categorical_accuracy: 0.6335 - val_loss: 0.6444 - val_sparse_categorical_accuracy: 0.7860 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9522 - sparse_categorical_accuracy: 0.6493 - val_loss: 0.6384 - val_sparse_categorical_accuracy: 0.7870 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9506 - sparse_categorical_accuracy: 0.6475 - val_loss: 0.6319 - val_sparse_categorical_accuracy: 0.7950 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9612 - sparse_categorical_accuracy: 0.6522 - val_loss: 0.6272 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9127 - sparse_categorical_accuracy: 0.6674 - val_loss: 0.6283 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8850 - sparse_categorical_accuracy: 0.6909 - val_loss: 0.6119 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8661 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.6151 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8496 - sparse_categorical_accuracy: 0.6985 - val_loss: 0.6074 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8246 - sparse_categorical_accuracy: 0.7043 - val_loss: 0.6039 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8295 - sparse_categorical_accuracy: 0.6850 - val_loss: 0.5961 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8046 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.5960 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7883 - sparse_categorical_accuracy: 0.7032 - val_loss: 0.5874 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7595 - sparse_categorical_accuracy: 0.7254 - val_loss: 0.5830 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7546 - sparse_categorical_accuracy: 0.7155 - val_loss: 0.5797 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7547 - sparse_categorical_accuracy: 0.7254 - val_loss: 0.5735 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7592 - sparse_categorical_accuracy: 0.7143 - val_loss: 0.5763 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7477 - sparse_categorical_accuracy: 0.7160 - val_loss: 0.5731 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.7218 - sparse_categorical_accuracy: 0.7295 - val_loss: 0.5703 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7265 - sparse_categorical_accuracy: 0.7272 - val_loss: 0.5618 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7078 - sparse_categorical_accuracy: 0.7254 - val_loss: 0.5602 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6809 - sparse_categorical_accuracy: 0.7559 - val_loss: 0.5580 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6879 - sparse_categorical_accuracy: 0.7400 - val_loss: 0.5510 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6718 - sparse_categorical_accuracy: 0.7488 - val_loss: 0.5495 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6753 - sparse_categorical_accuracy: 0.7523 - val_loss: 0.5446 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6793 - sparse_categorical_accuracy: 0.7459 - val_loss: 0.5420 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6557 - sparse_categorical_accuracy: 0.7488 - val_loss: 0.5388 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6398 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5383 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6350 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.5356 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6252 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.5340 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6339 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.5346 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6232 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.5285 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6192 - sparse_categorical_accuracy: 0.7728 - val_loss: 0.5250 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6232 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.5234 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6353 - sparse_categorical_accuracy: 0.7629 - val_loss: 0.5238 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6139 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.5232 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6128 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.5149 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5910 - sparse_categorical_accuracy: 0.7722 - val_loss: 0.5160 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5902 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.5124 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5891 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.5110 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6004 - sparse_categorical_accuracy: 0.7734 - val_loss: 0.5049 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5699 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.5085 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5964 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.5039 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5937 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.5043 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5865 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.5015 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5727 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.4974 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5787 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.4967 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5828 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.4941 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5681 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4951 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5744 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4908 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5571 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4888 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5927 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.4889 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5554 - sparse_categorical_accuracy: 0.7892 - val_loss: 0.4885 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5650 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4915 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5770 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4825 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5442 - sparse_categorical_accuracy: 0.7863 - val_loss: 0.4878 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5452 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.4887 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5426 - sparse_categorical_accuracy: 0.7916 - val_loss: 0.4808 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5346 - sparse_categorical_accuracy: 0.7968 - val_loss: 0.4824 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5343 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.4806 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5416 - sparse_categorical_accuracy: 0.7998 - val_loss: 0.4759 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5268 - sparse_categorical_accuracy: 0.8109 - val_loss: 0.4772 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5334 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4739 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 9, test #0, 2nd iteration: loss of 0.4967051148414612; sparse_categorical_accuracy of 84.20000076293945%\n",
            "appending basis + selective feature vector data\n",
            "[1994, 2290, 1874, 268, 511, 1086, 491, 98, 1785, 2195, 1530, 2509, 1251, 1705, 344, 2662, 281, 2484, 2497, 865, 359, 1819, 2173, 911, 100, 882, 1857, 1415, 239, 86, 2107, 2584, 2292, 1329, 2201, 1744, 522, 2576, 1539, 343, 628, 588, 138, 2355, 826, 2514, 106, 2060, 1664, 858, 1498, 1067, 1967, 135, 885, 1937, 1823, 1915, 2074, 292, 2285, 1866, 992, 713, 2109, 2190, 1100, 1662, 1204, 1396, 173, 970, 1725, 2088, 290, 277, 270, 1144, 843, 543, 927, 1406, 2063, 2454, 1789, 1687, 2521, 450, 1998, 2629, 818, 2702, 602, 495, 783, 426, 2477, 1134, 526, 1371, 1354, 2054, 1078, 1671, 2354, 685, 943, 1159, 1597, 1163, 2689, 1271, 1437, 421, 435, 31, 2661, 1198, 1342, 260, 156, 134, 240, 2447, 1641, 802, 89, 1448, 499, 1337, 1246, 2250, 2392, 2181, 2257, 507, 490, 2253, 2041, 1876, 468, 2359, 617, 1044, 233, 37, 1694, 1810, 53, 1581, 1334, 1768, 821, 1057, 207, 2527, 199, 976, 2369, 1167, 358, 286, 2322, 399, 1129, 1667, 2478, 609, 2495, 425, 664, 1854, 2615, 1654, 2307, 812, 479, 1965, 1631, 2094, 217, 254, 878, 68, 2681, 1422, 228, 657, 2583, 2160, 236, 1451, 500, 2602, 190, 754, 1576, 123, 857, 1733, 101, 2009, 2411, 2427, 1065, 989, 1880, 652, 42, 776, 1376, 2239, 587, 2541, 226, 1070, 346, 920, 647, 1732, 1397, 1464, 2042, 704, 2128, 218, 940, 353, 514, 148, 2441, 2086, 1055, 1410, 2597, 632, 1979, 572, 717, 1752, 1736, 1843, 2450, 1331, 2431, 2665, 2002, 159, 2336, 693, 1605, 1828, 419, 1421, 774, 1181, 1327, 330, 2085, 1683, 171, 1066, 643, 1074, 2612, 274, 1940, 815, 1249, 1096, 707, 206, 2287, 165, 1642, 113, 1412, 0, 1972, 1946, 1678, 2288, 1191, 2421, 558, 1992, 567, 1717, 934, 1578, 848, 579, 1969, 2619, 275, 1475, 1824, 782, 2433, 1128, 192, 2601, 904, 2522, 1447, 1024, 83, 2240, 1345, 1478, 1775, 1172, 645, 1080, 952, 2464, 56, 1681, 757, 820, 1321, 2306, 396, 1684, 1928, 930, 1629, 30, 1298, 1943, 1510, 1332, 1194, 698, 755, 2071, 2609, 1009, 2039, 1726, 1302, 20, 965, 1818, 1402, 2607, 518, 751, 2486, 875, 1123, 551, 795, 1431, 1185, 2564, 541, 2592, 1287, 393, 352, 836, 2549, 1936, 964, 170, 175, 1151, 905, 1427, 853, 444, 137, 1093, 1036, 1634, 1283, 405, 748, 1650, 1216, 143, 345, 420, 1627, 1111, 2364, 1485, 659, 2415, 2289, 1661, 1533, 1280, 1137, 367, 1368, 997, 768, 1574, 2593, 1092, 252, 505, 529, 582, 2508, 2672, 141, 651, 1790, 906, 1976, 2580, 1330, 1846, 2114, 953, 1845, 583, 2204, 926, 811, 109, 2261, 2247, 695, 1753, 303, 886, 1378, 2269, 921, 2703, 936, 1916, 1390, 1784, 2642, 2382, 954, 197, 1982, 1722, 607, 1221, 2295, 577, 1193, 852, 1492, 653, 1166, 2647, 646, 2596, 1319, 2407, 2131, 612, 1821, 2594, 1521, 2457, 599, 2262, 622, 40, 2698, 2545, 1575, 8, 2375, 2624, 868, 1105, 1847, 1112, 92, 2356, 525, 1927, 1908, 978, 1497, 1622, 1190, 174, 1010, 1462, 981, 2334, 1157, 321, 2127, 2548, 1727, 63, 1663, 1179, 1517, 1075, 411, 1981, 209, 384, 1707, 1779, 1557, 966, 2425, 2245, 2581, 1887, 2631, 238, 2338, 181, 2271, 324, 716, 1039, 1016, 1632, 2655, 2083, 2586, 2033, 1848, 1199, 299, 1900, 555, 357, 660, 1306, 66, 2529, 676, 1090, 1208, 234, 722, 2205, 1225, 2032, 1858, 1579, 2663, 447, 2120, 120, 1987, 1741, 2694, 1803, 1814, 1723, 2263, 1951, 2111, 2673, 304, 2153, 1465, 1822, 2333, 1942, 1955, 1468, 93, 2641, 51, 414, 1586, 1718, 1323, 814, 775, 2168, 473, 1379, 1017, 2704, 2423, 1919, 2006, 76, 1734, 1636, 566, 2398, 2418, 1230, 2660, 2274, 1366, 1472, 2341, 198, 250, 769, 319, 413, 1288, 2070, 1188, 2535, 1783, 1505, 2386, 1480, 1115, 1418, 1192, 271, 803, 2023, 283, 1685, 408, 2505, 2019, 1553, 1308, 1155, 1320, 149, 2507, 2110, 2565, 1984, 2349, 917, 1833, 929, 2053, 1322, 699, 1966, 832, 1121, 606, 2223, 1849, 1991, 2158, 837, 1022, 1688, 614, 2164, 1289, 845, 2197, 16, 1207, 1162, 1262, 1551, 736, 1423, 2283, 2363, 1351, 1136, 2012, 2516, 2511, 1160, 1248, 1713, 2001, 933, 1026, 2259, 262, 47, 1006, 601, 1056, 388, 727, 1742, 1083, 1501, 2443, 963, 1279, 916, 1934, 1904, 883, 907, 2301, 2372, 105, 1309, 689, 90, 102, 1310, 1766, 571, 2152, 1046, 2391, 705, 2177, 1122, 1689, 2328, 2401, 760, 730, 1264, 1455, 67, 731, 682, 1508, 2224, 1277, 1518, 855, 711, 191, 1590, 2360, 448, 1881, 2133, 1363, 1961, 2460, 2073, 193, 1293, 2492, 2235, 398, 1053, 1938, 834, 1138, 2150, 1515, 1747, 1615, 247, 2113, 935, 279, 502, 15, 735, 944, 2408, 2455, 2537, 189, 1537, 765, 2065, 690, 1616, 1062, 1232, 1381, 1963, 2126, 136, 1220, 738, 157, 1176, 323, 990, 2543, 2294, 1393, 2487, 472, 2256, 2534, 2620, 959, 888, 1829, 1691, 824, 520, 2134, 1417, 1324, 2573, 2482, 80, 1276, 1639, 1791, 1610, 1906, 1068, 937, 1284, 2305, 1486, 2044, 2368, 1350, 1419, 683, 7, 1834, 2136, 1399, 2618, 2025, 654, 118, 2621, 838, 741, 1012, 807, 1656, 884, 2144, 1404, 1269, 1401, 2686, 1607, 877, 2646, 1898, 1132, 27, 661, 2196, 2174, 2286, 427, 2059, 671, 2102, 471, 1620, 817, 2707, 2657, 2366, 2161, 2142, 2377, 1467, 2254, 1939, 1686, 23, 1416, 1295, 70, 1373, 82, 115, 1903, 1815, 293, 2213, 1059, 2649, 85, 1695, 527, 2608, 1265, 1300, 870, 792, 931, 2129, 2696, 2388, 823, 861, 2216, 813, 1336, 1614, 409, 1143, 1812, 627, 1787, 1020, 806, 1142, 1825, 1503, 375, 1569, 1282, 2176, 1347, 1680, 1878, 2532, 2297, 245, 129, 2034, 79, 1147, 2118, 1835, 1897, 476, 1885, 2246, 164, 467, 1658, 482, 36, 1535, 2605, 1651, 2406, 1905, 1041, 1403, 2078, 322, 152, 1349, 487, 610, 64, 2384, 1802, 465, 556, 2329, 761, 2599, 320, 81, 1069, 2680, 1988, 354, 590, 932, 489, 1930, 1079, 1592, 2659, 1580, 973, 284, 107, 2299, 2687, 2439, 1702, 1211, 611, 2311, 43, 697, 1820, 1077, 2590, 2480, 742, 2149, 146, 1646, 2588, 825, 1548, 1907, 394, 1529, 1700, 2562, 844, 2547, 1980, 1601, 2140, 272, 575, 25, 899, 317, 1247]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 11.7801 - sparse_categorical_accuracy: 0.1376 - val_loss: 2.7780 - val_sparse_categorical_accuracy: 0.1680 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 8.4327 - sparse_categorical_accuracy: 0.1604 - val_loss: 3.3808 - val_sparse_categorical_accuracy: 0.2780 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.7867 - sparse_categorical_accuracy: 0.1932 - val_loss: 3.0281 - val_sparse_categorical_accuracy: 0.2850 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.9917 - sparse_categorical_accuracy: 0.2108 - val_loss: 2.6949 - val_sparse_categorical_accuracy: 0.2900 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.7413 - sparse_categorical_accuracy: 0.2102 - val_loss: 2.5685 - val_sparse_categorical_accuracy: 0.3060 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.2403 - sparse_categorical_accuracy: 0.2002 - val_loss: 2.5279 - val_sparse_categorical_accuracy: 0.3030 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.8327 - sparse_categorical_accuracy: 0.2067 - val_loss: 2.2197 - val_sparse_categorical_accuracy: 0.3190 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.4709 - sparse_categorical_accuracy: 0.2178 - val_loss: 2.2169 - val_sparse_categorical_accuracy: 0.3280 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.1531 - sparse_categorical_accuracy: 0.2096 - val_loss: 2.1782 - val_sparse_categorical_accuracy: 0.3290 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.6706 - sparse_categorical_accuracy: 0.2541 - val_loss: 1.8896 - val_sparse_categorical_accuracy: 0.3590 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.4437 - sparse_categorical_accuracy: 0.2389 - val_loss: 1.6948 - val_sparse_categorical_accuracy: 0.3920 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.1869 - sparse_categorical_accuracy: 0.2594 - val_loss: 1.5515 - val_sparse_categorical_accuracy: 0.4370 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.8182 - sparse_categorical_accuracy: 0.2687 - val_loss: 1.5503 - val_sparse_categorical_accuracy: 0.4100 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5406 - sparse_categorical_accuracy: 0.2904 - val_loss: 1.4751 - val_sparse_categorical_accuracy: 0.4570 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.2857 - sparse_categorical_accuracy: 0.3056 - val_loss: 1.4465 - val_sparse_categorical_accuracy: 0.4460 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1038 - sparse_categorical_accuracy: 0.3214 - val_loss: 1.3287 - val_sparse_categorical_accuracy: 0.4960 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.9646 - sparse_categorical_accuracy: 0.3226 - val_loss: 1.2668 - val_sparse_categorical_accuracy: 0.4980 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8197 - sparse_categorical_accuracy: 0.3302 - val_loss: 1.2279 - val_sparse_categorical_accuracy: 0.5080 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.6315 - sparse_categorical_accuracy: 0.3361 - val_loss: 1.2101 - val_sparse_categorical_accuracy: 0.5270 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3530 - sparse_categorical_accuracy: 0.3817 - val_loss: 1.1426 - val_sparse_categorical_accuracy: 0.5550 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2327 - sparse_categorical_accuracy: 0.3970 - val_loss: 1.1143 - val_sparse_categorical_accuracy: 0.5690 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1724 - sparse_categorical_accuracy: 0.3970 - val_loss: 1.0770 - val_sparse_categorical_accuracy: 0.5900 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 2.0659 - sparse_categorical_accuracy: 0.4133 - val_loss: 1.0196 - val_sparse_categorical_accuracy: 0.6160 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 2.0773 - sparse_categorical_accuracy: 0.4075 - val_loss: 0.9929 - val_sparse_categorical_accuracy: 0.6330 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.9418 - sparse_categorical_accuracy: 0.4204 - val_loss: 0.9692 - val_sparse_categorical_accuracy: 0.6510 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8673 - sparse_categorical_accuracy: 0.4268 - val_loss: 0.9195 - val_sparse_categorical_accuracy: 0.6750 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7885 - sparse_categorical_accuracy: 0.4502 - val_loss: 0.9162 - val_sparse_categorical_accuracy: 0.6670 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6575 - sparse_categorical_accuracy: 0.4660 - val_loss: 0.9051 - val_sparse_categorical_accuracy: 0.6710 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6007 - sparse_categorical_accuracy: 0.4947 - val_loss: 0.8595 - val_sparse_categorical_accuracy: 0.7040 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5330 - sparse_categorical_accuracy: 0.5076 - val_loss: 0.8696 - val_sparse_categorical_accuracy: 0.6990 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.5292 - sparse_categorical_accuracy: 0.5012 - val_loss: 0.8311 - val_sparse_categorical_accuracy: 0.7150 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4484 - sparse_categorical_accuracy: 0.5000 - val_loss: 0.8232 - val_sparse_categorical_accuracy: 0.7140 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2950 - sparse_categorical_accuracy: 0.5427 - val_loss: 0.8055 - val_sparse_categorical_accuracy: 0.7290 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2880 - sparse_categorical_accuracy: 0.5533 - val_loss: 0.7844 - val_sparse_categorical_accuracy: 0.7420 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3075 - sparse_categorical_accuracy: 0.5357 - val_loss: 0.7694 - val_sparse_categorical_accuracy: 0.7450 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1667 - sparse_categorical_accuracy: 0.5837 - val_loss: 0.7636 - val_sparse_categorical_accuracy: 0.7510 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2328 - sparse_categorical_accuracy: 0.5632 - val_loss: 0.7492 - val_sparse_categorical_accuracy: 0.7570 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1731 - sparse_categorical_accuracy: 0.5714 - val_loss: 0.7440 - val_sparse_categorical_accuracy: 0.7580 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1432 - sparse_categorical_accuracy: 0.5890 - val_loss: 0.7321 - val_sparse_categorical_accuracy: 0.7710 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0895 - sparse_categorical_accuracy: 0.6077 - val_loss: 0.7265 - val_sparse_categorical_accuracy: 0.7670 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0657 - sparse_categorical_accuracy: 0.6095 - val_loss: 0.7132 - val_sparse_categorical_accuracy: 0.7760 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0566 - sparse_categorical_accuracy: 0.6142 - val_loss: 0.7088 - val_sparse_categorical_accuracy: 0.7710 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0416 - sparse_categorical_accuracy: 0.6095 - val_loss: 0.7004 - val_sparse_categorical_accuracy: 0.7750 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9668 - sparse_categorical_accuracy: 0.6440 - val_loss: 0.6916 - val_sparse_categorical_accuracy: 0.7820 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0015 - sparse_categorical_accuracy: 0.6306 - val_loss: 0.6786 - val_sparse_categorical_accuracy: 0.7850 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9358 - sparse_categorical_accuracy: 0.6569 - val_loss: 0.6712 - val_sparse_categorical_accuracy: 0.7890 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9081 - sparse_categorical_accuracy: 0.6739 - val_loss: 0.6624 - val_sparse_categorical_accuracy: 0.7950 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8518 - sparse_categorical_accuracy: 0.6897 - val_loss: 0.6588 - val_sparse_categorical_accuracy: 0.7920 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9166 - sparse_categorical_accuracy: 0.6598 - val_loss: 0.6543 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8958 - sparse_categorical_accuracy: 0.6657 - val_loss: 0.6553 - val_sparse_categorical_accuracy: 0.7820 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8474 - sparse_categorical_accuracy: 0.6809 - val_loss: 0.6421 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8632 - sparse_categorical_accuracy: 0.6569 - val_loss: 0.6373 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8238 - sparse_categorical_accuracy: 0.7020 - val_loss: 0.6304 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7854 - sparse_categorical_accuracy: 0.7008 - val_loss: 0.6239 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7969 - sparse_categorical_accuracy: 0.7073 - val_loss: 0.6203 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7842 - sparse_categorical_accuracy: 0.7049 - val_loss: 0.6146 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7633 - sparse_categorical_accuracy: 0.7061 - val_loss: 0.6100 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7613 - sparse_categorical_accuracy: 0.7219 - val_loss: 0.6067 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7511 - sparse_categorical_accuracy: 0.7114 - val_loss: 0.6006 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7402 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5970 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7556 - sparse_categorical_accuracy: 0.7184 - val_loss: 0.5947 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7238 - sparse_categorical_accuracy: 0.7301 - val_loss: 0.5867 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7193 - sparse_categorical_accuracy: 0.7354 - val_loss: 0.5843 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7210 - sparse_categorical_accuracy: 0.7342 - val_loss: 0.5798 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7078 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.5775 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6915 - sparse_categorical_accuracy: 0.7377 - val_loss: 0.5731 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6874 - sparse_categorical_accuracy: 0.7500 - val_loss: 0.5730 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6828 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.5647 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6721 - sparse_categorical_accuracy: 0.7488 - val_loss: 0.5617 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6631 - sparse_categorical_accuracy: 0.7453 - val_loss: 0.5598 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6381 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.5567 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6387 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.5514 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6522 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.5465 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6648 - sparse_categorical_accuracy: 0.7430 - val_loss: 0.5469 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6459 - sparse_categorical_accuracy: 0.7500 - val_loss: 0.5459 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6598 - sparse_categorical_accuracy: 0.7523 - val_loss: 0.5399 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6560 - sparse_categorical_accuracy: 0.7570 - val_loss: 0.5421 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6571 - sparse_categorical_accuracy: 0.7465 - val_loss: 0.5345 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6297 - sparse_categorical_accuracy: 0.7570 - val_loss: 0.5386 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6143 - sparse_categorical_accuracy: 0.7658 - val_loss: 0.5302 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6028 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.5286 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6241 - sparse_categorical_accuracy: 0.7693 - val_loss: 0.5255 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6024 - sparse_categorical_accuracy: 0.7728 - val_loss: 0.5224 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6088 - sparse_categorical_accuracy: 0.7699 - val_loss: 0.5220 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6040 - sparse_categorical_accuracy: 0.7758 - val_loss: 0.5197 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5924 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.5165 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6127 - sparse_categorical_accuracy: 0.7600 - val_loss: 0.5161 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5768 - sparse_categorical_accuracy: 0.7758 - val_loss: 0.5120 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5853 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.5120 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5919 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.5094 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5694 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.5068 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5616 - sparse_categorical_accuracy: 0.7927 - val_loss: 0.5037 - val_sparse_categorical_accuracy: 0.8390 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5660 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.5059 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5666 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.5009 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5774 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.4991 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5667 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4967 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5463 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4938 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5661 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.4990 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5775 - sparse_categorical_accuracy: 0.7822 - val_loss: 0.4931 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5351 - sparse_categorical_accuracy: 0.8021 - val_loss: 0.4889 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 9, test #1, 2nd iteration: loss of 0.4966595768928528; sparse_categorical_accuracy of 84.20000076293945%\n",
            "appending basis + selective feature vector data\n",
            "[1994, 2290, 1874, 268, 511, 1086, 491, 98, 1785, 2195, 1530, 2509, 1251, 1705, 344, 2662, 281, 2484, 2497, 865, 359, 1819, 2173, 911, 100, 882, 1857, 1415, 239, 86, 2107, 2584, 2292, 1329, 2201, 1744, 522, 2576, 1539, 343, 628, 588, 138, 2355, 826, 2514, 106, 2060, 1664, 858, 1498, 1067, 1967, 135, 885, 1937, 1823, 1915, 2074, 292, 2285, 1866, 992, 713, 2109, 2190, 1100, 1662, 1204, 1396, 173, 970, 1725, 2088, 290, 277, 270, 1144, 843, 543, 927, 1406, 2063, 2454, 1789, 1687, 2521, 450, 1998, 2629, 818, 2702, 602, 495, 783, 426, 2477, 1134, 526, 1371, 1354, 2054, 1078, 1671, 2354, 685, 943, 1159, 1597, 1163, 2689, 1271, 1437, 421, 435, 31, 2661, 1198, 1342, 260, 156, 134, 240, 2447, 1641, 802, 89, 1448, 499, 1337, 1246, 2250, 2392, 2181, 2257, 507, 490, 2253, 2041, 1876, 468, 2359, 617, 1044, 233, 37, 1694, 1810, 53, 1581, 1334, 1768, 821, 1057, 207, 2527, 199, 976, 2369, 1167, 358, 286, 2322, 399, 1129, 1667, 2478, 609, 2495, 425, 664, 1854, 2615, 1654, 2307, 812, 479, 1965, 1631, 2094, 217, 254, 878, 68, 2681, 1422, 228, 657, 2583, 2160, 236, 1451, 500, 2602, 190, 754, 1576, 123, 857, 1733, 101, 2009, 2411, 2427, 1065, 989, 1880, 652, 42, 776, 1376, 2239, 587, 2541, 226, 1070, 346, 920, 647, 1732, 1397, 1464, 2042, 704, 2128, 218, 940, 353, 514, 148, 2441, 2086, 1055, 1410, 2597, 632, 1979, 572, 717, 1752, 1736, 1843, 2450, 1331, 2431, 2665, 2002, 159, 2336, 693, 1605, 1828, 419, 1421, 774, 1181, 1327, 330, 2085, 1683, 171, 1066, 643, 1074, 2612, 274, 1940, 815, 1249, 1096, 707, 206, 2287, 165, 1642, 113, 1412, 0, 1972, 1946, 1678, 2288, 1191, 2421, 558, 1992, 567, 1717, 934, 1578, 848, 579, 1969, 2619, 275, 1475, 1824, 782, 2433, 1128, 192, 2601, 904, 2522, 1447, 1024, 83, 2240, 1345, 1478, 1775, 1172, 645, 1080, 952, 2464, 56, 1681, 757, 820, 1321, 2306, 396, 1684, 1928, 930, 1629, 30, 1298, 1943, 1510, 1332, 1194, 698, 755, 2071, 2609, 1009, 2039, 1726, 1302, 20, 965, 1818, 1402, 2607, 518, 751, 2486, 875, 1123, 551, 795, 1431, 1185, 2564, 541, 2592, 1287, 393, 352, 836, 2549, 1936, 964, 170, 175, 1151, 905, 1427, 853, 444, 137, 1093, 1036, 1634, 1283, 405, 748, 1650, 1216, 143, 345, 420, 1627, 1111, 2364, 1485, 659, 2415, 2289, 1661, 1533, 1280, 1137, 367, 1368, 997, 768, 1574, 2593, 1092, 252, 505, 529, 582, 2508, 2672, 141, 651, 1790, 906, 1976, 2580, 1330, 1846, 2114, 953, 1845, 583, 2204, 926, 811, 109, 2261, 2247, 695, 1753, 303, 886, 1378, 2269, 921, 2703, 936, 1916, 1390, 1784, 2642, 2382, 954, 197, 1982, 1722, 607, 1221, 2295, 577, 1193, 852, 1492, 653, 1166, 2647, 646, 2596, 1319, 2407, 2131, 612, 1821, 2594, 1521, 2457, 599, 2262, 622, 40, 2698, 2545, 1575, 8, 2375, 2624, 868, 1105, 1847, 1112, 92, 2356, 525, 1927, 1908, 978, 1497, 1622, 1190, 174, 1010, 1462, 981, 2334, 1157, 321, 2127, 2548, 1727, 63, 1663, 1179, 1517, 1075, 411, 1981, 209, 384, 1707, 1779, 1557, 966, 2425, 2245, 2581, 1887, 2631, 238, 2338, 181, 2271, 324, 716, 1039, 1016, 1632, 2655, 2083, 2586, 2033, 1848, 1199, 299, 1900, 555, 357, 660, 1306, 66, 2529, 676, 1090, 1208, 234, 722, 2205, 1225, 2032, 1858, 1579, 2663, 447, 2120, 120, 1987, 1741, 2694, 1803, 1814, 1723, 2263, 1951, 2111, 2673, 304, 2153, 1465, 1822, 2333, 1942, 1955, 1468, 93, 2641, 51, 414, 1586, 1718, 1323, 814, 775, 2168, 473, 1379, 1017, 2704, 2423, 1919, 2006, 76, 1734, 1636, 566, 2398, 2418, 1230, 2660, 2274, 1366, 1472, 2341, 198, 250, 769, 319, 413, 1288, 2070, 1188, 2535, 1783, 1505, 2386, 1480, 1115, 1418, 1192, 271, 803, 2023, 283, 1685, 408, 2505, 2019, 1553, 1308, 1155, 1320, 149, 2507, 2110, 2565, 1984, 2349, 917, 1833, 929, 2053, 1322, 699, 1966, 832, 1121, 606, 2223, 1849, 1991, 2158, 837, 1022, 1688, 614, 2164, 1289, 845, 2197, 16, 1207, 1162, 1262, 1551, 736, 1423, 2283, 2363, 1351, 1136, 2012, 2516, 2511, 1160, 1248, 1713, 2001, 933, 1026, 2259, 262, 47, 1006, 601, 1056, 388, 727, 1742, 1083, 1501, 2443, 963, 1279, 916, 1934, 1904, 883, 907, 2301, 2372, 105, 1309, 689, 90, 102, 1310, 1766, 571, 2152, 1046, 2391, 705, 2177, 1122, 1689, 2328, 2401, 760, 730, 1264, 1455, 67, 731, 682, 1508, 2224, 1277, 1518, 855, 711, 191, 1590, 2360, 448, 1881, 2133, 1363, 1961, 2460, 2073, 193, 1293, 2492, 2235, 398, 1053, 1938, 834, 1138, 2150, 1515, 1747, 1615, 247, 2113, 935, 279, 502, 15, 735, 944, 2408, 2455, 2537, 189, 1537, 765, 2065, 690, 1616, 1062, 1232, 1381, 1963, 2126, 136, 1220, 738, 157, 1176, 323, 990, 2543, 2294, 1393, 2487, 472, 2256, 2534, 2620, 959, 888, 1829, 1691, 824, 520, 2134, 1417, 1324, 2573, 2482, 80, 1276, 1639, 1791, 1610, 1906, 1068, 937, 1284, 2305, 1486, 2044, 2368, 1350, 1419, 683, 7, 1834, 2136, 1399, 2618, 2025, 654, 118, 2621, 838, 741, 1012, 807, 1656, 884, 2144, 1404, 1269, 1401, 2686, 1607, 877, 2646, 1898, 1132, 27, 661, 2196, 2174, 2286, 427, 2059, 671, 2102, 471, 1620, 817, 2707, 2657, 2366, 2161, 2142, 2377, 1467, 2254, 1939, 1686, 23, 1416, 1295, 70, 1373, 82, 115, 1903, 1815, 293, 2213, 1059, 2649, 85, 1695, 527, 2608, 1265, 1300, 870, 792, 931, 2129, 2696, 2388, 823, 861, 2216, 813, 1336, 1614, 409, 1143, 1812, 627, 1787, 1020, 806, 1142, 1825, 1503, 375, 1569, 1282, 2176, 1347, 1680, 1878, 2532, 2297, 245, 129, 2034, 79, 1147, 2118, 1835, 1897, 476, 1885, 2246, 164, 467, 1658, 482, 36, 1535, 2605, 1651, 2406, 1905, 1041, 1403, 2078, 322, 152, 1349, 487, 610, 64, 2384, 1802, 465, 556, 2329, 761, 2599, 320, 81, 1069, 2680, 1988, 354, 590, 932, 489, 1930, 1079, 1592, 2659, 1580, 973, 284, 107, 2299, 2687, 2439, 1702, 1211, 611, 2311, 43, 697, 1820, 1077, 2590, 2480, 742, 2149, 146, 1646, 2588, 825, 1548, 1907, 394, 1529, 1700, 2562, 844, 2547, 1980, 1601, 2140, 272, 575, 25, 899, 317, 1247]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 2s 5ms/step - loss: 9.1879 - sparse_categorical_accuracy: 0.1698 - val_loss: 3.4439 - val_sparse_categorical_accuracy: 0.2900 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.3870 - sparse_categorical_accuracy: 0.1856 - val_loss: 3.1112 - val_sparse_categorical_accuracy: 0.2880 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.9067 - sparse_categorical_accuracy: 0.1874 - val_loss: 2.9149 - val_sparse_categorical_accuracy: 0.2920 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.2103 - sparse_categorical_accuracy: 0.1944 - val_loss: 2.3328 - val_sparse_categorical_accuracy: 0.2890 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 6.1342 - sparse_categorical_accuracy: 0.1803 - val_loss: 2.3888 - val_sparse_categorical_accuracy: 0.2940 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.5486 - sparse_categorical_accuracy: 0.2049 - val_loss: 2.2739 - val_sparse_categorical_accuracy: 0.2950 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.8687 - sparse_categorical_accuracy: 0.2436 - val_loss: 2.2062 - val_sparse_categorical_accuracy: 0.3000 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.6273 - sparse_categorical_accuracy: 0.2348 - val_loss: 1.9919 - val_sparse_categorical_accuracy: 0.3420 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.4021 - sparse_categorical_accuracy: 0.2131 - val_loss: 1.8106 - val_sparse_categorical_accuracy: 0.3300 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.9999 - sparse_categorical_accuracy: 0.2377 - val_loss: 1.6548 - val_sparse_categorical_accuracy: 0.3530 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.7524 - sparse_categorical_accuracy: 0.2600 - val_loss: 1.7071 - val_sparse_categorical_accuracy: 0.3540 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.4771 - sparse_categorical_accuracy: 0.2623 - val_loss: 1.4332 - val_sparse_categorical_accuracy: 0.4650 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.2700 - sparse_categorical_accuracy: 0.2822 - val_loss: 1.4811 - val_sparse_categorical_accuracy: 0.4150 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.1489 - sparse_categorical_accuracy: 0.3074 - val_loss: 1.3692 - val_sparse_categorical_accuracy: 0.4520 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8945 - sparse_categorical_accuracy: 0.3085 - val_loss: 1.2852 - val_sparse_categorical_accuracy: 0.4930 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.6398 - sparse_categorical_accuracy: 0.3132 - val_loss: 1.1774 - val_sparse_categorical_accuracy: 0.5670 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3636 - sparse_categorical_accuracy: 0.3548 - val_loss: 1.1372 - val_sparse_categorical_accuracy: 0.5730 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3311 - sparse_categorical_accuracy: 0.3577 - val_loss: 1.0880 - val_sparse_categorical_accuracy: 0.5990 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2259 - sparse_categorical_accuracy: 0.3946 - val_loss: 1.0287 - val_sparse_categorical_accuracy: 0.6270 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.1908 - sparse_categorical_accuracy: 0.3794 - val_loss: 1.0389 - val_sparse_categorical_accuracy: 0.6030 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0461 - sparse_categorical_accuracy: 0.3806 - val_loss: 1.0063 - val_sparse_categorical_accuracy: 0.6320 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9360 - sparse_categorical_accuracy: 0.4087 - val_loss: 0.9276 - val_sparse_categorical_accuracy: 0.6960 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8399 - sparse_categorical_accuracy: 0.4104 - val_loss: 0.9092 - val_sparse_categorical_accuracy: 0.7090 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6234 - sparse_categorical_accuracy: 0.4678 - val_loss: 0.8994 - val_sparse_categorical_accuracy: 0.6980 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6068 - sparse_categorical_accuracy: 0.4666 - val_loss: 0.8725 - val_sparse_categorical_accuracy: 0.7090 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5576 - sparse_categorical_accuracy: 0.4660 - val_loss: 0.8687 - val_sparse_categorical_accuracy: 0.7040 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5053 - sparse_categorical_accuracy: 0.4953 - val_loss: 0.8395 - val_sparse_categorical_accuracy: 0.7320 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4221 - sparse_categorical_accuracy: 0.5228 - val_loss: 0.8327 - val_sparse_categorical_accuracy: 0.7280 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3408 - sparse_categorical_accuracy: 0.5287 - val_loss: 0.8159 - val_sparse_categorical_accuracy: 0.7300 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.3007 - sparse_categorical_accuracy: 0.5504 - val_loss: 0.7925 - val_sparse_categorical_accuracy: 0.7480 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2441 - sparse_categorical_accuracy: 0.5603 - val_loss: 0.7793 - val_sparse_categorical_accuracy: 0.7450 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2256 - sparse_categorical_accuracy: 0.5685 - val_loss: 0.7627 - val_sparse_categorical_accuracy: 0.7570 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1633 - sparse_categorical_accuracy: 0.5867 - val_loss: 0.7485 - val_sparse_categorical_accuracy: 0.7640 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1061 - sparse_categorical_accuracy: 0.5902 - val_loss: 0.7463 - val_sparse_categorical_accuracy: 0.7600 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0946 - sparse_categorical_accuracy: 0.6124 - val_loss: 0.7356 - val_sparse_categorical_accuracy: 0.7630 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0372 - sparse_categorical_accuracy: 0.6259 - val_loss: 0.7205 - val_sparse_categorical_accuracy: 0.7660 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0362 - sparse_categorical_accuracy: 0.6282 - val_loss: 0.7152 - val_sparse_categorical_accuracy: 0.7710 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9625 - sparse_categorical_accuracy: 0.6464 - val_loss: 0.7052 - val_sparse_categorical_accuracy: 0.7750 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9733 - sparse_categorical_accuracy: 0.6370 - val_loss: 0.6922 - val_sparse_categorical_accuracy: 0.7770 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9670 - sparse_categorical_accuracy: 0.6475 - val_loss: 0.6953 - val_sparse_categorical_accuracy: 0.7820 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9533 - sparse_categorical_accuracy: 0.6651 - val_loss: 0.6756 - val_sparse_categorical_accuracy: 0.7750 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8979 - sparse_categorical_accuracy: 0.6692 - val_loss: 0.6673 - val_sparse_categorical_accuracy: 0.7820 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8779 - sparse_categorical_accuracy: 0.6751 - val_loss: 0.6634 - val_sparse_categorical_accuracy: 0.7890 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8832 - sparse_categorical_accuracy: 0.6786 - val_loss: 0.6620 - val_sparse_categorical_accuracy: 0.7920 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8552 - sparse_categorical_accuracy: 0.6751 - val_loss: 0.6548 - val_sparse_categorical_accuracy: 0.7870 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.8159 - sparse_categorical_accuracy: 0.7002 - val_loss: 0.6442 - val_sparse_categorical_accuracy: 0.7870 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8490 - sparse_categorical_accuracy: 0.6961 - val_loss: 0.6416 - val_sparse_categorical_accuracy: 0.7950 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8190 - sparse_categorical_accuracy: 0.6891 - val_loss: 0.6318 - val_sparse_categorical_accuracy: 0.7930 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7940 - sparse_categorical_accuracy: 0.7196 - val_loss: 0.6279 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7920 - sparse_categorical_accuracy: 0.7002 - val_loss: 0.6244 - val_sparse_categorical_accuracy: 0.7870 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7723 - sparse_categorical_accuracy: 0.7237 - val_loss: 0.6141 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7344 - sparse_categorical_accuracy: 0.7377 - val_loss: 0.6159 - val_sparse_categorical_accuracy: 0.7950 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7330 - sparse_categorical_accuracy: 0.7248 - val_loss: 0.6104 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7331 - sparse_categorical_accuracy: 0.7219 - val_loss: 0.6024 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7257 - sparse_categorical_accuracy: 0.7348 - val_loss: 0.5957 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7137 - sparse_categorical_accuracy: 0.7406 - val_loss: 0.5893 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7117 - sparse_categorical_accuracy: 0.7418 - val_loss: 0.5845 - val_sparse_categorical_accuracy: 0.8040 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6999 - sparse_categorical_accuracy: 0.7354 - val_loss: 0.5813 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7067 - sparse_categorical_accuracy: 0.7471 - val_loss: 0.5776 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6628 - sparse_categorical_accuracy: 0.7547 - val_loss: 0.5748 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6636 - sparse_categorical_accuracy: 0.7465 - val_loss: 0.5700 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6820 - sparse_categorical_accuracy: 0.7430 - val_loss: 0.5651 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6727 - sparse_categorical_accuracy: 0.7553 - val_loss: 0.5613 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6562 - sparse_categorical_accuracy: 0.7541 - val_loss: 0.5610 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6369 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.5552 - val_sparse_categorical_accuracy: 0.8090 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6355 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.5552 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6273 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5484 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6185 - sparse_categorical_accuracy: 0.7758 - val_loss: 0.5447 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6362 - sparse_categorical_accuracy: 0.7547 - val_loss: 0.5413 - val_sparse_categorical_accuracy: 0.8180 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6193 - sparse_categorical_accuracy: 0.7758 - val_loss: 0.5392 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6070 - sparse_categorical_accuracy: 0.7763 - val_loss: 0.5392 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5869 - sparse_categorical_accuracy: 0.7904 - val_loss: 0.5331 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6353 - sparse_categorical_accuracy: 0.7635 - val_loss: 0.5319 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6032 - sparse_categorical_accuracy: 0.7728 - val_loss: 0.5266 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5871 - sparse_categorical_accuracy: 0.7916 - val_loss: 0.5269 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5941 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.5232 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5863 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.5234 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5864 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.5209 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5793 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.5151 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5602 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.5118 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5659 - sparse_categorical_accuracy: 0.8033 - val_loss: 0.5079 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5992 - sparse_categorical_accuracy: 0.7740 - val_loss: 0.5080 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.5905 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.5067 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5580 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.5082 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5700 - sparse_categorical_accuracy: 0.7851 - val_loss: 0.5037 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5517 - sparse_categorical_accuracy: 0.7957 - val_loss: 0.4995 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5495 - sparse_categorical_accuracy: 0.8004 - val_loss: 0.4977 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5434 - sparse_categorical_accuracy: 0.7980 - val_loss: 0.4967 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5667 - sparse_categorical_accuracy: 0.7799 - val_loss: 0.4945 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5293 - sparse_categorical_accuracy: 0.8062 - val_loss: 0.4913 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5562 - sparse_categorical_accuracy: 0.7992 - val_loss: 0.4911 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5547 - sparse_categorical_accuracy: 0.7927 - val_loss: 0.4883 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5355 - sparse_categorical_accuracy: 0.7951 - val_loss: 0.4861 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5479 - sparse_categorical_accuracy: 0.7933 - val_loss: 0.4899 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5268 - sparse_categorical_accuracy: 0.8004 - val_loss: 0.4809 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5454 - sparse_categorical_accuracy: 0.7939 - val_loss: 0.4815 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5338 - sparse_categorical_accuracy: 0.7922 - val_loss: 0.4793 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5305 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4848 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5328 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4805 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5420 - sparse_categorical_accuracy: 0.7986 - val_loss: 0.4744 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 9, test #2, 2nd iteration: loss of 0.4815085530281067; sparse_categorical_accuracy of 84.20000076293945%\n",
            "appending basis + selective feature vector data\n",
            "[1994, 2290, 1874, 268, 511, 1086, 491, 98, 1785, 2195, 1530, 2509, 1251, 1705, 344, 2662, 281, 2484, 2497, 865, 359, 1819, 2173, 911, 100, 882, 1857, 1415, 239, 86, 2107, 2584, 2292, 1329, 2201, 1744, 522, 2576, 1539, 343, 628, 588, 138, 2355, 826, 2514, 106, 2060, 1664, 858, 1498, 1067, 1967, 135, 885, 1937, 1823, 1915, 2074, 292, 2285, 1866, 992, 713, 2109, 2190, 1100, 1662, 1204, 1396, 173, 970, 1725, 2088, 290, 277, 270, 1144, 843, 543, 927, 1406, 2063, 2454, 1789, 1687, 2521, 450, 1998, 2629, 818, 2702, 602, 495, 783, 426, 2477, 1134, 526, 1371, 1354, 2054, 1078, 1671, 2354, 685, 943, 1159, 1597, 1163, 2689, 1271, 1437, 421, 435, 31, 2661, 1198, 1342, 260, 156, 134, 240, 2447, 1641, 802, 89, 1448, 499, 1337, 1246, 2250, 2392, 2181, 2257, 507, 490, 2253, 2041, 1876, 468, 2359, 617, 1044, 233, 37, 1694, 1810, 53, 1581, 1334, 1768, 821, 1057, 207, 2527, 199, 976, 2369, 1167, 358, 286, 2322, 399, 1129, 1667, 2478, 609, 2495, 425, 664, 1854, 2615, 1654, 2307, 812, 479, 1965, 1631, 2094, 217, 254, 878, 68, 2681, 1422, 228, 657, 2583, 2160, 236, 1451, 500, 2602, 190, 754, 1576, 123, 857, 1733, 101, 2009, 2411, 2427, 1065, 989, 1880, 652, 42, 776, 1376, 2239, 587, 2541, 226, 1070, 346, 920, 647, 1732, 1397, 1464, 2042, 704, 2128, 218, 940, 353, 514, 148, 2441, 2086, 1055, 1410, 2597, 632, 1979, 572, 717, 1752, 1736, 1843, 2450, 1331, 2431, 2665, 2002, 159, 2336, 693, 1605, 1828, 419, 1421, 774, 1181, 1327, 330, 2085, 1683, 171, 1066, 643, 1074, 2612, 274, 1940, 815, 1249, 1096, 707, 206, 2287, 165, 1642, 113, 1412, 0, 1972, 1946, 1678, 2288, 1191, 2421, 558, 1992, 567, 1717, 934, 1578, 848, 579, 1969, 2619, 275, 1475, 1824, 782, 2433, 1128, 192, 2601, 904, 2522, 1447, 1024, 83, 2240, 1345, 1478, 1775, 1172, 645, 1080, 952, 2464, 56, 1681, 757, 820, 1321, 2306, 396, 1684, 1928, 930, 1629, 30, 1298, 1943, 1510, 1332, 1194, 698, 755, 2071, 2609, 1009, 2039, 1726, 1302, 20, 965, 1818, 1402, 2607, 518, 751, 2486, 875, 1123, 551, 795, 1431, 1185, 2564, 541, 2592, 1287, 393, 352, 836, 2549, 1936, 964, 170, 175, 1151, 905, 1427, 853, 444, 137, 1093, 1036, 1634, 1283, 405, 748, 1650, 1216, 143, 345, 420, 1627, 1111, 2364, 1485, 659, 2415, 2289, 1661, 1533, 1280, 1137, 367, 1368, 997, 768, 1574, 2593, 1092, 252, 505, 529, 582, 2508, 2672, 141, 651, 1790, 906, 1976, 2580, 1330, 1846, 2114, 953, 1845, 583, 2204, 926, 811, 109, 2261, 2247, 695, 1753, 303, 886, 1378, 2269, 921, 2703, 936, 1916, 1390, 1784, 2642, 2382, 954, 197, 1982, 1722, 607, 1221, 2295, 577, 1193, 852, 1492, 653, 1166, 2647, 646, 2596, 1319, 2407, 2131, 612, 1821, 2594, 1521, 2457, 599, 2262, 622, 40, 2698, 2545, 1575, 8, 2375, 2624, 868, 1105, 1847, 1112, 92, 2356, 525, 1927, 1908, 978, 1497, 1622, 1190, 174, 1010, 1462, 981, 2334, 1157, 321, 2127, 2548, 1727, 63, 1663, 1179, 1517, 1075, 411, 1981, 209, 384, 1707, 1779, 1557, 966, 2425, 2245, 2581, 1887, 2631, 238, 2338, 181, 2271, 324, 716, 1039, 1016, 1632, 2655, 2083, 2586, 2033, 1848, 1199, 299, 1900, 555, 357, 660, 1306, 66, 2529, 676, 1090, 1208, 234, 722, 2205, 1225, 2032, 1858, 1579, 2663, 447, 2120, 120, 1987, 1741, 2694, 1803, 1814, 1723, 2263, 1951, 2111, 2673, 304, 2153, 1465, 1822, 2333, 1942, 1955, 1468, 93, 2641, 51, 414, 1586, 1718, 1323, 814, 775, 2168, 473, 1379, 1017, 2704, 2423, 1919, 2006, 76, 1734, 1636, 566, 2398, 2418, 1230, 2660, 2274, 1366, 1472, 2341, 198, 250, 769, 319, 413, 1288, 2070, 1188, 2535, 1783, 1505, 2386, 1480, 1115, 1418, 1192, 271, 803, 2023, 283, 1685, 408, 2505, 2019, 1553, 1308, 1155, 1320, 149, 2507, 2110, 2565, 1984, 2349, 917, 1833, 929, 2053, 1322, 699, 1966, 832, 1121, 606, 2223, 1849, 1991, 2158, 837, 1022, 1688, 614, 2164, 1289, 845, 2197, 16, 1207, 1162, 1262, 1551, 736, 1423, 2283, 2363, 1351, 1136, 2012, 2516, 2511, 1160, 1248, 1713, 2001, 933, 1026, 2259, 262, 47, 1006, 601, 1056, 388, 727, 1742, 1083, 1501, 2443, 963, 1279, 916, 1934, 1904, 883, 907, 2301, 2372, 105, 1309, 689, 90, 102, 1310, 1766, 571, 2152, 1046, 2391, 705, 2177, 1122, 1689, 2328, 2401, 760, 730, 1264, 1455, 67, 731, 682, 1508, 2224, 1277, 1518, 855, 711, 191, 1590, 2360, 448, 1881, 2133, 1363, 1961, 2460, 2073, 193, 1293, 2492, 2235, 398, 1053, 1938, 834, 1138, 2150, 1515, 1747, 1615, 247, 2113, 935, 279, 502, 15, 735, 944, 2408, 2455, 2537, 189, 1537, 765, 2065, 690, 1616, 1062, 1232, 1381, 1963, 2126, 136, 1220, 738, 157, 1176, 323, 990, 2543, 2294, 1393, 2487, 472, 2256, 2534, 2620, 959, 888, 1829, 1691, 824, 520, 2134, 1417, 1324, 2573, 2482, 80, 1276, 1639, 1791, 1610, 1906, 1068, 937, 1284, 2305, 1486, 2044, 2368, 1350, 1419, 683, 7, 1834, 2136, 1399, 2618, 2025, 654, 118, 2621, 838, 741, 1012, 807, 1656, 884, 2144, 1404, 1269, 1401, 2686, 1607, 877, 2646, 1898, 1132, 27, 661, 2196, 2174, 2286, 427, 2059, 671, 2102, 471, 1620, 817, 2707, 2657, 2366, 2161, 2142, 2377, 1467, 2254, 1939, 1686, 23, 1416, 1295, 70, 1373, 82, 115, 1903, 1815, 293, 2213, 1059, 2649, 85, 1695, 527, 2608, 1265, 1300, 870, 792, 931, 2129, 2696, 2388, 823, 861, 2216, 813, 1336, 1614, 409, 1143, 1812, 627, 1787, 1020, 806, 1142, 1825, 1503, 375, 1569, 1282, 2176, 1347, 1680, 1878, 2532, 2297, 245, 129, 2034, 79, 1147, 2118, 1835, 1897, 476, 1885, 2246, 164, 467, 1658, 482, 36, 1535, 2605, 1651, 2406, 1905, 1041, 1403, 2078, 322, 152, 1349, 487, 610, 64, 2384, 1802, 465, 556, 2329, 761, 2599, 320, 81, 1069, 2680, 1988, 354, 590, 932, 489, 1930, 1079, 1592, 2659, 1580, 973, 284, 107, 2299, 2687, 2439, 1702, 1211, 611, 2311, 43, 697, 1820, 1077, 2590, 2480, 742, 2149, 146, 1646, 2588, 825, 1548, 1907, 394, 1529, 1700, 2562, 844, 2547, 1980, 1601, 2140, 272, 575, 25, 899, 317, 1247]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 4ms/step - loss: 8.9909 - sparse_categorical_accuracy: 0.1516 - val_loss: 3.2257 - val_sparse_categorical_accuracy: 0.2670 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 8.0016 - sparse_categorical_accuracy: 0.1710 - val_loss: 2.9117 - val_sparse_categorical_accuracy: 0.2650 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.4528 - sparse_categorical_accuracy: 0.1663 - val_loss: 2.6470 - val_sparse_categorical_accuracy: 0.2570 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.7844 - sparse_categorical_accuracy: 0.1833 - val_loss: 2.4084 - val_sparse_categorical_accuracy: 0.2800 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.4951 - sparse_categorical_accuracy: 0.1803 - val_loss: 2.3896 - val_sparse_categorical_accuracy: 0.2960 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.8292 - sparse_categorical_accuracy: 0.1973 - val_loss: 2.1354 - val_sparse_categorical_accuracy: 0.2980 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.4025 - sparse_categorical_accuracy: 0.2055 - val_loss: 2.0069 - val_sparse_categorical_accuracy: 0.3070 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.9495 - sparse_categorical_accuracy: 0.2102 - val_loss: 2.1247 - val_sparse_categorical_accuracy: 0.3090 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.7879 - sparse_categorical_accuracy: 0.2119 - val_loss: 1.6900 - val_sparse_categorical_accuracy: 0.3470 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.2856 - sparse_categorical_accuracy: 0.2459 - val_loss: 1.6035 - val_sparse_categorical_accuracy: 0.4130 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.1016 - sparse_categorical_accuracy: 0.2324 - val_loss: 1.5632 - val_sparse_categorical_accuracy: 0.3680 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.6071 - sparse_categorical_accuracy: 0.2734 - val_loss: 1.5130 - val_sparse_categorical_accuracy: 0.4070 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.4416 - sparse_categorical_accuracy: 0.2822 - val_loss: 1.3914 - val_sparse_categorical_accuracy: 0.4480 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.2043 - sparse_categorical_accuracy: 0.2957 - val_loss: 1.4003 - val_sparse_categorical_accuracy: 0.4170 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 2.9695 - sparse_categorical_accuracy: 0.3126 - val_loss: 1.2195 - val_sparse_categorical_accuracy: 0.5320 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.8737 - sparse_categorical_accuracy: 0.3249 - val_loss: 1.1467 - val_sparse_categorical_accuracy: 0.5770 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.7496 - sparse_categorical_accuracy: 0.3226 - val_loss: 1.1148 - val_sparse_categorical_accuracy: 0.5890 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5046 - sparse_categorical_accuracy: 0.3694 - val_loss: 1.0818 - val_sparse_categorical_accuracy: 0.5770 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4231 - sparse_categorical_accuracy: 0.3712 - val_loss: 1.0593 - val_sparse_categorical_accuracy: 0.6020 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3013 - sparse_categorical_accuracy: 0.3835 - val_loss: 1.0234 - val_sparse_categorical_accuracy: 0.6120 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0616 - sparse_categorical_accuracy: 0.3882 - val_loss: 0.9255 - val_sparse_categorical_accuracy: 0.7110 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0399 - sparse_categorical_accuracy: 0.3917 - val_loss: 0.9333 - val_sparse_categorical_accuracy: 0.6860 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9224 - sparse_categorical_accuracy: 0.4180 - val_loss: 0.9611 - val_sparse_categorical_accuracy: 0.6500 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.8316 - sparse_categorical_accuracy: 0.4210 - val_loss: 0.8999 - val_sparse_categorical_accuracy: 0.7040 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7288 - sparse_categorical_accuracy: 0.4590 - val_loss: 0.8454 - val_sparse_categorical_accuracy: 0.7290 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.6996 - sparse_categorical_accuracy: 0.4561 - val_loss: 0.8375 - val_sparse_categorical_accuracy: 0.7300 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.6165 - sparse_categorical_accuracy: 0.4918 - val_loss: 0.7994 - val_sparse_categorical_accuracy: 0.7560 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5066 - sparse_categorical_accuracy: 0.4982 - val_loss: 0.7836 - val_sparse_categorical_accuracy: 0.7570 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4632 - sparse_categorical_accuracy: 0.4994 - val_loss: 0.7771 - val_sparse_categorical_accuracy: 0.7510 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4560 - sparse_categorical_accuracy: 0.5012 - val_loss: 0.7666 - val_sparse_categorical_accuracy: 0.7510 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3526 - sparse_categorical_accuracy: 0.5304 - val_loss: 0.7293 - val_sparse_categorical_accuracy: 0.7830 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3242 - sparse_categorical_accuracy: 0.5357 - val_loss: 0.7355 - val_sparse_categorical_accuracy: 0.7610 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2950 - sparse_categorical_accuracy: 0.5404 - val_loss: 0.7195 - val_sparse_categorical_accuracy: 0.7760 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2160 - sparse_categorical_accuracy: 0.5796 - val_loss: 0.7083 - val_sparse_categorical_accuracy: 0.7730 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1737 - sparse_categorical_accuracy: 0.5814 - val_loss: 0.6909 - val_sparse_categorical_accuracy: 0.7940 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.1853 - sparse_categorical_accuracy: 0.5697 - val_loss: 0.6817 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1246 - sparse_categorical_accuracy: 0.5943 - val_loss: 0.6785 - val_sparse_categorical_accuracy: 0.7870 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0958 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.6740 - val_sparse_categorical_accuracy: 0.7930 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1053 - sparse_categorical_accuracy: 0.5913 - val_loss: 0.6777 - val_sparse_categorical_accuracy: 0.7840 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0111 - sparse_categorical_accuracy: 0.6212 - val_loss: 0.6639 - val_sparse_categorical_accuracy: 0.7900 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0263 - sparse_categorical_accuracy: 0.6276 - val_loss: 0.6495 - val_sparse_categorical_accuracy: 0.7910 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9619 - sparse_categorical_accuracy: 0.6557 - val_loss: 0.6408 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9794 - sparse_categorical_accuracy: 0.6317 - val_loss: 0.6394 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9115 - sparse_categorical_accuracy: 0.6604 - val_loss: 0.6312 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9348 - sparse_categorical_accuracy: 0.6511 - val_loss: 0.6336 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8711 - sparse_categorical_accuracy: 0.6651 - val_loss: 0.6144 - val_sparse_categorical_accuracy: 0.8040 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8628 - sparse_categorical_accuracy: 0.6739 - val_loss: 0.6075 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8238 - sparse_categorical_accuracy: 0.6874 - val_loss: 0.6060 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8405 - sparse_categorical_accuracy: 0.6833 - val_loss: 0.5992 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8367 - sparse_categorical_accuracy: 0.6774 - val_loss: 0.5960 - val_sparse_categorical_accuracy: 0.8070 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7893 - sparse_categorical_accuracy: 0.6979 - val_loss: 0.5889 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8023 - sparse_categorical_accuracy: 0.7032 - val_loss: 0.5894 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7743 - sparse_categorical_accuracy: 0.6950 - val_loss: 0.5793 - val_sparse_categorical_accuracy: 0.8110 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7842 - sparse_categorical_accuracy: 0.7020 - val_loss: 0.5810 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7349 - sparse_categorical_accuracy: 0.7155 - val_loss: 0.5729 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7474 - sparse_categorical_accuracy: 0.7172 - val_loss: 0.5692 - val_sparse_categorical_accuracy: 0.8140 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7308 - sparse_categorical_accuracy: 0.7190 - val_loss: 0.5637 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7182 - sparse_categorical_accuracy: 0.7201 - val_loss: 0.5600 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7380 - sparse_categorical_accuracy: 0.7137 - val_loss: 0.5609 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6976 - sparse_categorical_accuracy: 0.7354 - val_loss: 0.5548 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7104 - sparse_categorical_accuracy: 0.7289 - val_loss: 0.5521 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6924 - sparse_categorical_accuracy: 0.7283 - val_loss: 0.5472 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6735 - sparse_categorical_accuracy: 0.7441 - val_loss: 0.5450 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6787 - sparse_categorical_accuracy: 0.7430 - val_loss: 0.5407 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6809 - sparse_categorical_accuracy: 0.7406 - val_loss: 0.5436 - val_sparse_categorical_accuracy: 0.8190 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6761 - sparse_categorical_accuracy: 0.7465 - val_loss: 0.5368 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6524 - sparse_categorical_accuracy: 0.7582 - val_loss: 0.5362 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6257 - sparse_categorical_accuracy: 0.7676 - val_loss: 0.5282 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6222 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.5281 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6344 - sparse_categorical_accuracy: 0.7629 - val_loss: 0.5243 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6442 - sparse_categorical_accuracy: 0.7576 - val_loss: 0.5233 - val_sparse_categorical_accuracy: 0.8240 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6379 - sparse_categorical_accuracy: 0.7559 - val_loss: 0.5183 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6326 - sparse_categorical_accuracy: 0.7605 - val_loss: 0.5232 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6283 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.5172 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6210 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.5130 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6135 - sparse_categorical_accuracy: 0.7564 - val_loss: 0.5104 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6100 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5117 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6013 - sparse_categorical_accuracy: 0.7758 - val_loss: 0.5122 - val_sparse_categorical_accuracy: 0.8290 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6091 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.5069 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5872 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.5023 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.6012 - sparse_categorical_accuracy: 0.7670 - val_loss: 0.5046 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5895 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.4975 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5799 - sparse_categorical_accuracy: 0.7746 - val_loss: 0.4980 - val_sparse_categorical_accuracy: 0.8400 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5666 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4957 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.5912 - sparse_categorical_accuracy: 0.7687 - val_loss: 0.4926 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5800 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.4918 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5821 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.4914 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5638 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4892 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5688 - sparse_categorical_accuracy: 0.7758 - val_loss: 0.4906 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5778 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.4876 - val_sparse_categorical_accuracy: 0.8410 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5659 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.4840 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5556 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.4834 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5509 - sparse_categorical_accuracy: 0.7869 - val_loss: 0.4822 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5417 - sparse_categorical_accuracy: 0.7992 - val_loss: 0.4805 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5499 - sparse_categorical_accuracy: 0.7834 - val_loss: 0.4788 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5435 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.4777 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5489 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.4802 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5393 - sparse_categorical_accuracy: 0.7980 - val_loss: 0.4780 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5430 - sparse_categorical_accuracy: 0.7845 - val_loss: 0.4733 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5453 - sparse_categorical_accuracy: 0.7945 - val_loss: 0.4746 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 9, test #3, 2nd iteration: loss of 0.4732762575149536; sparse_categorical_accuracy of 84.20000076293945%\n",
            "appending basis + selective feature vector data\n",
            "[1994, 2290, 1874, 268, 511, 1086, 491, 98, 1785, 2195, 1530, 2509, 1251, 1705, 344, 2662, 281, 2484, 2497, 865, 359, 1819, 2173, 911, 100, 882, 1857, 1415, 239, 86, 2107, 2584, 2292, 1329, 2201, 1744, 522, 2576, 1539, 343, 628, 588, 138, 2355, 826, 2514, 106, 2060, 1664, 858, 1498, 1067, 1967, 135, 885, 1937, 1823, 1915, 2074, 292, 2285, 1866, 992, 713, 2109, 2190, 1100, 1662, 1204, 1396, 173, 970, 1725, 2088, 290, 277, 270, 1144, 843, 543, 927, 1406, 2063, 2454, 1789, 1687, 2521, 450, 1998, 2629, 818, 2702, 602, 495, 783, 426, 2477, 1134, 526, 1371, 1354, 2054, 1078, 1671, 2354, 685, 943, 1159, 1597, 1163, 2689, 1271, 1437, 421, 435, 31, 2661, 1198, 1342, 260, 156, 134, 240, 2447, 1641, 802, 89, 1448, 499, 1337, 1246, 2250, 2392, 2181, 2257, 507, 490, 2253, 2041, 1876, 468, 2359, 617, 1044, 233, 37, 1694, 1810, 53, 1581, 1334, 1768, 821, 1057, 207, 2527, 199, 976, 2369, 1167, 358, 286, 2322, 399, 1129, 1667, 2478, 609, 2495, 425, 664, 1854, 2615, 1654, 2307, 812, 479, 1965, 1631, 2094, 217, 254, 878, 68, 2681, 1422, 228, 657, 2583, 2160, 236, 1451, 500, 2602, 190, 754, 1576, 123, 857, 1733, 101, 2009, 2411, 2427, 1065, 989, 1880, 652, 42, 776, 1376, 2239, 587, 2541, 226, 1070, 346, 920, 647, 1732, 1397, 1464, 2042, 704, 2128, 218, 940, 353, 514, 148, 2441, 2086, 1055, 1410, 2597, 632, 1979, 572, 717, 1752, 1736, 1843, 2450, 1331, 2431, 2665, 2002, 159, 2336, 693, 1605, 1828, 419, 1421, 774, 1181, 1327, 330, 2085, 1683, 171, 1066, 643, 1074, 2612, 274, 1940, 815, 1249, 1096, 707, 206, 2287, 165, 1642, 113, 1412, 0, 1972, 1946, 1678, 2288, 1191, 2421, 558, 1992, 567, 1717, 934, 1578, 848, 579, 1969, 2619, 275, 1475, 1824, 782, 2433, 1128, 192, 2601, 904, 2522, 1447, 1024, 83, 2240, 1345, 1478, 1775, 1172, 645, 1080, 952, 2464, 56, 1681, 757, 820, 1321, 2306, 396, 1684, 1928, 930, 1629, 30, 1298, 1943, 1510, 1332, 1194, 698, 755, 2071, 2609, 1009, 2039, 1726, 1302, 20, 965, 1818, 1402, 2607, 518, 751, 2486, 875, 1123, 551, 795, 1431, 1185, 2564, 541, 2592, 1287, 393, 352, 836, 2549, 1936, 964, 170, 175, 1151, 905, 1427, 853, 444, 137, 1093, 1036, 1634, 1283, 405, 748, 1650, 1216, 143, 345, 420, 1627, 1111, 2364, 1485, 659, 2415, 2289, 1661, 1533, 1280, 1137, 367, 1368, 997, 768, 1574, 2593, 1092, 252, 505, 529, 582, 2508, 2672, 141, 651, 1790, 906, 1976, 2580, 1330, 1846, 2114, 953, 1845, 583, 2204, 926, 811, 109, 2261, 2247, 695, 1753, 303, 886, 1378, 2269, 921, 2703, 936, 1916, 1390, 1784, 2642, 2382, 954, 197, 1982, 1722, 607, 1221, 2295, 577, 1193, 852, 1492, 653, 1166, 2647, 646, 2596, 1319, 2407, 2131, 612, 1821, 2594, 1521, 2457, 599, 2262, 622, 40, 2698, 2545, 1575, 8, 2375, 2624, 868, 1105, 1847, 1112, 92, 2356, 525, 1927, 1908, 978, 1497, 1622, 1190, 174, 1010, 1462, 981, 2334, 1157, 321, 2127, 2548, 1727, 63, 1663, 1179, 1517, 1075, 411, 1981, 209, 384, 1707, 1779, 1557, 966, 2425, 2245, 2581, 1887, 2631, 238, 2338, 181, 2271, 324, 716, 1039, 1016, 1632, 2655, 2083, 2586, 2033, 1848, 1199, 299, 1900, 555, 357, 660, 1306, 66, 2529, 676, 1090, 1208, 234, 722, 2205, 1225, 2032, 1858, 1579, 2663, 447, 2120, 120, 1987, 1741, 2694, 1803, 1814, 1723, 2263, 1951, 2111, 2673, 304, 2153, 1465, 1822, 2333, 1942, 1955, 1468, 93, 2641, 51, 414, 1586, 1718, 1323, 814, 775, 2168, 473, 1379, 1017, 2704, 2423, 1919, 2006, 76, 1734, 1636, 566, 2398, 2418, 1230, 2660, 2274, 1366, 1472, 2341, 198, 250, 769, 319, 413, 1288, 2070, 1188, 2535, 1783, 1505, 2386, 1480, 1115, 1418, 1192, 271, 803, 2023, 283, 1685, 408, 2505, 2019, 1553, 1308, 1155, 1320, 149, 2507, 2110, 2565, 1984, 2349, 917, 1833, 929, 2053, 1322, 699, 1966, 832, 1121, 606, 2223, 1849, 1991, 2158, 837, 1022, 1688, 614, 2164, 1289, 845, 2197, 16, 1207, 1162, 1262, 1551, 736, 1423, 2283, 2363, 1351, 1136, 2012, 2516, 2511, 1160, 1248, 1713, 2001, 933, 1026, 2259, 262, 47, 1006, 601, 1056, 388, 727, 1742, 1083, 1501, 2443, 963, 1279, 916, 1934, 1904, 883, 907, 2301, 2372, 105, 1309, 689, 90, 102, 1310, 1766, 571, 2152, 1046, 2391, 705, 2177, 1122, 1689, 2328, 2401, 760, 730, 1264, 1455, 67, 731, 682, 1508, 2224, 1277, 1518, 855, 711, 191, 1590, 2360, 448, 1881, 2133, 1363, 1961, 2460, 2073, 193, 1293, 2492, 2235, 398, 1053, 1938, 834, 1138, 2150, 1515, 1747, 1615, 247, 2113, 935, 279, 502, 15, 735, 944, 2408, 2455, 2537, 189, 1537, 765, 2065, 690, 1616, 1062, 1232, 1381, 1963, 2126, 136, 1220, 738, 157, 1176, 323, 990, 2543, 2294, 1393, 2487, 472, 2256, 2534, 2620, 959, 888, 1829, 1691, 824, 520, 2134, 1417, 1324, 2573, 2482, 80, 1276, 1639, 1791, 1610, 1906, 1068, 937, 1284, 2305, 1486, 2044, 2368, 1350, 1419, 683, 7, 1834, 2136, 1399, 2618, 2025, 654, 118, 2621, 838, 741, 1012, 807, 1656, 884, 2144, 1404, 1269, 1401, 2686, 1607, 877, 2646, 1898, 1132, 27, 661, 2196, 2174, 2286, 427, 2059, 671, 2102, 471, 1620, 817, 2707, 2657, 2366, 2161, 2142, 2377, 1467, 2254, 1939, 1686, 23, 1416, 1295, 70, 1373, 82, 115, 1903, 1815, 293, 2213, 1059, 2649, 85, 1695, 527, 2608, 1265, 1300, 870, 792, 931, 2129, 2696, 2388, 823, 861, 2216, 813, 1336, 1614, 409, 1143, 1812, 627, 1787, 1020, 806, 1142, 1825, 1503, 375, 1569, 1282, 2176, 1347, 1680, 1878, 2532, 2297, 245, 129, 2034, 79, 1147, 2118, 1835, 1897, 476, 1885, 2246, 164, 467, 1658, 482, 36, 1535, 2605, 1651, 2406, 1905, 1041, 1403, 2078, 322, 152, 1349, 487, 610, 64, 2384, 1802, 465, 556, 2329, 761, 2599, 320, 81, 1069, 2680, 1988, 354, 590, 932, 489, 1930, 1079, 1592, 2659, 1580, 973, 284, 107, 2299, 2687, 2439, 1702, 1211, 611, 2311, 43, 697, 1820, 1077, 2590, 2480, 742, 2149, 146, 1646, 2588, 825, 1548, 1907, 394, 1529, 1700, 2562, 844, 2547, 1980, 1601, 2140, 272, 575, 25, 899, 317, 1247]\n",
            "------------------------------------------------------------------------\n",
            "Epoch 1/100\n",
            "107/107 [==============================] - 1s 5ms/step - loss: 8.4030 - sparse_categorical_accuracy: 0.1739 - val_loss: 2.3092 - val_sparse_categorical_accuracy: 0.3150 - lr: 5.0000e-05\n",
            "Epoch 2/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 7.4851 - sparse_categorical_accuracy: 0.1909 - val_loss: 2.7036 - val_sparse_categorical_accuracy: 0.3130 - lr: 5.0000e-05\n",
            "Epoch 3/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.9806 - sparse_categorical_accuracy: 0.1833 - val_loss: 2.3373 - val_sparse_categorical_accuracy: 0.3080 - lr: 5.0000e-05\n",
            "Epoch 4/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.3688 - sparse_categorical_accuracy: 0.2084 - val_loss: 2.2253 - val_sparse_categorical_accuracy: 0.3210 - lr: 5.0000e-05\n",
            "Epoch 5/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 6.0033 - sparse_categorical_accuracy: 0.2049 - val_loss: 2.0480 - val_sparse_categorical_accuracy: 0.3320 - lr: 5.0000e-05\n",
            "Epoch 6/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.5343 - sparse_categorical_accuracy: 0.2119 - val_loss: 1.9822 - val_sparse_categorical_accuracy: 0.3340 - lr: 5.0000e-05\n",
            "Epoch 7/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 5.0750 - sparse_categorical_accuracy: 0.2359 - val_loss: 1.9320 - val_sparse_categorical_accuracy: 0.3450 - lr: 5.0000e-05\n",
            "Epoch 8/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.9150 - sparse_categorical_accuracy: 0.2307 - val_loss: 1.5903 - val_sparse_categorical_accuracy: 0.3960 - lr: 5.0000e-05\n",
            "Epoch 9/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.5783 - sparse_categorical_accuracy: 0.2494 - val_loss: 1.6588 - val_sparse_categorical_accuracy: 0.3850 - lr: 5.0000e-05\n",
            "Epoch 10/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 4.3585 - sparse_categorical_accuracy: 0.2389 - val_loss: 1.5172 - val_sparse_categorical_accuracy: 0.4250 - lr: 5.0000e-05\n",
            "Epoch 11/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.8249 - sparse_categorical_accuracy: 0.2652 - val_loss: 1.4898 - val_sparse_categorical_accuracy: 0.4380 - lr: 5.0000e-05\n",
            "Epoch 12/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.5864 - sparse_categorical_accuracy: 0.2816 - val_loss: 1.4043 - val_sparse_categorical_accuracy: 0.4730 - lr: 5.0000e-05\n",
            "Epoch 13/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.3806 - sparse_categorical_accuracy: 0.2892 - val_loss: 1.4434 - val_sparse_categorical_accuracy: 0.4410 - lr: 5.0000e-05\n",
            "Epoch 14/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.2781 - sparse_categorical_accuracy: 0.2933 - val_loss: 1.2594 - val_sparse_categorical_accuracy: 0.5110 - lr: 5.0000e-05\n",
            "Epoch 15/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.0926 - sparse_categorical_accuracy: 0.3132 - val_loss: 1.2574 - val_sparse_categorical_accuracy: 0.4970 - lr: 5.0000e-05\n",
            "Epoch 16/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 3.0008 - sparse_categorical_accuracy: 0.3138 - val_loss: 1.1995 - val_sparse_categorical_accuracy: 0.5330 - lr: 5.0000e-05\n",
            "Epoch 17/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 2.6948 - sparse_categorical_accuracy: 0.3232 - val_loss: 1.1736 - val_sparse_categorical_accuracy: 0.5570 - lr: 5.0000e-05\n",
            "Epoch 18/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.5909 - sparse_categorical_accuracy: 0.3431 - val_loss: 1.1458 - val_sparse_categorical_accuracy: 0.5660 - lr: 5.0000e-05\n",
            "Epoch 19/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.4656 - sparse_categorical_accuracy: 0.3507 - val_loss: 1.0894 - val_sparse_categorical_accuracy: 0.5850 - lr: 5.0000e-05\n",
            "Epoch 20/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.3302 - sparse_categorical_accuracy: 0.3823 - val_loss: 1.0479 - val_sparse_categorical_accuracy: 0.6280 - lr: 5.0000e-05\n",
            "Epoch 21/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.2452 - sparse_categorical_accuracy: 0.3753 - val_loss: 1.0087 - val_sparse_categorical_accuracy: 0.6510 - lr: 5.0000e-05\n",
            "Epoch 22/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0941 - sparse_categorical_accuracy: 0.3993 - val_loss: 0.9890 - val_sparse_categorical_accuracy: 0.6640 - lr: 5.0000e-05\n",
            "Epoch 23/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 2.0178 - sparse_categorical_accuracy: 0.4028 - val_loss: 0.9761 - val_sparse_categorical_accuracy: 0.6350 - lr: 5.0000e-05\n",
            "Epoch 24/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.9748 - sparse_categorical_accuracy: 0.4180 - val_loss: 0.9431 - val_sparse_categorical_accuracy: 0.6730 - lr: 5.0000e-05\n",
            "Epoch 25/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7744 - sparse_categorical_accuracy: 0.4362 - val_loss: 0.9223 - val_sparse_categorical_accuracy: 0.6850 - lr: 5.0000e-05\n",
            "Epoch 26/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7308 - sparse_categorical_accuracy: 0.4420 - val_loss: 0.9359 - val_sparse_categorical_accuracy: 0.6630 - lr: 5.0000e-05\n",
            "Epoch 27/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.7683 - sparse_categorical_accuracy: 0.4444 - val_loss: 0.8943 - val_sparse_categorical_accuracy: 0.6930 - lr: 5.0000e-05\n",
            "Epoch 28/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.6623 - sparse_categorical_accuracy: 0.4596 - val_loss: 0.8678 - val_sparse_categorical_accuracy: 0.7170 - lr: 5.0000e-05\n",
            "Epoch 29/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5575 - sparse_categorical_accuracy: 0.5018 - val_loss: 0.8533 - val_sparse_categorical_accuracy: 0.7130 - lr: 5.0000e-05\n",
            "Epoch 30/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.5072 - sparse_categorical_accuracy: 0.4830 - val_loss: 0.8680 - val_sparse_categorical_accuracy: 0.6940 - lr: 5.0000e-05\n",
            "Epoch 31/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4358 - sparse_categorical_accuracy: 0.5029 - val_loss: 0.8351 - val_sparse_categorical_accuracy: 0.7280 - lr: 5.0000e-05\n",
            "Epoch 32/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.4106 - sparse_categorical_accuracy: 0.5135 - val_loss: 0.8214 - val_sparse_categorical_accuracy: 0.7240 - lr: 5.0000e-05\n",
            "Epoch 33/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.3310 - sparse_categorical_accuracy: 0.5234 - val_loss: 0.7986 - val_sparse_categorical_accuracy: 0.7330 - lr: 5.0000e-05\n",
            "Epoch 34/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2879 - sparse_categorical_accuracy: 0.5381 - val_loss: 0.8011 - val_sparse_categorical_accuracy: 0.7300 - lr: 5.0000e-05\n",
            "Epoch 35/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2457 - sparse_categorical_accuracy: 0.5626 - val_loss: 0.7840 - val_sparse_categorical_accuracy: 0.7350 - lr: 5.0000e-05\n",
            "Epoch 36/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2412 - sparse_categorical_accuracy: 0.5463 - val_loss: 0.7792 - val_sparse_categorical_accuracy: 0.7480 - lr: 5.0000e-05\n",
            "Epoch 37/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.2332 - sparse_categorical_accuracy: 0.5574 - val_loss: 0.7731 - val_sparse_categorical_accuracy: 0.7310 - lr: 5.0000e-05\n",
            "Epoch 38/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1517 - sparse_categorical_accuracy: 0.5814 - val_loss: 0.7587 - val_sparse_categorical_accuracy: 0.7460 - lr: 5.0000e-05\n",
            "Epoch 39/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.1324 - sparse_categorical_accuracy: 0.5896 - val_loss: 0.7483 - val_sparse_categorical_accuracy: 0.7610 - lr: 5.0000e-05\n",
            "Epoch 40/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0809 - sparse_categorical_accuracy: 0.6036 - val_loss: 0.7378 - val_sparse_categorical_accuracy: 0.7590 - lr: 5.0000e-05\n",
            "Epoch 41/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0415 - sparse_categorical_accuracy: 0.6177 - val_loss: 0.7237 - val_sparse_categorical_accuracy: 0.7740 - lr: 5.0000e-05\n",
            "Epoch 42/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0441 - sparse_categorical_accuracy: 0.6165 - val_loss: 0.7210 - val_sparse_categorical_accuracy: 0.7570 - lr: 5.0000e-05\n",
            "Epoch 43/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 1.0444 - sparse_categorical_accuracy: 0.6101 - val_loss: 0.7124 - val_sparse_categorical_accuracy: 0.7800 - lr: 5.0000e-05\n",
            "Epoch 44/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 1.0188 - sparse_categorical_accuracy: 0.6159 - val_loss: 0.7035 - val_sparse_categorical_accuracy: 0.7800 - lr: 5.0000e-05\n",
            "Epoch 45/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9857 - sparse_categorical_accuracy: 0.6294 - val_loss: 0.7018 - val_sparse_categorical_accuracy: 0.7780 - lr: 5.0000e-05\n",
            "Epoch 46/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9457 - sparse_categorical_accuracy: 0.6452 - val_loss: 0.6904 - val_sparse_categorical_accuracy: 0.7820 - lr: 5.0000e-05\n",
            "Epoch 47/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9128 - sparse_categorical_accuracy: 0.6639 - val_loss: 0.6874 - val_sparse_categorical_accuracy: 0.7790 - lr: 5.0000e-05\n",
            "Epoch 48/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.9393 - sparse_categorical_accuracy: 0.6528 - val_loss: 0.6754 - val_sparse_categorical_accuracy: 0.7960 - lr: 5.0000e-05\n",
            "Epoch 49/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8754 - sparse_categorical_accuracy: 0.6739 - val_loss: 0.6692 - val_sparse_categorical_accuracy: 0.7870 - lr: 5.0000e-05\n",
            "Epoch 50/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8675 - sparse_categorical_accuracy: 0.6809 - val_loss: 0.6605 - val_sparse_categorical_accuracy: 0.7990 - lr: 5.0000e-05\n",
            "Epoch 51/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8434 - sparse_categorical_accuracy: 0.6885 - val_loss: 0.6599 - val_sparse_categorical_accuracy: 0.7930 - lr: 5.0000e-05\n",
            "Epoch 52/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8585 - sparse_categorical_accuracy: 0.6721 - val_loss: 0.6486 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 53/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8392 - sparse_categorical_accuracy: 0.6891 - val_loss: 0.6418 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 54/100\n",
            "107/107 [==============================] - 0s 4ms/step - loss: 0.8168 - sparse_categorical_accuracy: 0.7119 - val_loss: 0.6356 - val_sparse_categorical_accuracy: 0.8020 - lr: 5.0000e-05\n",
            "Epoch 55/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7817 - sparse_categorical_accuracy: 0.7043 - val_loss: 0.6312 - val_sparse_categorical_accuracy: 0.7970 - lr: 5.0000e-05\n",
            "Epoch 56/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.8109 - sparse_categorical_accuracy: 0.6991 - val_loss: 0.6266 - val_sparse_categorical_accuracy: 0.7980 - lr: 5.0000e-05\n",
            "Epoch 57/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7705 - sparse_categorical_accuracy: 0.7020 - val_loss: 0.6213 - val_sparse_categorical_accuracy: 0.8010 - lr: 5.0000e-05\n",
            "Epoch 58/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7769 - sparse_categorical_accuracy: 0.7143 - val_loss: 0.6172 - val_sparse_categorical_accuracy: 0.8040 - lr: 5.0000e-05\n",
            "Epoch 59/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7581 - sparse_categorical_accuracy: 0.7155 - val_loss: 0.6150 - val_sparse_categorical_accuracy: 0.8000 - lr: 5.0000e-05\n",
            "Epoch 60/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7144 - sparse_categorical_accuracy: 0.7371 - val_loss: 0.6066 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 61/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7326 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.6015 - val_sparse_categorical_accuracy: 0.8050 - lr: 5.0000e-05\n",
            "Epoch 62/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7277 - sparse_categorical_accuracy: 0.7313 - val_loss: 0.5965 - val_sparse_categorical_accuracy: 0.8030 - lr: 5.0000e-05\n",
            "Epoch 63/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7138 - sparse_categorical_accuracy: 0.7330 - val_loss: 0.5955 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 64/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7116 - sparse_categorical_accuracy: 0.7406 - val_loss: 0.5944 - val_sparse_categorical_accuracy: 0.8060 - lr: 5.0000e-05\n",
            "Epoch 65/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.7051 - sparse_categorical_accuracy: 0.7319 - val_loss: 0.5909 - val_sparse_categorical_accuracy: 0.8080 - lr: 5.0000e-05\n",
            "Epoch 66/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6612 - sparse_categorical_accuracy: 0.7488 - val_loss: 0.5837 - val_sparse_categorical_accuracy: 0.8100 - lr: 5.0000e-05\n",
            "Epoch 67/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6733 - sparse_categorical_accuracy: 0.7529 - val_loss: 0.5794 - val_sparse_categorical_accuracy: 0.8120 - lr: 5.0000e-05\n",
            "Epoch 68/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6852 - sparse_categorical_accuracy: 0.7418 - val_loss: 0.5777 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 69/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6601 - sparse_categorical_accuracy: 0.7594 - val_loss: 0.5716 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 70/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6446 - sparse_categorical_accuracy: 0.7518 - val_loss: 0.5682 - val_sparse_categorical_accuracy: 0.8130 - lr: 5.0000e-05\n",
            "Epoch 71/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6677 - sparse_categorical_accuracy: 0.7523 - val_loss: 0.5645 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 72/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6278 - sparse_categorical_accuracy: 0.7681 - val_loss: 0.5609 - val_sparse_categorical_accuracy: 0.8170 - lr: 5.0000e-05\n",
            "Epoch 73/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6332 - sparse_categorical_accuracy: 0.7635 - val_loss: 0.5605 - val_sparse_categorical_accuracy: 0.8160 - lr: 5.0000e-05\n",
            "Epoch 74/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6385 - sparse_categorical_accuracy: 0.7652 - val_loss: 0.5549 - val_sparse_categorical_accuracy: 0.8250 - lr: 5.0000e-05\n",
            "Epoch 75/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6390 - sparse_categorical_accuracy: 0.7635 - val_loss: 0.5489 - val_sparse_categorical_accuracy: 0.8150 - lr: 5.0000e-05\n",
            "Epoch 76/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6116 - sparse_categorical_accuracy: 0.7717 - val_loss: 0.5511 - val_sparse_categorical_accuracy: 0.8220 - lr: 5.0000e-05\n",
            "Epoch 77/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6049 - sparse_categorical_accuracy: 0.7787 - val_loss: 0.5419 - val_sparse_categorical_accuracy: 0.8200 - lr: 5.0000e-05\n",
            "Epoch 78/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6157 - sparse_categorical_accuracy: 0.7711 - val_loss: 0.5416 - val_sparse_categorical_accuracy: 0.8210 - lr: 5.0000e-05\n",
            "Epoch 79/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6065 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.5388 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 80/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6172 - sparse_categorical_accuracy: 0.7781 - val_loss: 0.5350 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 81/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.6001 - sparse_categorical_accuracy: 0.7769 - val_loss: 0.5347 - val_sparse_categorical_accuracy: 0.8270 - lr: 5.0000e-05\n",
            "Epoch 82/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5983 - sparse_categorical_accuracy: 0.7822 - val_loss: 0.5282 - val_sparse_categorical_accuracy: 0.8230 - lr: 5.0000e-05\n",
            "Epoch 83/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5941 - sparse_categorical_accuracy: 0.7752 - val_loss: 0.5261 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 84/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5837 - sparse_categorical_accuracy: 0.7857 - val_loss: 0.5229 - val_sparse_categorical_accuracy: 0.8280 - lr: 5.0000e-05\n",
            "Epoch 85/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5871 - sparse_categorical_accuracy: 0.7793 - val_loss: 0.5222 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 86/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5828 - sparse_categorical_accuracy: 0.7851 - val_loss: 0.5191 - val_sparse_categorical_accuracy: 0.8300 - lr: 5.0000e-05\n",
            "Epoch 87/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5650 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.5184 - val_sparse_categorical_accuracy: 0.8320 - lr: 5.0000e-05\n",
            "Epoch 88/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5619 - sparse_categorical_accuracy: 0.7951 - val_loss: 0.5142 - val_sparse_categorical_accuracy: 0.8260 - lr: 5.0000e-05\n",
            "Epoch 89/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5696 - sparse_categorical_accuracy: 0.7828 - val_loss: 0.5115 - val_sparse_categorical_accuracy: 0.8310 - lr: 5.0000e-05\n",
            "Epoch 90/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5930 - sparse_categorical_accuracy: 0.7816 - val_loss: 0.5074 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 91/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5595 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.5080 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 92/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5744 - sparse_categorical_accuracy: 0.7886 - val_loss: 0.5046 - val_sparse_categorical_accuracy: 0.8370 - lr: 5.0000e-05\n",
            "Epoch 93/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5458 - sparse_categorical_accuracy: 0.7810 - val_loss: 0.5046 - val_sparse_categorical_accuracy: 0.8340 - lr: 5.0000e-05\n",
            "Epoch 94/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5621 - sparse_categorical_accuracy: 0.7939 - val_loss: 0.5018 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 95/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5608 - sparse_categorical_accuracy: 0.7986 - val_loss: 0.5008 - val_sparse_categorical_accuracy: 0.8420 - lr: 5.0000e-05\n",
            "Epoch 96/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5736 - sparse_categorical_accuracy: 0.7840 - val_loss: 0.4972 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "Epoch 97/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5667 - sparse_categorical_accuracy: 0.7875 - val_loss: 0.4965 - val_sparse_categorical_accuracy: 0.8380 - lr: 5.0000e-05\n",
            "Epoch 98/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5537 - sparse_categorical_accuracy: 0.7775 - val_loss: 0.4935 - val_sparse_categorical_accuracy: 0.8350 - lr: 5.0000e-05\n",
            "Epoch 99/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5587 - sparse_categorical_accuracy: 0.7804 - val_loss: 0.4920 - val_sparse_categorical_accuracy: 0.8330 - lr: 5.0000e-05\n",
            "Epoch 100/100\n",
            "107/107 [==============================] - 0s 3ms/step - loss: 0.5467 - sparse_categorical_accuracy: 0.7898 - val_loss: 0.4913 - val_sparse_categorical_accuracy: 0.8360 - lr: 5.0000e-05\n",
            "32/32 [==============================] - 0s 1ms/step\n",
            "Score for fold 9, test #4, 2nd iteration: loss of 0.5007896423339844; sparse_categorical_accuracy of 84.20000076293945%\n"
          ]
        }
      ],
      "source": [
        "data_by_class = {cls: Data.loc[Data['class'] == cls].drop(['class'], axis=1) for cls in range(max(catagories) + 1)}\n",
        "basis = [[max(df[f'w_{i-1}']) for i in range(1, len(df.columns))] for df in data_by_class.values()]\\\n",
        "        if node_data_name != \"data/Pubmed-Diabetes.NODE.paper.tab\" else []\n",
        "sel_basis = [[int(list(df[f'w_{i-1}'].to_numpy()).count(1) >= int(len(df[f'w_{i-1}'].index)*0.1)) \n",
        "        for i in range(1, len(df.columns))]\n",
        "        for df in data_by_class.values()] if node_data_name != \"data/Pubmed-Diabetes.NODE.paper.tab\" else\\\n",
        "        [[int(np.count_nonzero(list(df.iloc[:, i].astype('float'))) >= int(len(df.index)*0.25)) \n",
        "        for i in range(1, len(df.columns))] for df in data_by_class.values()]\n",
        "\n",
        "data_array = data_s.drop(['ID', 'class'], axis=1).astype(float).to_numpy()\n",
        "ID_ind = data_s.columns.get_loc('ID')\n",
        "\n",
        "avg_acc_per_fold = [[],[]]\n",
        "if final_test:\n",
        "  earlystop1 = EarlyStopping(patience=7)\n",
        "  earlystop2 = ReduceLROnPlateau(monitor='val_loss',\n",
        "                                min_lr = 3e-7, \n",
        "                                patience = 4,\n",
        "                                factor=0.3,\n",
        "                                verbose = 1)\n",
        "  mat = np.array(adj)\n",
        "  for split_ind in range(10):\n",
        "    np.fill_diagonal(mat, class_60[split_ind])\n",
        "    acc_per_fold = [[], []]\n",
        "    loss_per_fold = [[], []]\n",
        "    for train_iter in range(5):\n",
        "      \n",
        "      # F_vec = create_feature_vectors(mat, max(catagories) + 2)\n",
        "\n",
        "      # # if node_data_name == \"data/Pubmed-Diabetes.NODE.paper.tab\":\n",
        "      # #    print(\"appending PCA feature vector data\")\n",
        "      # #    F_vec = np.hstack((F_vec, Scaled_data))\n",
        "      # \"\"\" TRAINING 1st ITER \"\"\"\n",
        "      # model, scores, y_pred = train_model(F_vec, test_splits[split_ind], train_splits[split_ind], earlystop1, earlystop2, 3e-5)\n",
        "      # print(f'Score for fold {split_ind}, test #{train_iter}, 1st iteration: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "      # acc_per_fold[0].append(scores[1] * 100)\n",
        "      # loss_per_fold[0].append(scores[0])\n",
        "\n",
        "      # updated_class_60=np.array(catagories)\n",
        "      # i=0\n",
        "      # for d in test_splits[split_ind]:\n",
        "      #     class_60[split_ind][d]=y_pred[i]\n",
        "      #     i=i+1\n",
        "      # print(class_60[split_ind])\n",
        "      # np.fill_diagonal(mat, class_60[split_ind])\n",
        "      # print(mat)\n",
        "\n",
        "      F_vec = create_feature_vectors(mat, max(catagories) + 1)\n",
        "\n",
        "      # Adding keyword features to F_vec\n",
        "      if node_data_name == \"data/Pubmed-Diabetes.NODE.paper.tab\":\n",
        "         print(\"appending PCA feature vector data\")\n",
        "         F_vec = np.hstack((F_vec, Scaled_data))\n",
        "      else:\n",
        "        print(\"appending basis + selective feature vector data\")\n",
        "        for i, row in enumerate(data_array):\n",
        "          index = node_ID[data_s.iloc[i]['ID']]\n",
        "          f = [int(i > 0) for i in row]\n",
        "          features = ([np.dot(f, basis[cls]) for cls in range(max(catagories)+1)]\\\n",
        "                    + [np.dot(f,sel_basis[cls]) for cls in range(max(catagories)+1)])\\\n",
        "                    if node_data_name != \"data/Pubmed-Diabetes.NODE.paper.tab\" else\\\n",
        "                    [np.dot(f,sel_basis[cls]) for cls in range(max(catagories)+1)]\n",
        "          F_vec[index] += features\n",
        "\n",
        "      model, scores, y_pred = train_model(F_vec, test_splits[split_ind], train_splits[split_ind], earlystop1, earlystop2, 5e-5)\n",
        "      print(f'Score for fold {split_ind}, test #{train_iter}, 2nd iteration: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "      acc_per_fold[1].append(scores[1] * 100)\n",
        "      loss_per_fold[1].append(scores[0])\n",
        "    # avg_acc_per_fold[0].append(sum(acc_per_fold[0]) / len(acc_per_fold[0]))\n",
        "    avg_acc_per_fold[1].append(sum(acc_per_fold[1]) / len(acc_per_fold[1]))\n",
        "\n",
        "else:\n",
        "  F_vec = create_feature_vectors(adj, max(catagories) + 2) \\\n",
        "              if not os.path.exists(\"/content/Feature_cora.csv\") else get_feature_vectors(\"/content/Feature_cora.csv\")\n",
        "  x = np.array(F_vec)\n",
        "  k = len(F_vec[0])\n",
        "  data=pd.DataFrame({\"a_0\":x[:,0]})\n",
        "  data.insert(loc=1, column=\"b_0\", value=x[:,1])\n",
        "  for i in range(1,k-1,2):\n",
        "      data.insert(loc=i+1, column=\"a_{}\".format(int((i+1)/2)), value=x[:,i+1])\n",
        "      data.insert(loc=i+2, column=\"b_{}\".format(int((i+1)/2)), value=x[:,i+2])\n",
        "  print(k)\n",
        "  data.insert(loc=k,column='Class',value=data_s['class'].to_numpy())\n",
        "  print(data.head(30))\n",
        "  feature=[\"a_0\",\"b_0\"]\n",
        "  for i in range(1,k-1,2):\n",
        "      feature.append(\"a_{}\".format(int((i+1)/2)))\n",
        "      feature.append(\"b_{}\".format(int((i+1)/2)))\n",
        "\n",
        "  X=data[feature] # Features\n",
        "  y=data['Class']  # Labels\n",
        "  X_train=X.iloc[train_splits[0]]\n",
        "  X_test=X.iloc[test_splits[0]]\n",
        "  y_train=y.iloc[train_splits[0]]\n",
        "  y_test=y.iloc[test_splits[0]]\n",
        "      "
      ],
      "id": "XOyN9Vyph5pb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "76rqSeUg0aXS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "outputId": "b5f0fe4a-a36f-4057-cb74-80fbdc99fee4"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ZeroDivisionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-42078c50cc4a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_acc_per_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_acc_per_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-60-42078c50cc4a>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;34m[\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_acc_per_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavg_acc_per_fold\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mZeroDivisionError\u001b[0m: division by zero"
          ]
        }
      ],
      "source": [
        "[sum(avg_acc_per_fold[i]) / len(avg_acc_per_fold[i]) for i in range(2)]"
      ],
      "id": "76rqSeUg0aXS"
    },
    {
      "cell_type": "code",
      "source": [
        "sum(avg_acc_per_fold[1]) / len(avg_acc_per_fold[1])"
      ],
      "metadata": {
        "id": "wVB6mBDxl2y8",
        "outputId": "f9af563d-fad9-4be1-8b80-28b33604a064",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "wVB6mBDxl2y8",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "83.90000057220459"
            ]
          },
          "metadata": {},
          "execution_count": 61
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max(avg_acc_per_fold)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DttQWID8jS4-",
        "outputId": "2030441a-b39c-4ce4-a67c-5ff85c628178"
      },
      "id": "DttQWID8jS4-",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "91.79999828338623"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z3Uq6Q530s0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04322fdc-b3b5-43d9-fad5-5b5657a47079"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9284660588142666"
            ]
          },
          "metadata": {},
          "execution_count": 53
        }
      ],
      "source": [
        "import statistics\n",
        "statistics.stdev(avg_acc_per_fold[0])"
      ],
      "id": "z3Uq6Q530s0f"
    },
    {
      "cell_type": "code",
      "source": [
        "statistics.stdev(avg_acc_per_fold[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_7kKYnNOFJ89",
        "outputId": "13456ac6-a7a8-461e-ef2d-5d75057f1a4f"
      },
      "id": "_7kKYnNOFJ89",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6712516005771747"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3OQY5PXu-Sgi"
      },
      "source": [
        "#1st iteration"
      ],
      "id": "3OQY5PXu-Sgi"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rgAZSEZx-X6H"
      },
      "source": [
        "## XGBoost"
      ],
      "id": "rgAZSEZx-X6H"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "76e68822",
        "outputId": "6b53ebbb-77ce-4882-87a2-9a419ca6b89f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy for  100 estimators:  84.0 %\n",
            "Accuracy for  105 estimators:  84.0 %\n",
            "Accuracy for  110 estimators:  84.0 %\n",
            "Accuracy for  115 estimators:  83.89999999999999 %\n",
            "Accuracy for  120 estimators:  83.89999999999999 %\n",
            "Accuracy for  125 estimators:  83.8 %\n",
            "Accuracy for  130 estimators:  83.8 %\n",
            "Accuracy for  135 estimators:  83.8 %\n",
            "Accuracy for  140 estimators:  83.8 %\n",
            "Accuracy for  145 estimators:  83.8 %\n",
            "Accuracy for  150 estimators:  83.8 %\n"
          ]
        }
      ],
      "source": [
        "for n_est in range(100, 151, 5):\n",
        "  bst = XGBClassifier(n_estimators=n_est, max_depth=3, learning_rate=0.3)\n",
        "  bst.fit(X_train,y_train)\n",
        "  y_pred=bst.predict(X_test)\n",
        "  curr_score = metrics.accuracy_score(y_test, y_pred)\n",
        "  #Import scikit-learn metrics module for accuracy calculation\n",
        "  \n",
        "  # Model Accuracy, how often is the classifier correct?\n",
        "  print(\"Accuracy for \", n_est, \"estimators: \", curr_score*100,\"%\")"
      ],
      "id": "76e68822"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CeEKIKfWTYne"
      },
      "outputs": [],
      "source": [
        "model = XGBClassifier(n_estimators=115, max_depth=3, learning_rate=0.2)\n",
        "model.fit(X_train,y_train)\n",
        "y_pred=model.predict(X_test)"
      ],
      "id": "CeEKIKfWTYne"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BuEKTHk4-vi-"
      },
      "source": [
        "## Neural Net Classifier"
      ],
      "id": "BuEKTHk4-vi-"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WZTXurg-zHS"
      },
      "outputs": [],
      "source": [
        "kfold = KFold(n_splits=5)"
      ],
      "id": "0WZTXurg-zHS"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAdgMFIc-6fu"
      },
      "outputs": [],
      "source": [
        "earlystop1 = EarlyStopping(patience=7)\n",
        "earlystop2 = ReduceLROnPlateau(monitor='val_loss',\n",
        "                               min_lr = 3e-7, \n",
        "                               patience = 4,\n",
        "                               factor=0.3,\n",
        "                               verbose = 1)"
      ],
      "id": "lAdgMFIc-6fu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "60KnJ1_F-zrc",
        "outputId": "44afe4dd-b1d8-43cd-b519-3304d08ed483"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "10851/10851 [==============================] - 40s 3ms/step - loss: 4.1202 - sparse_categorical_accuracy: 0.2205 - val_loss: 3.0987 - val_sparse_categorical_accuracy: 0.3693 - lr: 3.0000e-05\n",
            "Epoch 2/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 3.1798 - sparse_categorical_accuracy: 0.3271 - val_loss: 2.7891 - val_sparse_categorical_accuracy: 0.3968 - lr: 3.0000e-05\n",
            "Epoch 3/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.9163 - sparse_categorical_accuracy: 0.3580 - val_loss: 2.6832 - val_sparse_categorical_accuracy: 0.4041 - lr: 3.0000e-05\n",
            "Epoch 4/100\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 2.7858 - sparse_categorical_accuracy: 0.3752 - val_loss: 2.6328 - val_sparse_categorical_accuracy: 0.4070 - lr: 3.0000e-05\n",
            "Epoch 5/100\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 2.7062 - sparse_categorical_accuracy: 0.3861 - val_loss: 2.6011 - val_sparse_categorical_accuracy: 0.4085 - lr: 3.0000e-05\n",
            "Epoch 6/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.6519 - sparse_categorical_accuracy: 0.3929 - val_loss: 2.5709 - val_sparse_categorical_accuracy: 0.4105 - lr: 3.0000e-05\n",
            "Epoch 7/100\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 2.6124 - sparse_categorical_accuracy: 0.3989 - val_loss: 2.5504 - val_sparse_categorical_accuracy: 0.4122 - lr: 3.0000e-05\n",
            "Epoch 8/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.5831 - sparse_categorical_accuracy: 0.4026 - val_loss: 2.5402 - val_sparse_categorical_accuracy: 0.4127 - lr: 3.0000e-05\n",
            "Epoch 9/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.5585 - sparse_categorical_accuracy: 0.4054 - val_loss: 2.5282 - val_sparse_categorical_accuracy: 0.4123 - lr: 3.0000e-05\n",
            "Epoch 10/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.5400 - sparse_categorical_accuracy: 0.4080 - val_loss: 2.5186 - val_sparse_categorical_accuracy: 0.4128 - lr: 3.0000e-05\n",
            "Epoch 11/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.5235 - sparse_categorical_accuracy: 0.4103 - val_loss: 2.5042 - val_sparse_categorical_accuracy: 0.4145 - lr: 3.0000e-05\n",
            "Epoch 12/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.5108 - sparse_categorical_accuracy: 0.4118 - val_loss: 2.4997 - val_sparse_categorical_accuracy: 0.4158 - lr: 3.0000e-05\n",
            "Epoch 13/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4971 - sparse_categorical_accuracy: 0.4133 - val_loss: 2.4949 - val_sparse_categorical_accuracy: 0.4153 - lr: 3.0000e-05\n",
            "Epoch 14/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4870 - sparse_categorical_accuracy: 0.4148 - val_loss: 2.4893 - val_sparse_categorical_accuracy: 0.4145 - lr: 3.0000e-05\n",
            "Epoch 15/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4780 - sparse_categorical_accuracy: 0.4151 - val_loss: 2.4839 - val_sparse_categorical_accuracy: 0.4152 - lr: 3.0000e-05\n",
            "Epoch 16/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4706 - sparse_categorical_accuracy: 0.4165 - val_loss: 2.4815 - val_sparse_categorical_accuracy: 0.4146 - lr: 3.0000e-05\n",
            "Epoch 17/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4618 - sparse_categorical_accuracy: 0.4173 - val_loss: 2.4780 - val_sparse_categorical_accuracy: 0.4150 - lr: 3.0000e-05\n",
            "Epoch 18/100\n",
            "10851/10851 [==============================] - 32s 3ms/step - loss: 2.4571 - sparse_categorical_accuracy: 0.4181 - val_loss: 2.4712 - val_sparse_categorical_accuracy: 0.4152 - lr: 3.0000e-05\n",
            "Epoch 19/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4508 - sparse_categorical_accuracy: 0.4186 - val_loss: 2.4692 - val_sparse_categorical_accuracy: 0.4151 - lr: 3.0000e-05\n",
            "Epoch 20/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4447 - sparse_categorical_accuracy: 0.4199 - val_loss: 2.4677 - val_sparse_categorical_accuracy: 0.4163 - lr: 3.0000e-05\n",
            "Epoch 21/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4376 - sparse_categorical_accuracy: 0.4202 - val_loss: 2.4698 - val_sparse_categorical_accuracy: 0.4149 - lr: 3.0000e-05\n",
            "Epoch 22/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4346 - sparse_categorical_accuracy: 0.4204 - val_loss: 2.4618 - val_sparse_categorical_accuracy: 0.4169 - lr: 3.0000e-05\n",
            "Epoch 23/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4306 - sparse_categorical_accuracy: 0.4206 - val_loss: 2.4612 - val_sparse_categorical_accuracy: 0.4151 - lr: 3.0000e-05\n",
            "Epoch 24/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4264 - sparse_categorical_accuracy: 0.4211 - val_loss: 2.4581 - val_sparse_categorical_accuracy: 0.4151 - lr: 3.0000e-05\n",
            "Epoch 25/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4231 - sparse_categorical_accuracy: 0.4214 - val_loss: 2.4542 - val_sparse_categorical_accuracy: 0.4167 - lr: 3.0000e-05\n",
            "Epoch 26/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4185 - sparse_categorical_accuracy: 0.4220 - val_loss: 2.4525 - val_sparse_categorical_accuracy: 0.4164 - lr: 3.0000e-05\n",
            "Epoch 27/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4147 - sparse_categorical_accuracy: 0.4228 - val_loss: 2.4560 - val_sparse_categorical_accuracy: 0.4159 - lr: 3.0000e-05\n",
            "Epoch 28/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4130 - sparse_categorical_accuracy: 0.4220 - val_loss: 2.4512 - val_sparse_categorical_accuracy: 0.4167 - lr: 3.0000e-05\n",
            "Epoch 29/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4096 - sparse_categorical_accuracy: 0.4230 - val_loss: 2.4492 - val_sparse_categorical_accuracy: 0.4162 - lr: 3.0000e-05\n",
            "Epoch 30/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4061 - sparse_categorical_accuracy: 0.4235 - val_loss: 2.4454 - val_sparse_categorical_accuracy: 0.4172 - lr: 3.0000e-05\n",
            "Epoch 31/100\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 2.4043 - sparse_categorical_accuracy: 0.4233 - val_loss: 2.4512 - val_sparse_categorical_accuracy: 0.4158 - lr: 3.0000e-05\n",
            "Epoch 32/100\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 2.4021 - sparse_categorical_accuracy: 0.4235 - val_loss: 2.4459 - val_sparse_categorical_accuracy: 0.4160 - lr: 3.0000e-05\n",
            "Epoch 33/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3987 - sparse_categorical_accuracy: 0.4242 - val_loss: 2.4382 - val_sparse_categorical_accuracy: 0.4180 - lr: 3.0000e-05\n",
            "Epoch 34/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3958 - sparse_categorical_accuracy: 0.4245 - val_loss: 2.4466 - val_sparse_categorical_accuracy: 0.4166 - lr: 3.0000e-05\n",
            "Epoch 35/100\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 2.3945 - sparse_categorical_accuracy: 0.4239 - val_loss: 2.4414 - val_sparse_categorical_accuracy: 0.4162 - lr: 3.0000e-05\n",
            "Epoch 36/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3934 - sparse_categorical_accuracy: 0.4247 - val_loss: 2.4388 - val_sparse_categorical_accuracy: 0.4166 - lr: 3.0000e-05\n",
            "Epoch 37/100\n",
            "10843/10851 [============================>.] - ETA: 0s - loss: 2.3892 - sparse_categorical_accuracy: 0.4246\n",
            "Epoch 37: ReduceLROnPlateau reducing learning rate to 8.999999772640877e-06.\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 2.3893 - sparse_categorical_accuracy: 0.4246 - val_loss: 2.4413 - val_sparse_categorical_accuracy: 0.4172 - lr: 3.0000e-05\n",
            "Epoch 38/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3869 - sparse_categorical_accuracy: 0.4251 - val_loss: 2.4369 - val_sparse_categorical_accuracy: 0.4170 - lr: 9.0000e-06\n",
            "Epoch 39/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3843 - sparse_categorical_accuracy: 0.4250 - val_loss: 2.4375 - val_sparse_categorical_accuracy: 0.4166 - lr: 9.0000e-06\n",
            "Epoch 40/100\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 2.3854 - sparse_categorical_accuracy: 0.4255 - val_loss: 2.4389 - val_sparse_categorical_accuracy: 0.4165 - lr: 9.0000e-06\n",
            "Epoch 41/100\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 2.3853 - sparse_categorical_accuracy: 0.4253 - val_loss: 2.4363 - val_sparse_categorical_accuracy: 0.4172 - lr: 9.0000e-06\n",
            "Epoch 42/100\n",
            "10851/10851 [==============================] - 35s 3ms/step - loss: 2.3844 - sparse_categorical_accuracy: 0.4250 - val_loss: 2.4397 - val_sparse_categorical_accuracy: 0.4159 - lr: 9.0000e-06\n",
            "Epoch 43/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3839 - sparse_categorical_accuracy: 0.4253 - val_loss: 2.4365 - val_sparse_categorical_accuracy: 0.4173 - lr: 9.0000e-06\n",
            "Epoch 44/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3835 - sparse_categorical_accuracy: 0.4254 - val_loss: 2.4351 - val_sparse_categorical_accuracy: 0.4167 - lr: 9.0000e-06\n",
            "Epoch 45/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3823 - sparse_categorical_accuracy: 0.4255 - val_loss: 2.4382 - val_sparse_categorical_accuracy: 0.4173 - lr: 9.0000e-06\n",
            "Epoch 46/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3819 - sparse_categorical_accuracy: 0.4257 - val_loss: 2.4350 - val_sparse_categorical_accuracy: 0.4164 - lr: 9.0000e-06\n",
            "Epoch 47/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3816 - sparse_categorical_accuracy: 0.4256 - val_loss: 2.4364 - val_sparse_categorical_accuracy: 0.4167 - lr: 9.0000e-06\n",
            "Epoch 48/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3798 - sparse_categorical_accuracy: 0.4259 - val_loss: 2.4347 - val_sparse_categorical_accuracy: 0.4165 - lr: 9.0000e-06\n",
            "Epoch 49/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3802 - sparse_categorical_accuracy: 0.4261 - val_loss: 2.4364 - val_sparse_categorical_accuracy: 0.4165 - lr: 9.0000e-06\n",
            "Epoch 50/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3800 - sparse_categorical_accuracy: 0.4259 - val_loss: 2.4353 - val_sparse_categorical_accuracy: 0.4167 - lr: 9.0000e-06\n",
            "Epoch 51/100\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 2.3795 - sparse_categorical_accuracy: 0.4251 - val_loss: 2.4328 - val_sparse_categorical_accuracy: 0.4176 - lr: 9.0000e-06\n",
            "Epoch 52/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3792 - sparse_categorical_accuracy: 0.4260 - val_loss: 2.4341 - val_sparse_categorical_accuracy: 0.4164 - lr: 9.0000e-06\n",
            "Epoch 53/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3792 - sparse_categorical_accuracy: 0.4262 - val_loss: 2.4326 - val_sparse_categorical_accuracy: 0.4166 - lr: 9.0000e-06\n",
            "Epoch 54/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3777 - sparse_categorical_accuracy: 0.4257 - val_loss: 2.4309 - val_sparse_categorical_accuracy: 0.4171 - lr: 9.0000e-06\n",
            "Epoch 55/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3770 - sparse_categorical_accuracy: 0.4258 - val_loss: 2.4342 - val_sparse_categorical_accuracy: 0.4170 - lr: 9.0000e-06\n",
            "Epoch 56/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3763 - sparse_categorical_accuracy: 0.4258 - val_loss: 2.4334 - val_sparse_categorical_accuracy: 0.4169 - lr: 9.0000e-06\n",
            "Epoch 57/100\n",
            "10851/10851 [==============================] - 32s 3ms/step - loss: 2.3758 - sparse_categorical_accuracy: 0.4262 - val_loss: 2.4325 - val_sparse_categorical_accuracy: 0.4176 - lr: 9.0000e-06\n",
            "Epoch 58/100\n",
            "10837/10851 [============================>.] - ETA: 0s - loss: 2.3766 - sparse_categorical_accuracy: 0.4260\n",
            "Epoch 58: ReduceLROnPlateau reducing learning rate to 2.6999998226528985e-06.\n",
            "10851/10851 [==============================] - 32s 3ms/step - loss: 2.3768 - sparse_categorical_accuracy: 0.4260 - val_loss: 2.4333 - val_sparse_categorical_accuracy: 0.4167 - lr: 9.0000e-06\n",
            "Epoch 59/100\n",
            "10851/10851 [==============================] - 32s 3ms/step - loss: 2.3754 - sparse_categorical_accuracy: 0.4264 - val_loss: 2.4322 - val_sparse_categorical_accuracy: 0.4176 - lr: 2.7000e-06\n",
            "Epoch 60/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3753 - sparse_categorical_accuracy: 0.4263 - val_loss: 2.4328 - val_sparse_categorical_accuracy: 0.4171 - lr: 2.7000e-06\n",
            "Epoch 61/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3739 - sparse_categorical_accuracy: 0.4264 - val_loss: 2.4333 - val_sparse_categorical_accuracy: 0.4166 - lr: 2.7000e-06\n",
            "Score for fold 1: loss of 2.438161611557007; sparse_categorical_accuracy of 41.79641902446747%\n"
          ]
        }
      ],
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "fold_no = 1\n",
        "checkpoint_filepath='/tmp/checkpoint'\n",
        "# for train, test in kfold.split(X_train, y_train):\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(len(X_train.iloc[0]),)),\n",
        "    tf.keras.layers.Dense(90),\n",
        "    tf.keras.layers.Dropout(.2),\n",
        "    tf.keras.layers.Dense(90),\n",
        "    tf.keras.layers.Dropout(.2),\n",
        "    tf.keras.layers.Dense(max(y_train)+1, activation='softmax')\n",
        "])\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(3e-5), loss=loss_fn, \n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "print('------------------------------------------------------------------------')\n",
        "print(f'Training for fold {fold_no} ...')\n",
        "history = model.fit(np.array(X_train), np.array(y_train),\n",
        "                    batch_size = 64,\n",
        "                    epochs=100,\n",
        "                    use_multiprocessing=True,\n",
        "                    workers=4,\n",
        "                    validation_data=(np.array(X_test), np.array(y_test)),\n",
        "                    callbacks=[earlystop1, earlystop2,\n",
        "                                tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                                save_best_only=True,\n",
        "                                                                save_weights_only=True,\n",
        "                                                                monitor='val_sparse_categorical_accuracy',\n",
        "                                                                mode='max')])\n",
        "model.load_weights(checkpoint_filepath)\n",
        "scores = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
        "print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "acc_per_fold.append(scores[1] * 100)\n",
        "loss_per_fold.append(scores[0])\n",
        "fold_no += 1"
      ],
      "id": "60KnJ1_F-zrc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "HjmrU2XSCxpO",
        "outputId": "5ddcf4a1-7569-4f44-83b7-ed295b178d14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1311/1311 [==============================] - 2s 1ms/step\n",
            "Accuracy: 41.796418607978254 %\n"
          ]
        }
      ],
      "source": [
        "from sklearn import metrics\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\",metrics.accuracy_score(y_test, [np.argmax(i) for i in y_pred])*100,\"%\")"
      ],
      "id": "HjmrU2XSCxpO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KEwawyTEgp9",
        "outputId": "2b2637e2-5272-4371-83c6-38994253899f"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "5"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(y_test)"
      ],
      "id": "9KEwawyTEgp9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l2AuOQjvDA75",
        "outputId": "92464a14-e3f0-4f00-ef67-7aa32be7bb30"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4"
            ]
          },
          "execution_count": 74,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "np.argmax(y_pred[1])"
      ],
      "id": "l2AuOQjvDA75"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HrSfBAemDKp0",
        "outputId": "077ecf9a-2092-4c21-84ce-13c5db567585"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "2368    1\n",
              "895     5\n",
              "725     2\n",
              "485     2\n",
              "634     2\n",
              "       ..\n",
              "496     2\n",
              "497     6\n",
              "1584    1\n",
              "1345    6\n",
              "1627    1\n",
              "Name: Class, Length: 1083, dtype: int64"
            ]
          },
          "execution_count": 143,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test"
      ],
      "id": "HrSfBAemDKp0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM7yhWVDJkBv"
      },
      "outputs": [],
      "source": [
        "y_pred = [np.argmax(i) for i in y_pred]"
      ],
      "id": "dM7yhWVDJkBv"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D3FVEpYhRhJE",
        "outputId": "61996510-356c-4657-dbaa-3f3b94c107f3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 126,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(y_pred)"
      ],
      "id": "D3FVEpYhRhJE"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0JuJ4ymRUJFC"
      },
      "outputs": [],
      "source": [
        "adj = A.copy()"
      ],
      "id": "0JuJ4ymRUJFC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96edc53e",
        "outputId": "910a4d1b-9479-4577-f808-6b0c93353751"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[5 2 0 ... 2 2 2]\n"
          ]
        }
      ],
      "source": [
        "updated_class_60=np.array(catagories)\n",
        "i=0\n",
        "for d in test_splits[0]:\n",
        "    class_60[d]=y_pred[i]\n",
        "    i=i+1\n",
        "print(class_60)"
      ],
      "id": "96edc53e"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "947eb8be",
        "outputId": "e8c988b3-1d3b-4326-c006-f6e88985691d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[5 0 0 ... 0 0 0]\n",
            " [0 2 0 ... 0 0 0]\n",
            " [0 0 0 ... 0 0 0]\n",
            " ...\n",
            " [0 0 0 ... 2 0 0]\n",
            " [0 0 0 ... 0 2 0]\n",
            " [0 0 0 ... 0 0 2]]\n"
          ]
        }
      ],
      "source": [
        "for i in range(n):\n",
        "    adj[i][i]=class_60[i]\n",
        "print(adj)"
      ],
      "id": "947eb8be"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V4SQ9OMIPFPY",
        "outputId": "87320d84-0bc4-419e-dbb4-f698c57a41f6"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 129,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "max(class_60)"
      ],
      "id": "V4SQ9OMIPFPY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3aa8d0c2",
        "outputId": "a4caf248-239a-4531-b01f-d112577e80f2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Counter()\n"
          ]
        }
      ],
      "source": [
        "# Node_class=[0,1,2,3,4,5,6]\n",
        "# F_vec=[]\n",
        "# for i in range(n):\n",
        "#     node_F=[]\n",
        "#     list_out=[]\n",
        "#     list_In=[]\n",
        "#     for j in range(n):\n",
        "#         if A[i][j]==1 and i!=j:\n",
        "#             list_out.append(A[j][j])\n",
        "#         if A[j][i]==1 and i!=j:\n",
        "#             list_In.append(A[j][j])\n",
        "#     for d in Node_class:\n",
        "#         count=0\n",
        "#         count_in=0\n",
        "#         for k in range(len(list_out)):\n",
        "#             if list_out[k]==d:\n",
        "#                 count=count+1\n",
        "#         node_F.append(count)\n",
        "#         for k in range(len(list_In)):\n",
        "#             if list_In[k]==d:\n",
        "#                 count_in=count_in+1\n",
        "#         node_F.append(count_in)\n",
        "#     F_vec.append(node_F)\n",
        "\n",
        "F_vec = create_feature_vectors(adj, max(catagories) + 1)"
      ],
      "id": "3aa8d0c2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9KMYw03Pesp",
        "outputId": "36b45d4d-7441-43d4-dda5-4619e4996dac"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(2708, 2708)"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "adj.shape"
      ],
      "id": "d9KMYw03Pesp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfBQ75c3kLdh"
      },
      "source": [
        "# Adding Keyword Features to input vectors"
      ],
      "id": "zfBQ75c3kLdh"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9iraU6fo9rcd"
      },
      "outputs": [],
      "source": [
        "data_by_class = {cls: Data.loc[Data['class'] == cls].drop(['class'], axis=1) for cls in range(max(catagories) + 1)}"
      ],
      "id": "9iraU6fo9rcd"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RFt60AFxi_X"
      },
      "outputs": [],
      "source": [
        "data_by_class"
      ],
      "id": "-RFt60AFxi_X"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e_6WW0ck_iT8"
      },
      "outputs": [],
      "source": [
        "basis = [[max(df[f'w_{i-1}']) for i in range(1, len(df.columns))] for df in data_by_class.values()]"
      ],
      "id": "e_6WW0ck_iT8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIn4Bzt02jOQ",
        "outputId": "01277930-5a98-4e14-98c1-51491b754d3d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "execution_count": 95,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "basis[0][-1]"
      ],
      "id": "cIn4Bzt02jOQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fW9wjRL-zP7c",
        "outputId": "5b64d67f-ad5c-4df5-8b83-40e4d64bc33f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "449\n",
            "984\n"
          ]
        }
      ],
      "source": [
        "count0=0\n",
        "count1=0\n",
        "for i in range(1433):\n",
        "    if basis[0][i]==1:\n",
        "        count1=count1+1\n",
        "    else:\n",
        "        count0=count0+1\n",
        "print(count0)\n",
        "print(count1)"
      ],
      "id": "fW9wjRL-zP7c"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXwbNn-KvCOJ"
      },
      "outputs": [],
      "source": [
        "sel_basis = [[int(list(df[f'w_{i-1}'].to_numpy()).count(1) >= int(len(df[f'w_{i-1}'].index)*0.1)) \n",
        "              for i in range(1, len(df.columns))]\n",
        "             for df in data_by_class.values()] if node_data_name != \"data/Pubmed-Diabetes.NODE.paper.tab\" else\\\n",
        "             [[int(np.count_nonzero(list(df.iloc[:, i].astype('float'))) >= int(len(df.index)*0.25)) \n",
        "              for i in range(1, len(df.columns))] for df in data_by_class.values()]"
      ],
      "id": "nXwbNn-KvCOJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cK0u9weXs0ih"
      },
      "outputs": [],
      "source": [
        "for i in range(1, len(data_by_class[0].columns)):\n",
        "  print(f'FOR COLUMN {i}:')\n",
        "  print(data_by_class[0].iloc[:,i])\n",
        "  print(data_by_class[0].iloc[:,i].astype('float'))"
      ],
      "id": "cK0u9weXs0ih"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oI7GN57tsjka",
        "outputId": "18534fd0-598d-478b-e0ae-9f7b4e0b12f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "467\n",
            "33\n"
          ]
        }
      ],
      "source": [
        "count0=0\n",
        "count1=0\n",
        "for i in range(500):\n",
        "    if sel_basis[0][i]==1:\n",
        "        count1=count1+1\n",
        "    else:\n",
        "        count0=count0+1\n",
        "print(count0)\n",
        "print(count1)"
      ],
      "id": "oI7GN57tsjka"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kabLU5xQyqHQ",
        "outputId": "49317ef5-aa6e-4d98-ff4a-ab018a7e1149"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0    295\n",
              "1      3\n",
              "Name: w_3, dtype: int64"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data_by_class[0]['w_3'].value_counts()"
      ],
      "id": "kabLU5xQyqHQ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gkzlHHNFwq49",
        "outputId": "2abbb949-b14a-4e33-c0f4-0def132b6453"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1414\n",
            "19\n"
          ]
        }
      ],
      "source": [
        "count0=0\n",
        "count1=0\n",
        "for i in range(1433):\n",
        "    if sel_basis[0][i]==1:\n",
        "        count1=count1+1\n",
        "    else:\n",
        "        count0=count0+1\n",
        "print(count0)\n",
        "print(count1)"
      ],
      "id": "gkzlHHNFwq49"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rt-NfRhthAiY",
        "outputId": "5634f720-1f03-447b-8384-0a1c2e50d73a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6"
            ]
          },
          "execution_count": 86,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(basis)"
      ],
      "id": "Rt-NfRhthAiY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "any5JOQpuDaq"
      },
      "outputs": [],
      "source": [
        "data_array = data_s.astype(float).to_numpy()  "
      ],
      "id": "any5JOQpuDaq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gsEjPpft1A8h"
      },
      "outputs": [],
      "source": [
        "ID_ind = data_s.columns.get_loc('ID')"
      ],
      "id": "gsEjPpft1A8h"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0HAFT3u6aTD"
      },
      "outputs": [],
      "source": [
        "for row in data_array:\n",
        "  index = node_ID[str(int(row[ID_ind]))]\n",
        "  f = [int(i > 0) for i in row[1:-1]]\n",
        "  features = ([np.dot(f, basis[cls]) for cls in range(max(catagories)+1)]\\\n",
        "             + [np.dot(f,sel_basis[cls]) for cls in range(max(catagories)+1)])\\\n",
        "             if node_data_name != \"data/Pubmed-Diabetes.NODE.paper.tab\" else\\\n",
        "             [np.dot(f,sel_basis[cls]) for cls in range(max(catagories)+1)]\n",
        "  F_vec[index] += features"
      ],
      "id": "i0HAFT3u6aTD"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_R3_iQ3OWqX_",
        "outputId": "b51ef843-0cfc-418b-eceb-0f169af4875a"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 1,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0,\n",
              " 0]"
            ]
          },
          "execution_count": 148,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "f"
      ],
      "id": "_R3_iQ3OWqX_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S36m3EA9rv00",
        "outputId": "cfedd945-ad7c-4a0c-9e65-a3ac1204010e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[[0, 0, 0, 2, 0, 0, 0, 0, 0, 8, 0, 0, 8, 8, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 11, 13],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 6, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 6, 8],\n",
              " [0, 2, 0, 1, 0, 11, 0, 11, 0, 3, 0, 20, 7, 7, 8],\n",
              " [2, 9, 1, 2, 2, 2, 8, 12, 6, 2, 0, 6, 12, 13, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 17, 18],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 11, 11, 14],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 6, 7, 7],\n",
              " [0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 8, 8, 12],\n",
              " [0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 9, 10, 12, 14],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 9, 10, 11],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 6, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 8, 7, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 7, 0, 0, 7, 9, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 13, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 6, 6],\n",
              " [1, 0, 1, 1, 0, 0, 10, 0, 6, 5, 0, 0, 5, 5, 5],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 6, 9, 9],\n",
              " [0, 0, 1, 0, 12, 13, 0, 2, 1, 1, 16, 84, 9, 8, 9],\n",
              " [0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 11, 10, 11],\n",
              " [1, 0, 1, 0, 10, 1, 16, 0, 5, 0, 34, 0, 5, 5, 5],\n",
              " [2, 0, 1, 0, 1, 0, 18, 0, 3, 0, 1, 0, 7, 7, 6],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 2],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 7, 7, 7],\n",
              " [0, 0, 24, 13, 0, 0, 10, 0, 72, 48, 2, 1, 8, 9, 8],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 5, 5],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 1, 5, 6, 6],\n",
              " [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 6, 7, 11, 10],\n",
              " [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 8, 6, 7, 10],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 14, 11, 11],\n",
              " [0, 7, 0, 1, 0, 2, 0, 2, 0, 1, 0, 2, 11, 6, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 14, 17],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 7, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 15, 13, 15],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 8, 8, 8],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 11],\n",
              " [0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 3, 14, 16, 17],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 9, 14, 16],\n",
              " [0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 2, 15, 16, 19],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 4, 4],\n",
              " [0, 0, 0, 5, 0, 0, 0, 0, 0, 6, 0, 0, 3, 3, 3],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 7, 10, 11],\n",
              " [0, 0, 14, 2, 0, 0, 4, 0, 43, 5, 1, 0, 13, 14, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 8, 6, 9],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 9, 10, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 13, 15],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 10, 13],\n",
              " [2, 1, 3, 1, 0, 0, 0, 0, 0, 3, 0, 0, 7, 4, 6],\n",
              " [0, 0, 0, 14, 0, 0, 0, 1, 0, 27, 0, 3, 10, 10, 11],\n",
              " [0, 0, 0, 6, 0, 0, 0, 1, 0, 23, 0, 0, 10, 13, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 12, 11, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 10, 14],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 8, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 9, 9],\n",
              " [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 10, 11, 14],\n",
              " [0, 3, 0, 0, 0, 0, 0, 6, 0, 2, 0, 3, 9, 7, 7],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 11, 13, 14],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 5, 0, 0, 9, 9, 10],\n",
              " [0, 0, 0, 1, 0, 2, 0, 0, 0, 1, 0, 0, 10, 8, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 9, 11],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 8, 10, 9],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 6, 8, 8],\n",
              " [0, 1, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 7, 8, 7],\n",
              " [0, 0, 13, 1, 0, 0, 0, 0, 28, 2, 0, 0, 7, 5, 8],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 9, 12, 15],\n",
              " [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 11, 10, 10],\n",
              " [0, 3, 0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 8, 5, 6],\n",
              " [0, 0, 0, 2, 20, 14, 0, 2, 2, 4, 15, 82, 5, 8, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 5, 5],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 8, 10, 13],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 8, 0, 0, 10, 9, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5],\n",
              " [0, 0, 0, 0, 0, 2, 0, 1, 0, 3, 0, 15, 12, 16, 17],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 9, 9],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 5, 8],\n",
              " [0, 1, 0, 2, 0, 0, 0, 2, 0, 5, 0, 3, 9, 9, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 9, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 11, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 9, 11],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 12, 13, 13],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 5, 5, 6],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 15, 13, 15],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 10, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 13, 11, 13],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 9, 14, 15],\n",
              " [0, 0, 0, 1, 0, 16, 0, 0, 0, 0, 0, 6, 15, 12, 18],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 13, 13, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 16, 16, 20],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 15, 15, 18],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 12, 14, 16],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 6, 6, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 13, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 5, 8],\n",
              " [0, 3, 0, 0, 0, 0, 0, 2, 0, 4, 0, 0, 14, 13, 16],\n",
              " [0, 1, 0, 3, 0, 3, 0, 2, 0, 3, 0, 7, 12, 11, 10],\n",
              " [0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 14, 10, 10],\n",
              " [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 7, 9],\n",
              " [0, 0, 0, 8, 0, 0, 0, 7, 0, 56, 0, 31, 7, 9, 9],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 10, 14, 15],\n",
              " [0, 2, 0, 1, 0, 3, 0, 0, 0, 0, 0, 6, 10, 8, 11],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 9, 12, 13],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 12, 10, 12],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 5, 8, 6, 9],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 6, 8, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 12, 18, 22],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 11, 16],\n",
              " [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 13, 15, 18],\n",
              " [0, 0, 1, 0, 17, 28, 0, 0, 0, 3, 14, 211, 9, 9, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 5, 6],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 9, 10, 11],\n",
              " [8, 1, 18, 5, 0, 0, 69, 0, 126, 8, 4, 1, 7, 7, 8],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 7, 0, 0, 3, 4, 5],\n",
              " [0, 0, 0, 0, 11, 3, 0, 0, 0, 0, 14, 2, 6, 8, 12],\n",
              " [0, 1, 0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 2, 3, 3],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 4, 6, 6],\n",
              " [0, 0, 10, 1, 0, 0, 0, 0, 35, 1, 1, 0, 4, 5, 6],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 4, 3, 5],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 22, 0, 19, 14, 15, 15],\n",
              " [0, 0, 2, 0, 3, 0, 0, 0, 0, 0, 0, 0, 11, 12, 16],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 12, 12, 15],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 4, 6, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 11, 15, 16],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 24, 0, 1, 11, 13, 14],\n",
              " [0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 12, 8, 7, 9],\n",
              " [0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 3, 4, 5],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 14, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 6, 5, 4],\n",
              " [0, 1, 0, 6, 0, 0, 0, 0, 0, 10, 0, 0, 6, 8, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 6, 5, 4],\n",
              " [0, 0, 0, 4, 0, 0, 0, 0, 0, 6, 0, 0, 5, 7, 8],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 6, 0, 0, 5, 8, 8],\n",
              " [0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 8, 5, 5],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 14, 17, 17],\n",
              " [6, 0, 22, 6, 1, 0, 52, 0, 92, 13, 8, 0, 4, 7, 7],\n",
              " [0, 0, 2, 0, 1, 0, 0, 0, 2, 0, 0, 0, 10, 9, 13],\n",
              " [0, 0, 22, 1, 0, 0, 16, 0, 42, 0, 3, 0, 6, 6, 5],\n",
              " [0, 0, 21, 1, 0, 0, 3, 0, 81, 0, 0, 0, 11, 12, 13],\n",
              " [0, 0, 9, 0, 0, 0, 0, 0, 14, 0, 1, 0, 7, 9, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 5, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 6, 8, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 3, 4, 4],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 8, 0, 0, 6, 5, 6],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 11, 12, 15],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 0, 8, 7, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 4, 6],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 7, 5, 5],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 10, 12, 15],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 10],\n",
              " [0, 0, 11, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 10, 10],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 13, 16, 21],\n",
              " [0, 0, 0, 5, 0, 0, 0, 0, 0, 25, 0, 0, 8, 6, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 7, 9, 9],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 11, 13, 17],\n",
              " [0, 0, 0, 0, 0, 2, 0, 1, 0, 9, 0, 47, 13, 14, 15],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 8, 9, 14],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 12, 11, 12],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 15, 12, 14],\n",
              " [0, 1, 0, 0, 0, 6, 0, 0, 0, 0, 0, 6, 5, 7, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 13, 11, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 17, 18],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 10, 15, 17],\n",
              " [0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 2, 12, 12, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 11],\n",
              " [0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 11, 9, 14],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 9, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 6, 7, 7],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 14, 11, 14],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 12, 10, 12],\n",
              " [1, 0, 2, 0, 4, 0, 22, 0, 1, 0, 0, 0, 7, 9, 12],\n",
              " [1, 1, 7, 0, 0, 0, 25, 0, 50, 0, 0, 0, 5, 6, 5],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 10, 11],\n",
              " [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 5, 8, 10],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 9, 9],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 2, 5, 4],\n",
              " [0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 1, 13, 12, 16],\n",
              " [0, 1, 0, 1, 0, 0, 0, 0, 0, 19, 0, 0, 4, 5, 5],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 15, 16],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 14, 14],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 6, 9, 7],\n",
              " [6, 0, 1, 0, 7, 0, 3, 0, 1, 0, 1, 0, 4, 5, 5],\n",
              " [0, 0, 0, 12, 0, 0, 0, 0, 0, 27, 0, 0, 4, 5, 5],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 6, 5],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 12, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 7, 11],\n",
              " [0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 6, 9, 7, 7],\n",
              " [0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 5, 5, 6],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 35, 0, 0, 5, 7, 6],\n",
              " [0, 0, 0, 7, 0, 0, 0, 0, 0, 10, 0, 0, 7, 8, 10],\n",
              " [0, 0, 0, 5, 0, 0, 0, 0, 0, 17, 0, 0, 8, 6, 8],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 8, 10, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 1, 0, 24, 0, 0, 9, 11, 9],\n",
              " [1, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 10, 12, 13],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 8, 9, 8],\n",
              " [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 9, 10, 11],\n",
              " [0, 0, 0, 4, 0, 0, 0, 0, 0, 7, 0, 0, 9, 8, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 9, 10],\n",
              " [0, 1, 0, 9, 0, 0, 0, 2, 0, 45, 0, 0, 10, 10, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 15, 19],\n",
              " [0, 0, 0, 0, 0, 3, 0, 2, 0, 0, 0, 22, 14, 15, 18],\n",
              " [0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 9],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 6, 8, 8],\n",
              " [0, 0, 0, 0, 0, 19, 0, 2, 0, 2, 0, 88, 8, 11, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 10, 11],\n",
              " [0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 13, 14, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 4, 10, 10, 13],\n",
              " [0, 0, 0, 4, 0, 1, 0, 0, 0, 5, 0, 0, 11, 11, 17],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 12, 14, 16],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 10, 13, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 12, 16],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 10, 11, 17],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 9, 13, 18],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 13, 14, 16],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 8, 10, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 13, 13, 18],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 10, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 8, 0, 0, 6, 9, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 13],\n",
              " [0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 7, 8, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 7, 8, 11],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 10, 13, 17],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 9, 10, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 9, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 12, 12, 17],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 1, 13, 13, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 10, 14],\n",
              " [0, 0, 0, 2, 0, 1, 0, 0, 0, 1, 0, 0, 9, 12, 16],\n",
              " [0, 1, 17, 15, 0, 1, 0, 3, 11, 63, 0, 29, 19, 19, 23],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 11, 13, 16],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 15, 17, 18],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 14, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 12, 11],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 9, 12, 14],\n",
              " [0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 15, 10, 14, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 14, 18],\n",
              " [0, 0, 0, 2, 0, 6, 0, 0, 0, 1, 0, 4, 7, 8, 9],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 9, 9, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 6, 0, 0, 9, 10, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 11, 9, 10],\n",
              " [0, 0, 13, 2, 2, 0, 0, 0, 0, 10, 0, 1, 13, 15, 17],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 7],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 9],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 8, 10, 14],\n",
              " [0, 0, 0, 1, 0, 6, 0, 0, 0, 1, 0, 13, 9, 10, 11],\n",
              " [2, 0, 5, 0, 5, 0, 0, 0, 1, 0, 0, 0, 7, 10, 13],\n",
              " [8, 0, 20, 0, 0, 0, 67, 0, 63, 0, 1, 0, 4, 7, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 15, 11, 11],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 11, 12, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 9, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 7, 9, 9],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 10, 9, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 12, 15],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 16, 16, 17],\n",
              " [0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 5, 6, 10, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 8, 8, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 6, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 10, 11],\n",
              " [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 14, 11, 13],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 13, 15, 15],\n",
              " [0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 6, 7, 10],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 5, 5],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 15, 15, 15],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 8, 6, 7],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 8, 8, 10],\n",
              " [0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 2, 9, 9, 9],\n",
              " [0, 0, 0, 1, 0, 22, 0, 0, 0, 2, 0, 18, 14, 16, 21],\n",
              " [3, 0, 10, 4, 0, 0, 0, 0, 0, 5, 0, 0, 5, 6, 6],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 5],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 2, 11, 15, 16],\n",
              " [0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 0, 5, 6, 5, 6],\n",
              " [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 10, 7, 8, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 12, 15],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 9, 9, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 14, 11, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 15, 15, 20],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 3, 10, 10, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6],\n",
              " [0, 0, 0, 5, 0, 0, 0, 0, 0, 9, 0, 0, 7, 8, 11],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 9, 7, 9],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 15, 16, 21],\n",
              " [0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 12, 14, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 11, 14],\n",
              " [0, 0, 0, 4, 0, 1, 0, 0, 0, 1, 0, 2, 9, 10, 11],\n",
              " [0, 0, 0, 2, 0, 2, 0, 0, 0, 1, 0, 0, 13, 15, 16],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 14, 17, 22],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 13, 17],\n",
              " [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 14, 13],\n",
              " [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 7, 9, 9],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 6, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 12, 16],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 15, 0, 0, 5, 4, 5],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 6, 6, 8],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 11, 12, 17],\n",
              " [2, 0, 22, 1, 18, 2, 0, 0, 2, 0, 1, 0, 10, 9, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 14, 14, 18],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 10, 11, 17],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 10, 0, 1, 8, 9, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 13, 14],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 8, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 10, 7, 10, 10],\n",
              " [0, 3, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 5, 5],\n",
              " [0, 0, 0, 5, 0, 0, 0, 0, 0, 26, 0, 0, 10, 11, 10],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 5, 4, 5],\n",
              " [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 13, 15],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 3, 9, 10, 11],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 1, 12, 14, 16],\n",
              " [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 8, 9, 11, 13],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 7, 8, 9, 11],\n",
              " [0, 0, 0, 1, 0, 9, 0, 0, 0, 0, 0, 21, 13, 11, 19],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 5, 11, 12, 17],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 15],\n",
              " [0, 0, 14, 1, 0, 0, 0, 0, 16, 1, 3, 0, 12, 14, 16],\n",
              " [0, 1, 0, 5, 0, 1, 0, 0, 0, 6, 0, 0, 7, 9, 13],\n",
              " [0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 6, 8, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 12, 9],\n",
              " [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 10, 13],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 7, 11, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 7, 7, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 6, 8, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 13, 9, 11],\n",
              " [0, 3, 0, 0, 0, 0, 0, 7, 0, 0, 0, 6, 8, 5, 9],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 11, 12],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 11, 13, 14],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 21, 0, 19, 5, 8, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 15, 19],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 7],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 9, 10, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 10, 13],\n",
              " [0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 6, 7, 8],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 12, 11, 14],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 5, 6],\n",
              " [1, 0, 12, 0, 3, 1, 11, 0, 10, 0, 4, 0, 8, 9, 8],\n",
              " [0, 0, 12, 1, 0, 0, 1, 0, 17, 1, 0, 0, 5, 9, 9],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 8, 9, 9],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 6],\n",
              " [0, 0, 1, 0, 2, 1, 0, 0, 0, 0, 0, 0, 8, 11, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 11, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 9, 9, 12],\n",
              " [0, 0, 0, 0, 0, 9, 0, 0, 0, 1, 0, 5, 14, 15, 18],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 7, 9, 10],\n",
              " [0, 2, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 10, 9, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 6, 8, 9],\n",
              " [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 7, 5, 5],\n",
              " [1, 0, 20, 0, 0, 0, 4, 0, 67, 0, 2, 0, 12, 11, 12],\n",
              " [4, 0, 12, 2, 0, 0, 32, 1, 30, 8, 0, 0, 5, 6, 5],\n",
              " [5, 1, 3, 0, 0, 0, 0, 0, 6, 1, 0, 0, 7, 5, 6],\n",
              " [0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 10, 7, 7],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 8, 9],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 6, 7, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 12, 16],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 7, 8, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 15],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 14, 13, 15],\n",
              " [0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 8, 6, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 11],\n",
              " [0, 0, 0, 0, 6, 0, 0, 0, 2, 0, 25, 0, 9, 11, 14],\n",
              " [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 13, 9, 12],\n",
              " [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 11, 13],\n",
              " [4, 0, 6, 5, 0, 0, 16, 0, 39, 20, 0, 0, 3, 4, 3],\n",
              " [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 7, 9],\n",
              " [0, 0, 0, 0, 0, 8, 0, 1, 0, 0, 0, 14, 9, 9, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 14, 17],\n",
              " [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 8, 6, 5],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 14, 16],\n",
              " [0, 0, 0, 5, 0, 0, 0, 0, 0, 11, 0, 0, 4, 4, 5],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 9],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 5, 0, 0, 5, 9, 8],\n",
              " [0, 1, 0, 4, 0, 0, 0, 0, 0, 5, 0, 0, 4, 3, 3],\n",
              " [1, 0, 2, 2, 1, 0, 0, 1, 0, 0, 0, 1, 12, 15, 21],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 4],\n",
              " [0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 0, 7, 6, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 4, 7, 8],\n",
              " [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 14, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 8, 7, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 4, 4, 4],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 10, 10, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 12, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 8, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 10, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 10, 8, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 9, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 9, 9],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 6, 8, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 9, 10, 11],\n",
              " [0, 2, 0, 0, 0, 0, 0, 5, 0, 0, 0, 1, 9, 5, 7],\n",
              " [0, 0, 0, 0, 0, 9, 0, 3, 0, 0, 0, 31, 11, 13, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 14, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 8, 8],\n",
              " [0, 0, 0, 5, 0, 0, 0, 0, 0, 6, 0, 0, 7, 7, 7],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 12, 13, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 10, 12],\n",
              " [4, 0, 13, 8, 0, 0, 43, 0, 53, 26, 1, 0, 5, 7, 7],\n",
              " [7, 0, 18, 8, 1, 0, 30, 0, 60, 38, 5, 0, 4, 6, 7],\n",
              " [0, 2, 0, 0, 0, 0, 0, 2, 0, 1, 0, 0, 7, 6, 10],\n",
              " [0, 1, 0, 0, 0, 2, 0, 2, 0, 2, 0, 2, 4, 6, 5],\n",
              " [0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 10, 11, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 13, 15],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 7, 9, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 7, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 9, 11, 14],\n",
              " [19, 5, 1, 1, 1, 0, 154, 4, 15, 3, 6, 2, 13, 12, 13],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 3, 2, 3],\n",
              " [0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 7, 7, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 11, 11, 10],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 10, 9, 10],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 12, 10, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 11, 17],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6],\n",
              " [0, 1, 0, 3, 0, 0, 0, 3, 0, 11, 0, 0, 2, 5, 4],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 5, 6, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 6, 6],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 9, 12, 12],\n",
              " [0, 0, 0, 0, 0, 4, 0, 1, 0, 3, 0, 34, 13, 16, 20],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 9, 13],\n",
              " [0, 1, 0, 0, 0, 1, 0, 3, 0, 0, 0, 1, 11, 9, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 8, 10, 10],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 5, 0, 1, 8, 11, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 6, 11, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 6, 8, 11],\n",
              " [0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 12, 8, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 9, 5, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 9, 15],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 0, 4, 11, 11, 11],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 11, 9, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 3, 6, 6],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 13, 17],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 12, 12],\n",
              " [0, 0, 0, 6, 0, 0, 0, 0, 0, 10, 0, 0, 8, 9, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 8, 13, 15],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 13, 13],\n",
              " [2, 0, 17, 0, 0, 0, 15, 0, 22, 0, 0, 0, 2, 2, 2],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 5, 7],\n",
              " [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 4, 4],\n",
              " [2, 1, 9, 3, 0, 0, 0, 0, 0, 6, 0, 0, 8, 8, 7],\n",
              " [3, 0, 8, 3, 0, 0, 32, 0, 60, 5, 1, 0, 6, 6, 5],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 10, 11],\n",
              " [0, 0, 0, 3, 0, 0, 0, 1, 0, 14, 0, 0, 3, 5, 4],\n",
              " [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 4, 8, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 1, 0, 7, 0, 0, 4, 4, 4],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 10, 6, 7],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 10, 12],\n",
              " [0, 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5],\n",
              " [0, 1, 0, 1, 0, 12, 0, 1, 0, 3, 0, 130, 10, 11, 13],\n",
              " [5, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 6, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 5, 5, 8],\n",
              " [0, 0, 0, 1, 0, 3, 0, 0, 0, 0, 0, 4, 8, 9, 9],\n",
              " [0, 0, 0, 3, 0, 3, 0, 0, 0, 4, 0, 4, 2, 4, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 8, 6],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 10, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 7, 9],\n",
              " [1, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 0, 10, 10, 13],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 5, 6],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 11, 9, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 12, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 17, 17, 23],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 10, 13],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 8, 7, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 9, 11],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 8, 9, 9],\n",
              " [7, 0, 5, 0, 1, 0, 0, 0, 0, 0, 0, 0, 11, 10, 13],\n",
              " [15, 0, 4, 0, 0, 0, 7, 0, 3, 0, 0, 0, 9, 7, 9],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 9],\n",
              " [0, 1, 0, 18, 0, 0, 0, 0, 0, 85, 0, 0, 4, 4, 4],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 10, 13, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 5, 5],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 13, 17],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 6, 6, 7],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 12, 11, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 8, 9],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 13, 12, 17],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 10, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 13, 11, 16],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 13, 12, 18],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 7, 8],\n",
              " [0, 0, 0, 4, 0, 0, 0, 0, 0, 28, 0, 0, 10, 7, 10],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 8, 8, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 16, 15],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 12, 13, 15],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 11, 17, 18],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 15, 14, 18],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 9, 11, 14],\n",
              " [0, 0, 0, 1, 0, 4, 0, 0, 0, 0, 0, 4, 14, 17, 20],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 15, 16],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 10, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 8, 6, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 9, 9, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 8, 9],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 10, 9, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 5, 6],\n",
              " [0, 1, 0, 0, 0, 3, 0, 1, 0, 0, 0, 1, 19, 17, 22],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 12, 18, 18],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 15, 13, 18],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 13, 16],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 5, 16, 20, 24],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 6, 8, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 14, 14, 20],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 8, 10, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 14, 18],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 14, 15],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 5, 5],\n",
              " [5, 0, 9, 1, 0, 0, 26, 0, 21, 0, 0, 0, 7, 4, 5],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 12, 18],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2],\n",
              " [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 13, 14, 16],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 12, 16],\n",
              " [0, 0, 0, 0, 0, 4, 0, 0, 0, 3, 0, 6, 11, 10, 12],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 10, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 6, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 0, 0, 10, 11, 14],\n",
              " [0, 2, 0, 0, 0, 6, 0, 0, 0, 0, 0, 6, 3, 4, 3],\n",
              " [4, 0, 2, 0, 4, 4, 5, 0, 1, 1, 1, 0, 7, 6, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 6, 7, 9],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 8, 10, 9],\n",
              " [0, 1, 13, 2, 0, 1, 0, 0, 21, 0, 0, 0, 8, 9, 9],\n",
              " [0, 0, 17, 2, 0, 0, 0, 0, 22, 0, 0, 0, 10, 9, 11],\n",
              " [2, 0, 17, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 7, 6],\n",
              " [0, 0, 3, 1, 0, 0, 0, 0, 23, 3, 0, 0, 11, 13, 15],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 5, 5, 5],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 6, 8],\n",
              " [0, 1, 1, 0, 13, 2, 4, 0, 0, 0, 17, 0, 11, 10, 13],\n",
              " [0, 0, 0, 0, 29, 22, 2, 1, 0, 2, 0, 50, 9, 9, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 10, 13, 15, 18],\n",
              " [0, 0, 0, 3, 0, 4, 0, 0, 0, 4, 0, 3, 9, 12, 16],\n",
              " [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 8, 9, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 11, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 6, 7, 11],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 6, 9, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 11, 14],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 10, 10, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 6, 6, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 5],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 12, 15, 15],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 10, 15, 15],\n",
              " [0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 16, 13, 14],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 4],\n",
              " [3, 0, 1, 0, 13, 0, 3, 0, 0, 0, 13, 0, 5, 4, 5],\n",
              " [2, 0, 21, 0, 0, 0, 1, 0, 15, 0, 1, 0, 2, 4, 3],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 8, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 4, 4, 5],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 12, 14, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 6, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 8, 11],\n",
              " [0, 0, 4, 8, 0, 0, 10, 2, 26, 40, 0, 0, 6, 5, 5],\n",
              " [0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 3, 5, 6],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 9, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 9, 11, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 12, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 12, 17],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 7, 8, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 7],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 8, 7, 8],\n",
              " [0, 1, 0, 0, 0, 0, 0, 6, 0, 1, 0, 0, 9, 8, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 8, 6, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 14, 15],\n",
              " [22, 1, 1, 0, 0, 1, 57, 0, 10, 0, 1, 0, 10, 8, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 5, 6, 5],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 7, 8, 7],\n",
              " [0, 3, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 10, 7, 12],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 12, 14, 18],\n",
              " [0, 0, 10, 4, 1, 0, 0, 0, 15, 5, 1, 0, 5, 4, 4],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 9, 9, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 10, 11],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 8, 10, 9],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 5, 0, 0, 6, 8, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 9, 10, 11],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 10, 15],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 7, 8],\n",
              " [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 7, 6, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 9, 8],\n",
              " [0, 3, 0, 0, 0, 0, 0, 9, 0, 1, 0, 0, 9, 8, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 10, 11],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 10, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 16, 18],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 8, 9, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 16, 17],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 1, 0, 2, 11, 13, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 11, 0, 0, 8, 10, 12],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 15, 0, 0, 13, 12, 17],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 15, 20, 22],\n",
              " [0, 3, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 12, 7, 6],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 6, 6],\n",
              " [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 5, 7, 9, 13],\n",
              " [0, 0, 0, 3, 0, 0, 0, 1, 0, 7, 0, 0, 9, 10, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 6, 6, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 9, 11],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 11, 17, 18],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 6, 0, 0, 4, 5, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 7, 10, 10],\n",
              " [0, 1, 0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 11, 10, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 7, 7, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 9, 7, 6],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 11, 9, 10],\n",
              " [0, 1, 0, 0, 0, 3, 0, 2, 0, 2, 0, 3, 6, 4, 5],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 7, 8],\n",
              " [0, 0, 0, 0, 0, 6, 0, 1, 0, 1, 0, 40, 9, 9, 9],\n",
              " [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 17, 14, 17],\n",
              " [0, 0, 8, 0, 2, 1, 7, 0, 6, 0, 0, 0, 9, 8, 9],\n",
              " [1, 0, 3, 0, 2, 0, 8, 0, 8, 0, 0, 0, 9, 7, 7],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 0, 0, 4, 6, 5],\n",
              " [0, 0, 0, 2, 0, 0, 0, 1, 0, 2, 0, 0, 11, 10, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 11, 11, 10],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 8, 9, 14],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 9, 7, 7],\n",
              " [0, 1, 0, 0, 8, 6, 0, 1, 1, 0, 5, 10, 7, 7, 6],\n",
              " [0, 0, 0, 0, 12, 6, 0, 2, 6, 0, 33, 25, 6, 9, 10],\n",
              " [0, 0, 0, 2, 0, 6, 0, 1, 0, 1, 0, 26, 5, 6, 6],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 13],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 10, 14, 16],\n",
              " [0, 0, 0, 4, 0, 0, 0, 1, 0, 33, 0, 0, 11, 14, 15],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 14, 16],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 12, 12, 15],\n",
              " [0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 2, 13, 16, 18],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 12, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 10, 11],\n",
              " [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 15],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 16, 18, 18],\n",
              " [0, 0, 0, 8, 0, 0, 0, 0, 0, 24, 0, 0, 6, 4, 5],\n",
              " [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 7],\n",
              " [22, 1, 4, 1, 0, 1, 10, 2, 0, 1, 0, 2, 5, 4, 4],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 15, 16, 18],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 6, 11, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 15, 18],\n",
              " [0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 9, 7, 11],\n",
              " [0, 0, 2, 1, 4, 0, 0, 0, 0, 0, 0, 0, 9, 11, 16],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 3, 3],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 6, 7, 6],\n",
              " [0, 0, 0, 5, 0, 0, 0, 0, 0, 4, 0, 0, 8, 8, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 5],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 7, 9, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 3, 6, 7],\n",
              " [0, 0, 0, 1, 0, 5, 0, 0, 0, 1, 0, 1, 6, 6, 5],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 11, 14],\n",
              " [3, 1, 1, 0, 0, 1, 5, 2, 10, 0, 0, 5, 9, 7, 7],\n",
              " [0, 0, 9, 8, 0, 0, 4, 0, 60, 26, 1, 0, 4, 7, 8],\n",
              " [0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 11, 11, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 9, 7, 8],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 8, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 7, 7, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 9, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 6, 10, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 9, 11],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 11],\n",
              " [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 5, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 11, 13],\n",
              " [0, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 3, 15, 8, 9],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 13, 0, 0, 4, 5, 6],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 12, 10, 13],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 8, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 13, 16, 15],\n",
              " [0, 2, 0, 7, 0, 5, 0, 0, 0, 20, 0, 7, 4, 5, 4],\n",
              " [0, 0, 0, 5, 0, 0, 0, 2, 0, 13, 0, 0, 6, 9, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 12, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 9, 6, 6],\n",
              " [0, 1, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 11, 8, 10],\n",
              " [0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 10, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 12, 15, 17],\n",
              " [0, 0, 0, 0, 0, 5, 0, 1, 0, 0, 0, 16, 7, 9, 8],\n",
              " [0, 0, 0, 0, 0, 4, 0, 0, 0, 2, 0, 12, 9, 9, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 10, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 10, 11, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 9, 11],\n",
              " [12, 2, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 7, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 12, 16, 15],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 5, 6, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 8, 10, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 11, 15, 16],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 6, 7, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 9, 8, 8],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 4],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 5, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 11, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 8, 10, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 13, 17],\n",
              " [0, 5, 0, 0, 0, 1, 0, 3, 0, 0, 0, 1, 8, 10, 14],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 5, 9],\n",
              " [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 9, 5, 6],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 2, 12, 14, 17],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 16, 17],\n",
              " [5, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 9, 10],\n",
              " [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 12, 9, 11],\n",
              " [0, 3, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 4, 3, 3],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 9, 8, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 12, 15],\n",
              " [0, 2, 0, 0, 0, 0, 0, 1, 0, 2, 0, 3, 10, 8, 8],\n",
              " [0, 0, 0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 15, 16, 17],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 4, 6],\n",
              " [0, 0, 0, 2, 0, 0, 0, 1, 0, 5, 0, 0, 7, 7, 6],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 13, 0, 0, 11, 12, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 6, 8, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 8, 9],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 7, 7, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 10, 11],\n",
              " [1, 0, 33, 2, 1, 0, 24, 0, 159, 8, 10, 1, 6, 6, 8],\n",
              " [10, 0, 8, 0, 0, 0, 20, 0, 1, 0, 0, 0, 4, 3, 4],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 16, 18, 21],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 6, 0, 0, 3, 5, 5],\n",
              " [0, 3, 0, 1, 0, 8, 0, 0, 0, 0, 0, 2, 10, 12, 13],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 8, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 8, 6, 7],\n",
              " [0, 2, 0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 6, 6, 7],\n",
              " [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 9, 9, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 9, 10],\n",
              " [0, 0, 0, 9, 0, 0, 0, 0, 0, 36, 0, 0, 7, 4, 5],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 5, 6, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 10, 10, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 7, 5, 5],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 7, 7, 7],\n",
              " [1, 0, 2, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 9, 11],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 11, 13],\n",
              " [0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 6, 5, 6],\n",
              " [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 6, 6, 6],\n",
              " [0, 2, 0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 14, 13, 15],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 13, 16],\n",
              " [0, 0, 0, 4, 0, 0, 0, 0, 0, 1, 0, 2, 13, 13, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 10, 11, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 13, 16, 17],\n",
              " [0, 0, 0, 12, 0, 0, 0, 1, 0, 40, 0, 0, 5, 6, 6],\n",
              " [0, 0, 0, 2, 0, 3, 0, 1, 0, 4, 0, 30, 8, 11, 12],\n",
              " [0, 2, 0, 1, 0, 2, 0, 3, 0, 0, 0, 3, 15, 12, 13],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 10, 9, 14],\n",
              " [1, 0, 5, 1, 0, 0, 5, 0, 4, 2, 0, 0, 5, 4, 6],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 14, 13, 16],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 6, 7],\n",
              " [0, 1, 0, 1, 0, 1, 0, 0, 0, 2, 0, 0, 12, 16, 17],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 13, 15, 17],\n",
              " [6, 0, 22, 3, 0, 0, 41, 0, 87, 6, 2, 0, 6, 9, 9],\n",
              " [0, 0, 3, 0, 3, 0, 0, 0, 3, 0, 0, 0, 11, 10, 18],\n",
              " [0, 0, 0, 0, 0, 7, 0, 0, 0, 0, 0, 12, 6, 8, 9],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 4],\n",
              " [0, 0, 1, 0, 10, 1, 0, 0, 0, 0, 10, 0, 9, 10, 11],\n",
              " [0, 0, 6, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 8, 11],\n",
              " [0, 1, 0, 3, 0, 0, 0, 0, 0, 16, 0, 0, 5, 7, 7],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 8, 9, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 6],\n",
              " [0, 0, 0, 1, 0, 2, 0, 0, 0, 0, 0, 14, 8, 10, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 14, 16],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 3, 0, 0, 9, 12, 12],\n",
              " [0, 6, 0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 13, 14, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 9, 10, 15],\n",
              " [6, 0, 16, 2, 1, 0, 45, 0, 66, 7, 3, 0, 3, 4, 3],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 3, 4],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 8, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 13, 14],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 8, 9, 9],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 7, 8],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 11, 12],\n",
              " [0, 4, 0, 0, 9, 7, 0, 7, 0, 2, 0, 11, 6, 9, 8],\n",
              " [0, 2, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 11, 10, 12],\n",
              " [0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 0, 0, 12, 9, 11],\n",
              " [0, 0, 0, 0, 0, 4, 0, 0, 0, 1, 0, 43, 9, 10, 12],\n",
              " [0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 7, 9],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 16, 16],\n",
              " [0, 0, 0, 0, 0, 4, 0, 0, 0, 0, 0, 2, 14, 16, 18],\n",
              " [0, 1, 0, 16, 0, 0, 0, 1, 0, 14, 0, 6, 11, 13, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 11, 16, 20],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 8, 7, 8],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 16, 18, 22],\n",
              " [0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 13, 17],\n",
              " [0, 1, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 10, 11, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 16, 18],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 8, 11],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 7, 7, 6],\n",
              " [0, 0, 0, 1, 0, 0, 0, 1, 0, 2, 0, 0, 8, 13, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 13, 11, 16],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 15],\n",
              " [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 11, 10, 12],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 1, 0, 2, 9, 9, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 6, 0, 0, 7, 7, 8],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 3, 4],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 8, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 1, 0, 6, 0, 0, 3, 4, 5],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 10, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 15, 16],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 6, 8, 8],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 9, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 6, 6],\n",
              " [0, 2, 9, 16, 0, 5, 0, 5, 17, 64, 0, 29, 10, 13, 17],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 10, 11],\n",
              " [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 8, 9, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 10],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 9, 6, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 9, 11],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 4, 4, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 7, 6, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 10, 12, 15],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 4, 5, 6],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 8, 9, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 7, 10, 9],\n",
              " [2, 0, 0, 0, 4, 2, 6, 0, 2, 0, 20, 1, 5, 6, 6],\n",
              " [0, 0, 1, 0, 8, 3, 2, 2, 3, 0, 8, 22, 8, 6, 7],\n",
              " [0, 0, 0, 0, 3, 0, 0, 0, 3, 0, 26, 0, 10, 11, 11],\n",
              " [0, 0, 14, 1, 0, 0, 1, 0, 83, 1, 1, 0, 6, 7, 6],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 12, 12, 15],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 12, 12, 12],\n",
              " [2, 0, 20, 18, 0, 0, 4, 2, 37, 81, 1, 1, 8, 9, 8],\n",
              " [0, 3, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 9, 9, 10],\n",
              " [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 4, 4],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 8, 10, 10],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 10, 11, 10],\n",
              " [0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 7, 7, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 7, 6, 7],\n",
              " [2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 10, 6, 8],\n",
              " [0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 9, 6, 7],\n",
              " [3, 0, 21, 5, 0, 0, 74, 0, 146, 4, 7, 0, 1, 6, 6],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 8, 6, 9],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 15, 15],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 14, 15, 17],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 9, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 10, 11, 10],\n",
              " [0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 0, 23, 8, 11, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 14, 18],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 8, 11, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 16, 21],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 11, 11],\n",
              " [1, 0, 8, 0, 0, 0, 3, 0, 29, 0, 1, 0, 8, 5, 6],\n",
              " [0, 0, 8, 0, 0, 0, 4, 0, 0, 0, 0, 0, 12, 10, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 14, 15],\n",
              " [0, 0, 0, 2, 0, 1, 0, 0, 0, 2, 0, 0, 4, 7, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 10, 13, 15],\n",
              " [0, 0, 2, 0, 10, 0, 0, 0, 3, 0, 15, 0, 2, 3, 4],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 5, 8],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 13, 11, 11],\n",
              " [0, 1, 0, 0, 0, 3, 0, 2, 0, 0, 0, 0, 8, 6, 7],\n",
              " [1, 0, 23, 0, 1, 0, 7, 0, 89, 0, 23, 0, 9, 8, 10],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 8, 6, 6],\n",
              " [0, 1, 0, 6, 0, 1, 0, 2, 0, 10, 0, 3, 5, 3, 3],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 2, 4, 3],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 14, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 11, 16, 17],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 9, 12, 13],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 7, 9, 8],\n",
              " [0, 0, 7, 0, 0, 0, 0, 0, 6, 0, 0, 0, 8, 6, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 10, 13],\n",
              " [0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 6, 5, 5],\n",
              " [0, 0, 0, 4, 0, 0, 0, 0, 0, 5, 0, 1, 5, 7, 7],\n",
              " [0, 0, 0, 5, 0, 0, 0, 1, 0, 11, 0, 0, 4, 3, 3],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 11, 13],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 7, 5, 5],\n",
              " [3, 0, 12, 4, 0, 0, 25, 0, 35, 15, 2, 0, 4, 5, 5],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 13, 14, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 10, 10],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 11, 10, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 11, 10],\n",
              " [0, 0, 11, 3, 0, 0, 0, 0, 0, 9, 0, 0, 7, 11, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 8, 7, 8],\n",
              " [0, 0, 1, 0, 5, 0, 0, 0, 0, 0, 0, 0, 10, 12, 18],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 4, 0, 0, 5, 5, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 12, 13],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 0, 0, 4, 5, 5],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 3, 8, 6, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 11, 15],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 6, 5, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 8, 12],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 5, 4, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 9, 13],\n",
              " [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 11, 9, 10],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 14, 15, 17],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 11, 8, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 5, 10, 9, 12],\n",
              " [23, 0, 0, 0, 0, 0, 125, 0, 11, 0, 6, 0, 15, 14, 15],\n",
              " [0, 0, 0, 7, 0, 0, 0, 0, 0, 4, 0, 0, 4, 6, 5],\n",
              " [0, 0, 0, 4, 0, 0, 0, 1, 0, 24, 0, 0, 3, 5, 4],\n",
              " [0, 0, 0, 4, 0, 0, 0, 0, 0, 7, 0, 0, 9, 11, 13],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 11, 11, 13],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 12, 12, 15],\n",
              " [2, 2, 1, 1, 9, 1, 7, 0, 3, 1, 6, 0, 12, 10, 9],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 14, 18],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 15, 14, 17],\n",
              " [0, 0, 0, 1, 6, 0, 0, 0, 0, 0, 0, 1, 1, 2, 2],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 9, 10, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 11, 13],\n",
              " [0, 0, 0, 0, 0, 11, 0, 0, 0, 0, 0, 4, 7, 12, 11],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 13, 15, 21],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 5, 0, 0, 12, 13, 13],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 7, 8, 11],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 12, 13, 18],\n",
              " [0, 0, 19, 3, 2, 0, 0, 0, 47, 2, 23, 0, 3, 6, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 7, 8, 9],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 10, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 14, 18],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 6, 9, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 13, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 8, 11, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 15, 17],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 12, 8, 7],\n",
              " [5, 0, 2, 0, 8, 1, 3, 0, 1, 0, 5, 3, 10, 7, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 11, 11, 13],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 10, 12, 15],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 10, 14],\n",
              " [0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 8, 7, 9],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 16, 16, 18],\n",
              " [0, 0, 4, 0, 2, 1, 0, 0, 0, 0, 0, 0, 11, 15, 16],\n",
              " [0, 0, 0, 7, 0, 0, 0, 1, 0, 4, 0, 2, 11, 10, 12],\n",
              " [0, 4, 0, 0, 0, 2, 0, 2, 0, 0, 0, 2, 11, 10, 14],\n",
              " [0, 0, 0, 1, 0, 3, 0, 0, 0, 9, 0, 1, 9, 13, 16],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 9, 0, 1, 13, 13, 15],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 10, 0, 1, 9, 11, 12],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 8, 10, 11],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 7, 9, 11],\n",
              " [0, 0, 0, 0, 0, 9, 0, 0, 0, 0, 0, 3, 16, 16, 20],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 14, 15, 17],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 4, 8],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 5, 5, 8, 9],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 8, 8, 8],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 12, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 13, 14],\n",
              " [0, 0, 0, 2, 0, 1, 0, 0, 0, 2, 0, 0, 8, 5, 4],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 15, 18],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 12, 15, 18],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 5, 6, 6],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 6, 6, 7],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 10, 12, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 7, 6, 7],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 12, 13, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 9, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 8, 10, 9],\n",
              " [0, 0, 0, 0, 0, 5, 0, 0, 0, 0, 0, 11, 13, 15, 17],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 8, 8, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 7, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 11, 16, 18],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 11, 11, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 12, 11, 12],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 11, 12, 12, 14],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 9, 14, 14],\n",
              " [0, 1, 1, 0, 1, 0, 4, 0, 0, 0, 1, 0, 12, 14, 14],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 16, 19],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 13, 13, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 12, 15],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 3, 11, 10, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 13, 11, 12],\n",
              " [5, 1, 5, 1, 4, 1, 1, 3, 1, 0, 1, 3, 5, 3, 3],\n",
              " [0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 9, 11, 11],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 2, 0, 0, 4, 6, 6],\n",
              " [0, 0, 0, 9, 0, 0, 0, 0, 0, 28, 0, 0, 6, 7, 7],\n",
              " [0, 0, 3, 2, 0, 0, 0, 0, 13, 11, 1, 1, 5, 7, 8],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 10, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 12, 13, 12],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 5, 8, 7],\n",
              " [0, 0, 0, 0, 0, 6, 0, 0, 0, 1, 0, 15, 8, 7, 8],\n",
              " [0, 2, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 5, 7, 9],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 7, 0, 0, 7, 6, 5],\n",
              " [1, 0, 14, 4, 0, 0, 17, 0, 46, 0, 1, 0, 11, 9, 8],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 4, 7, 10],\n",
              " [0, 8, 0, 1, 0, 1, 0, 13, 0, 0, 0, 4, 7, 9, 10],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 11, 9, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 6, 7, 9],\n",
              " [3, 0, 4, 0, 1, 1, 4, 0, 6, 0, 0, 0, 13, 11, 15],\n",
              " [0, 0, 4, 0, 1, 0, 0, 0, 0, 0, 0, 0, 7, 9, 9],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 10, 11, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 10, 11],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 6, 8, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 10, 11, 14],\n",
              " [0, 0, 0, 6, 0, 0, 0, 0, 0, 1, 0, 0, 10, 13, 16],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 12, 12, 16],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 8, 10, 14],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 13, 12, 13],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 13, 15, 16],\n",
              " [0, 0, 0, 1, 0, 0, 0, 1, 0, 9, 0, 0, 6, 6, 6],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 4, 6, 7, 9],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 3, 4, 6],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 7, 6, 8],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 5, 0, 0, 13, 15, 18],\n",
              " [0, 0, 1, 1, 18, 17, 0, 2, 2, 0, 47, 73, 8, 8, 9],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 9, 9, 9],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 4, 5, 5],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 10, 10],\n",
              " [0, 3, 0, 38, 0, 0, 0, 4, 0, 163, 0, 1, 5, 5, 4],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 13, 13, 13],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 4, 4],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 11, 10, 11],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 9, 7, 7],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 2, 6, 5],\n",
              " [0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 10, 10, 11],\n",
              " [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 7, 6, 7],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 9, 13, 12],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 9, 11],\n",
              " [0, 0, 0, 1, 0, 7, 0, 0, 0, 1, 0, 8, 10, 10, 11],\n",
              " [0, 0, 0, 3, 0, 0, 0, 0, 0, 4, 0, 0, 8, 10, 10],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 9, 11, 14],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 13, 13, 16],\n",
              " [0, 0, 0, 0, 0, 9, 0, 0, 0, 1, 0, 5, 12, 17, 17],\n",
              " [5, 4, 2, 0, 0, 1, 0, 5, 0, 0, 0, 0, 11, 9, 10],\n",
              " [0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 2, 9, 16, 21],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 17, 20, 20],\n",
              " [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 11, 13, 15],\n",
              " [1, 0, 13, 0, 0, 0, 3, 0, 10, 0, 0, 0, 16, 16, 18],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 6, 6, 10],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 8, 9, 14],\n",
              " [0, 0, 2, 0, 4, 5, 0, 0, 0, 0, 0, 9, 11, 11, 15],\n",
              " [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 10, 12, 15],\n",
              " [0, 1, 0, 0, 0, 0, 0, 2, 0, 0, 0, 1, 12, 13, 16],\n",
              " [0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 12, 13, 14],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 14, 13, 17],\n",
              " [0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 14, 18, 22],\n",
              " ...]"
            ]
          },
          "execution_count": 157,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "F_vec"
      ],
      "id": "S36m3EA9rv00"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7yhdiKtMkboW"
      },
      "source": [
        "# 2nd Iteration training"
      ],
      "id": "7yhdiKtMkboW"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z87Aw3qBXv4N",
        "outputId": "387d49ca-5205-4360-84af-df058a031e71"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[ 0,  0,  0, ...,  8,  8,  9],\n",
              "       [ 0,  0,  0, ...,  8, 11, 13],\n",
              "       [ 0,  0,  0, ...,  4,  6,  6],\n",
              "       ...,\n",
              "       [ 0,  0,  0, ...,  7,  7,  8],\n",
              "       [ 0,  0,  0, ..., 13, 15, 20],\n",
              "       [ 0,  0,  0, ...,  4,  6,  9]])"
            ]
          },
          "execution_count": 160,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x"
      ],
      "id": "z87Aw3qBXv4N"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "624cdcd2",
        "outputId": "89fb2790-bdc2-4142-8774-76f322cd847b"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-f9a36942-00b9-4a99-95f3-2112655f6b64\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>a_0</th>\n",
              "      <th>b_0</th>\n",
              "      <th>a_1</th>\n",
              "      <th>b_1</th>\n",
              "      <th>a_2</th>\n",
              "      <th>b_2</th>\n",
              "      <th>a_3</th>\n",
              "      <th>b_3</th>\n",
              "      <th>a_4</th>\n",
              "      <th>b_4</th>\n",
              "      <th>...</th>\n",
              "      <th>S_5</th>\n",
              "      <th>S_6</th>\n",
              "      <th>S_7</th>\n",
              "      <th>S_8</th>\n",
              "      <th>S_9</th>\n",
              "      <th>S_10</th>\n",
              "      <th>S_11</th>\n",
              "      <th>S_12</th>\n",
              "      <th>S_13</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>24</td>\n",
              "      <td>24</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>8</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>18</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>13</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>20</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>10</td>\n",
              "      <td>20</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>10</td>\n",
              "      <td>19</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>18</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>16</td>\n",
              "      <td>18</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>19</td>\n",
              "      <td>20</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>21</td>\n",
              "      <td>24</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>8</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>7</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>19</td>\n",
              "      <td>23</td>\n",
              "      <td>7</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>6</td>\n",
              "      <td>6</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>16</td>\n",
              "      <td>21</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>17</td>\n",
              "      <td>19</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>21</td>\n",
              "      <td>22</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>5</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>12</td>\n",
              "      <td>17</td>\n",
              "      <td>2</td>\n",
              "      <td>4</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>23</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>5</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>23</td>\n",
              "      <td>19</td>\n",
              "      <td>5</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>5</td>\n",
              "      <td>6</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>11</td>\n",
              "      <td>16</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>20</td>\n",
              "      <td>21</td>\n",
              "      <td>4</td>\n",
              "      <td>7</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>5</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>16</td>\n",
              "      <td>17</td>\n",
              "      <td>4</td>\n",
              "      <td>6</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>6</td>\n",
              "      <td>10</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>13</td>\n",
              "      <td>17</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>3</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>30 rows × 43 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f9a36942-00b9-4a99-95f3-2112655f6b64')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-f9a36942-00b9-4a99-95f3-2112655f6b64 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-f9a36942-00b9-4a99-95f3-2112655f6b64');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "    a_0  b_0  a_1  b_1  a_2  b_2  a_3  b_3  a_4  b_4  ...  S_5  S_6  S_7  S_8  \\\n",
              "0     0    0    0    0    0    0    0    0    0    0  ...   24   24    5    3   \n",
              "1     0    0    0    0    3    1    0    0    0    0  ...    7    8    1    3   \n",
              "2     1    0    0    0    0    0    0    0    0    0  ...   20   18    5    2   \n",
              "3     0    0    2    1    0    0    0    0    0    0  ...    7    6    1    2   \n",
              "4     0    0    0    0    1    2    0    0    0    0  ...   13   16    3    5   \n",
              "5     2    3    0    0    0    0    0    0    0    0  ...   20   20    3    3   \n",
              "6     0    2    0    0    0    0    0    0    0    0  ...    4    3    1    1   \n",
              "7     0    0    0    0    0    0    0    0    0    0  ...   10   20    0    1   \n",
              "8     0    0    0    0    0    0    0    0    0    0  ...   10   19    2    3   \n",
              "9     0    0    0    0    0    0    0    0    0    0  ...   18   17    2    2   \n",
              "10    0    0    0    0    0    0    0    0    0    0  ...   16   18    2    3   \n",
              "11    6    0    0    0    1    0    1    0    0    0  ...   19   20    4    3   \n",
              "12    0    0    0    0    0    0    0    0    0    0  ...   21   24    4    3   \n",
              "13    0    0    0    0    0    0    0    0    0    0  ...   17   19    5    5   \n",
              "14    0    0    0    0    0    0    0    0    0    0  ...    7    7    1    2   \n",
              "15    7    1    0    0    0    0    1    0    1    0  ...    5    4    2    1   \n",
              "16    0    1    0    0    0    0    0    0    0    0  ...   19   23    7    3   \n",
              "17    0    0    1    0    0    0    0    0    0    0  ...   16   21    3    5   \n",
              "18    0    1    0    0    0    1    0    0    0    0  ...   17   19    4    2   \n",
              "19    0    0    0    0    0    1    0    0    0    0  ...   21   22    5    4   \n",
              "20    0    0    5    0    0    0    0    0    0    0  ...   12   17    2    4   \n",
              "21    0    0    0    0    0    0    0    0    0    0  ...   20   23    5    4   \n",
              "22    0    0    0    0    0    0    0    0    0    0  ...    2    3    0    0   \n",
              "23    0    0    0    0    1    3    0    0    0    0  ...   23   19    5    4   \n",
              "24    0    0    0    0    2    0    0    0    0    0  ...    3    3    0    2   \n",
              "25    0    0    0    0    0    1    0    0    0    1  ...   11   16    3    4   \n",
              "26    0    0    2    3    0    0    0    0    0    0  ...   20   21    4    7   \n",
              "27    0    0    1    2    0    0    0    0    0    1  ...   16   17    4    6   \n",
              "28    0    0    0    0    2    1    0    0    0    0  ...    6   10    1    1   \n",
              "29    0    0    0    1    2    0    0    0    0    0  ...   13   17    3    3   \n",
              "\n",
              "    S_9  S_10  S_11  S_12  S_13  Class  \n",
              "0     2     3     3     6     5      5  \n",
              "1     3     3     2     0     1      2  \n",
              "2     1     1     1     3     1      0  \n",
              "3     1     2     1     2     1      1  \n",
              "4     4     3     4     2     2      2  \n",
              "5     1     1     2     3     2      0  \n",
              "6     1     1     1     0     1      0  \n",
              "7     0     1     1     1     2      6  \n",
              "8     2     2     3     3     4      6  \n",
              "9     2     1     2     3     2      5  \n",
              "10    1     2     2     3     3      6  \n",
              "11    3     3     3     4     6      6  \n",
              "12    4     6     5     4     7      6  \n",
              "13    5     6     6     5     8      6  \n",
              "14    1     1     2     1     1      6  \n",
              "15    1     1     1     1     2      0  \n",
              "16    4     2     6     6     5      6  \n",
              "17    3     5     5     2     3      1  \n",
              "18    0     0     1     1     0      0  \n",
              "19    4     3     4     4     5      6  \n",
              "20    2     3     2     2     2      1  \n",
              "21    4     5     5     3     5      6  \n",
              "22    0     0     0     0     0      6  \n",
              "23    6     2     5     6     2      2  \n",
              "24    2     1     1     0     0      2  \n",
              "25    4     3     5     2     2      2  \n",
              "26    4     3     5     3     3      1  \n",
              "27    4     3     4     3     3      1  \n",
              "28    1     0     3     1     0      2  \n",
              "29    3     2     2     2     2      2  \n",
              "\n",
              "[30 rows x 43 columns]"
            ]
          },
          "execution_count": 137,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "x = np.array(F_vec)\n",
        "k = int((max(catagories)+1)*4)\n",
        "data=pd.DataFrame({\"a_0\":x[:,0]})\n",
        "data.insert(loc=1, column=\"b_0\", value=x[:,1])\n",
        "for i in range(1,k-1,2):\n",
        "    # print(i)\n",
        "    data.insert(loc=i+1, column=\"a_{}\".format(int((i+1)/2)), value=x[:,i+1])\n",
        "    data.insert(loc=i+2, column=\"b_{}\".format(int((i+1)/2)), value=x[:,i+2])\n",
        "for i in range(len(F_vec[0]) - k):\n",
        "  data.insert(loc=k+i, column=f'S_{i}', value=x[:,k+i])\n",
        "data.insert(loc=len(F_vec[0]),column='Class',value=data_s['class'].to_numpy())\n",
        "data.head(30)"
      ],
      "id": "624cdcd2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "da9f852b"
      },
      "outputs": [],
      "source": [
        "# feature=[\"a_0\",\"b_0\"]\n",
        "# for i in range(1,k-1,2):\n",
        "#     feature.append(\"a_{}\".format(int((i+1)/2)))\n",
        "#     feature.append(\"b_{}\".format(int((i+1)/2)))\n",
        "\n",
        "X=data.drop('Class', axis=1) # Features\n",
        "y=data['Class']  # Labels\n",
        "X_train=X.iloc[q]\n",
        "X_test=X.iloc[p]\n",
        "y_train=y.iloc[q]\n",
        "y_test=y.iloc[p]"
      ],
      "id": "da9f852b"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d9667a28"
      },
      "outputs": [],
      "source": [
        "for n_est in range(100, 151, 5):\n",
        "  bst = XGBClassifier(n_estimators=n_est, max_depth=3, learning_rate=0.07)\n",
        "  bst.fit(X_train,y_train)\n",
        "  y_pred=bst.predict(X_test)\n",
        "  curr_score = metrics.accuracy_score(y_test, y_pred)\n",
        "  #Import scikit-learn metrics module for accuracy calculation\n",
        "  \n",
        "  # Model Accuracy, how often is the classifier correct?\n",
        "  print(\"Accuracy for \", n_est, \"estimators: \", curr_score*100,\"%\")"
      ],
      "id": "d9667a28"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IVIroZVLE4D"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau\n",
        "import tensorflow_hub as hub\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "kfold = KFold(n_splits=5)"
      ],
      "id": "-IVIroZVLE4D"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "blPiT5VSTY8T"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('/content/Feature_nbd_mag.csv', index_col=0)"
      ],
      "id": "blPiT5VSTY8T"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "XICOneMaTqxb",
        "outputId": "540daabd-5842-4c49-c68f-00aa97b18294"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-1defada9-0a7a-4ace-9a4b-7b0e0e6e502d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>689</th>\n",
              "      <th>690</th>\n",
              "      <th>691</th>\n",
              "      <th>692</th>\n",
              "      <th>693</th>\n",
              "      <th>694</th>\n",
              "      <th>695</th>\n",
              "      <th>696</th>\n",
              "      <th>697</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 699 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-1defada9-0a7a-4ace-9a4b-7b0e0e6e502d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-1defada9-0a7a-4ace-9a4b-7b0e0e6e502d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-1defada9-0a7a-4ace-9a4b-7b0e0e6e502d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   0  1  2  3  4  5  6  7  8  9  ...  689  690  691  692  693  694  695  696  \\\n",
              "0  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
              "1  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
              "2  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
              "3  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
              "4  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
              "\n",
              "   697  Class  \n",
              "0    0    246  \n",
              "1    0    131  \n",
              "2    0    189  \n",
              "3    0    131  \n",
              "4    0     95  \n",
              "\n",
              "[5 rows x 699 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ],
      "id": "XICOneMaTqxb"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DI4L9w7dU0Hw"
      },
      "outputs": [],
      "source": [
        "for i, c in enumerate(data['Class'].isnull().values):\n",
        "  if c:\n",
        "    print(i)"
      ],
      "id": "DI4L9w7dU0Hw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IobrQnkTJ7nl"
      },
      "outputs": [],
      "source": [
        "Node_Class = graph[1]['paper'][:,0]"
      ],
      "id": "IobrQnkTJ7nl"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KtwFSk8eKFlJ"
      },
      "outputs": [],
      "source": [
        "data['Class'] = Node_Class"
      ],
      "id": "KtwFSk8eKFlJ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzk5l-85BAqt"
      },
      "outputs": [],
      "source": [
        "Node_Fec = list(graph[0]['node_feat_dict']['paper'])\n",
        "feature_names = [\"w_{}\".format(ii) for ii in range(128)]\n",
        "Node_Fec = pd.DataFrame(Node_Fec,columns=feature_names)"
      ],
      "id": "tzk5l-85BAqt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "LiKcKi-DBOTZ",
        "outputId": "4d45cf69-1a0f-4061-864a-feda1918abee"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-7986ed55-436b-4557-bd65-b292d4aa12c8\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>w_0</th>\n",
              "      <th>w_1</th>\n",
              "      <th>w_2</th>\n",
              "      <th>w_3</th>\n",
              "      <th>w_4</th>\n",
              "      <th>w_5</th>\n",
              "      <th>w_6</th>\n",
              "      <th>w_7</th>\n",
              "      <th>w_8</th>\n",
              "      <th>w_9</th>\n",
              "      <th>...</th>\n",
              "      <th>w_118</th>\n",
              "      <th>w_119</th>\n",
              "      <th>w_120</th>\n",
              "      <th>w_121</th>\n",
              "      <th>w_122</th>\n",
              "      <th>w_123</th>\n",
              "      <th>w_124</th>\n",
              "      <th>w_125</th>\n",
              "      <th>w_126</th>\n",
              "      <th>w_127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-0.095379</td>\n",
              "      <td>0.040758</td>\n",
              "      <td>-0.210948</td>\n",
              "      <td>-0.064362</td>\n",
              "      <td>-0.225940</td>\n",
              "      <td>-0.002323</td>\n",
              "      <td>-0.555826</td>\n",
              "      <td>-0.352909</td>\n",
              "      <td>0.040634</td>\n",
              "      <td>-0.195226</td>\n",
              "      <td>...</td>\n",
              "      <td>0.146386</td>\n",
              "      <td>-0.378653</td>\n",
              "      <td>-0.328871</td>\n",
              "      <td>-0.136878</td>\n",
              "      <td>-0.007779</td>\n",
              "      <td>-0.157634</td>\n",
              "      <td>-0.069693</td>\n",
              "      <td>0.061569</td>\n",
              "      <td>-0.027663</td>\n",
              "      <td>-0.133832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-0.151047</td>\n",
              "      <td>-0.107315</td>\n",
              "      <td>-0.221964</td>\n",
              "      <td>-0.034725</td>\n",
              "      <td>0.010814</td>\n",
              "      <td>-0.009673</td>\n",
              "      <td>-0.434323</td>\n",
              "      <td>-0.461733</td>\n",
              "      <td>0.112732</td>\n",
              "      <td>0.091751</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047749</td>\n",
              "      <td>-0.065133</td>\n",
              "      <td>-0.208891</td>\n",
              "      <td>-0.366014</td>\n",
              "      <td>-0.045869</td>\n",
              "      <td>-0.297474</td>\n",
              "      <td>0.056755</td>\n",
              "      <td>0.345754</td>\n",
              "      <td>-0.027737</td>\n",
              "      <td>-0.218527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.114799</td>\n",
              "      <td>-0.175982</td>\n",
              "      <td>-0.260556</td>\n",
              "      <td>0.011920</td>\n",
              "      <td>-0.129273</td>\n",
              "      <td>-0.044518</td>\n",
              "      <td>-0.345423</td>\n",
              "      <td>-0.378261</td>\n",
              "      <td>-0.088449</td>\n",
              "      <td>-0.091060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047763</td>\n",
              "      <td>-0.394512</td>\n",
              "      <td>-0.238776</td>\n",
              "      <td>0.080928</td>\n",
              "      <td>0.057844</td>\n",
              "      <td>-0.205098</td>\n",
              "      <td>-0.067513</td>\n",
              "      <td>0.173058</td>\n",
              "      <td>-0.156445</td>\n",
              "      <td>-0.277954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.004506</td>\n",
              "      <td>0.042368</td>\n",
              "      <td>-0.178465</td>\n",
              "      <td>-0.138479</td>\n",
              "      <td>-0.138268</td>\n",
              "      <td>0.024902</td>\n",
              "      <td>-0.438153</td>\n",
              "      <td>-0.450127</td>\n",
              "      <td>0.066984</td>\n",
              "      <td>-0.017753</td>\n",
              "      <td>...</td>\n",
              "      <td>0.116500</td>\n",
              "      <td>-0.255289</td>\n",
              "      <td>-0.278228</td>\n",
              "      <td>-0.081235</td>\n",
              "      <td>0.251969</td>\n",
              "      <td>-0.066206</td>\n",
              "      <td>0.095656</td>\n",
              "      <td>0.282228</td>\n",
              "      <td>0.016337</td>\n",
              "      <td>-0.212731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.094474</td>\n",
              "      <td>-0.080044</td>\n",
              "      <td>-0.222468</td>\n",
              "      <td>-0.158952</td>\n",
              "      <td>-0.024708</td>\n",
              "      <td>0.004351</td>\n",
              "      <td>-0.463666</td>\n",
              "      <td>-0.344648</td>\n",
              "      <td>-0.101808</td>\n",
              "      <td>-0.012426</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.117917</td>\n",
              "      <td>-0.139355</td>\n",
              "      <td>-0.228969</td>\n",
              "      <td>-0.062332</td>\n",
              "      <td>0.132850</td>\n",
              "      <td>-0.269122</td>\n",
              "      <td>0.024864</td>\n",
              "      <td>0.288779</td>\n",
              "      <td>-0.186985</td>\n",
              "      <td>-0.205087</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 128 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7986ed55-436b-4557-bd65-b292d4aa12c8')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-7986ed55-436b-4557-bd65-b292d4aa12c8 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-7986ed55-436b-4557-bd65-b292d4aa12c8');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        w_0       w_1       w_2       w_3       w_4       w_5       w_6  \\\n",
              "0 -0.095379  0.040758 -0.210948 -0.064362 -0.225940 -0.002323 -0.555826   \n",
              "1 -0.151047 -0.107315 -0.221964 -0.034725  0.010814 -0.009673 -0.434323   \n",
              "2 -0.114799 -0.175982 -0.260556  0.011920 -0.129273 -0.044518 -0.345423   \n",
              "3  0.004506  0.042368 -0.178465 -0.138479 -0.138268  0.024902 -0.438153   \n",
              "4 -0.094474 -0.080044 -0.222468 -0.158952 -0.024708  0.004351 -0.463666   \n",
              "\n",
              "        w_7       w_8       w_9  ...     w_118     w_119     w_120     w_121  \\\n",
              "0 -0.352909  0.040634 -0.195226  ...  0.146386 -0.378653 -0.328871 -0.136878   \n",
              "1 -0.461733  0.112732  0.091751  ...  0.047749 -0.065133 -0.208891 -0.366014   \n",
              "2 -0.378261 -0.088449 -0.091060  ...  0.047763 -0.394512 -0.238776  0.080928   \n",
              "3 -0.450127  0.066984 -0.017753  ...  0.116500 -0.255289 -0.278228 -0.081235   \n",
              "4 -0.344648 -0.101808 -0.012426  ... -0.117917 -0.139355 -0.228969 -0.062332   \n",
              "\n",
              "      w_122     w_123     w_124     w_125     w_126     w_127  \n",
              "0 -0.007779 -0.157634 -0.069693  0.061569 -0.027663 -0.133832  \n",
              "1 -0.045869 -0.297474  0.056755  0.345754 -0.027737 -0.218527  \n",
              "2  0.057844 -0.205098 -0.067513  0.173058 -0.156445 -0.277954  \n",
              "3  0.251969 -0.066206  0.095656  0.282228  0.016337 -0.212731  \n",
              "4  0.132850 -0.269122  0.024864  0.288779 -0.186985 -0.205087  \n",
              "\n",
              "[5 rows x 128 columns]"
            ]
          },
          "execution_count": 20,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Node_Fec.head()"
      ],
      "id": "LiKcKi-DBOTZ"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TrWhL3xfBY9R",
        "outputId": "efe45913-bb33-41f0-8d94-b3df2016606e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "(736389, 50)\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "scaling = StandardScaler()\n",
        "\n",
        "Scaled_data = Node_Fec\n",
        "\n",
        "scaling.fit(Scaled_data)\n",
        "Scaled_data = scaling.transform(Scaled_data)\n",
        "\n",
        "m = 50\n",
        "principal = PCA(n_components=m)\n",
        "principal.fit(Scaled_data)\n",
        "x = principal.transform(Scaled_data)\n",
        "\n",
        "print(x.shape)"
      ],
      "id": "TrWhL3xfBY9R"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQgblr6jCOYf"
      },
      "outputs": [],
      "source": [
        "Scaled_data = pd.DataFrame(x)"
      ],
      "id": "BQgblr6jCOYf"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 299
        },
        "id": "SnL9pYsmCkj_",
        "outputId": "70c62ddd-8152-44ba-ff36-7c62e533aed7"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-611928cb-986c-456e-9bfe-aa2d6c8a6db6\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-3.082088</td>\n",
              "      <td>-3.770587</td>\n",
              "      <td>0.435245</td>\n",
              "      <td>5.163740</td>\n",
              "      <td>0.475839</td>\n",
              "      <td>0.605648</td>\n",
              "      <td>-0.021783</td>\n",
              "      <td>2.444352</td>\n",
              "      <td>1.843078</td>\n",
              "      <td>0.596841</td>\n",
              "      <td>...</td>\n",
              "      <td>0.079877</td>\n",
              "      <td>-0.421684</td>\n",
              "      <td>0.577886</td>\n",
              "      <td>0.675871</td>\n",
              "      <td>0.317556</td>\n",
              "      <td>0.489458</td>\n",
              "      <td>0.695124</td>\n",
              "      <td>0.216502</td>\n",
              "      <td>0.288270</td>\n",
              "      <td>0.049476</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.728420</td>\n",
              "      <td>-2.530880</td>\n",
              "      <td>-2.933542</td>\n",
              "      <td>0.058950</td>\n",
              "      <td>3.020178</td>\n",
              "      <td>2.115058</td>\n",
              "      <td>0.902393</td>\n",
              "      <td>0.766614</td>\n",
              "      <td>1.266361</td>\n",
              "      <td>0.704747</td>\n",
              "      <td>...</td>\n",
              "      <td>0.662028</td>\n",
              "      <td>-0.053322</td>\n",
              "      <td>1.400128</td>\n",
              "      <td>0.167979</td>\n",
              "      <td>-0.548252</td>\n",
              "      <td>-0.075149</td>\n",
              "      <td>-0.206946</td>\n",
              "      <td>-0.419678</td>\n",
              "      <td>-0.878682</td>\n",
              "      <td>0.092751</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-5.002440</td>\n",
              "      <td>5.485702</td>\n",
              "      <td>-1.026681</td>\n",
              "      <td>0.128345</td>\n",
              "      <td>-0.158239</td>\n",
              "      <td>-0.279623</td>\n",
              "      <td>2.143083</td>\n",
              "      <td>2.579699</td>\n",
              "      <td>1.360662</td>\n",
              "      <td>0.643789</td>\n",
              "      <td>...</td>\n",
              "      <td>0.089046</td>\n",
              "      <td>0.193530</td>\n",
              "      <td>1.257107</td>\n",
              "      <td>-0.758059</td>\n",
              "      <td>0.343196</td>\n",
              "      <td>0.729315</td>\n",
              "      <td>0.628578</td>\n",
              "      <td>0.103256</td>\n",
              "      <td>1.271947</td>\n",
              "      <td>0.382716</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-1.776126</td>\n",
              "      <td>-3.535830</td>\n",
              "      <td>0.390636</td>\n",
              "      <td>0.905836</td>\n",
              "      <td>0.544934</td>\n",
              "      <td>2.867392</td>\n",
              "      <td>1.611869</td>\n",
              "      <td>-0.885575</td>\n",
              "      <td>1.645747</td>\n",
              "      <td>1.556516</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.691473</td>\n",
              "      <td>-1.383707</td>\n",
              "      <td>0.888929</td>\n",
              "      <td>0.680112</td>\n",
              "      <td>-0.544849</td>\n",
              "      <td>1.186543</td>\n",
              "      <td>0.049447</td>\n",
              "      <td>0.043830</td>\n",
              "      <td>-0.440982</td>\n",
              "      <td>-1.941840</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.486382</td>\n",
              "      <td>-1.171743</td>\n",
              "      <td>-1.731701</td>\n",
              "      <td>-0.380522</td>\n",
              "      <td>-1.250264</td>\n",
              "      <td>0.951093</td>\n",
              "      <td>2.504003</td>\n",
              "      <td>1.279308</td>\n",
              "      <td>0.567930</td>\n",
              "      <td>1.005565</td>\n",
              "      <td>...</td>\n",
              "      <td>0.361851</td>\n",
              "      <td>-0.307620</td>\n",
              "      <td>0.215941</td>\n",
              "      <td>0.266164</td>\n",
              "      <td>-0.103523</td>\n",
              "      <td>0.545216</td>\n",
              "      <td>-0.205958</td>\n",
              "      <td>0.311127</td>\n",
              "      <td>-0.758992</td>\n",
              "      <td>-0.590508</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 50 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-611928cb-986c-456e-9bfe-aa2d6c8a6db6')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-611928cb-986c-456e-9bfe-aa2d6c8a6db6 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-611928cb-986c-456e-9bfe-aa2d6c8a6db6');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "         0         1         2         3         4         5         6   \\\n",
              "0 -3.082088 -3.770587  0.435245  5.163740  0.475839  0.605648 -0.021783   \n",
              "1  0.728420 -2.530880 -2.933542  0.058950  3.020178  2.115058  0.902393   \n",
              "2 -5.002440  5.485702 -1.026681  0.128345 -0.158239 -0.279623  2.143083   \n",
              "3 -1.776126 -3.535830  0.390636  0.905836  0.544934  2.867392  1.611869   \n",
              "4 -1.486382 -1.171743 -1.731701 -0.380522 -1.250264  0.951093  2.504003   \n",
              "\n",
              "         7         8         9   ...        40        41        42        43  \\\n",
              "0  2.444352  1.843078  0.596841  ...  0.079877 -0.421684  0.577886  0.675871   \n",
              "1  0.766614  1.266361  0.704747  ...  0.662028 -0.053322  1.400128  0.167979   \n",
              "2  2.579699  1.360662  0.643789  ...  0.089046  0.193530  1.257107 -0.758059   \n",
              "3 -0.885575  1.645747  1.556516  ... -0.691473 -1.383707  0.888929  0.680112   \n",
              "4  1.279308  0.567930  1.005565  ...  0.361851 -0.307620  0.215941  0.266164   \n",
              "\n",
              "         44        45        46        47        48        49  \n",
              "0  0.317556  0.489458  0.695124  0.216502  0.288270  0.049476  \n",
              "1 -0.548252 -0.075149 -0.206946 -0.419678 -0.878682  0.092751  \n",
              "2  0.343196  0.729315  0.628578  0.103256  1.271947  0.382716  \n",
              "3 -0.544849  1.186543  0.049447  0.043830 -0.440982 -1.941840  \n",
              "4 -0.103523  0.545216 -0.205958  0.311127 -0.758992 -0.590508  \n",
              "\n",
              "[5 rows x 50 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Scaled_data.head()"
      ],
      "id": "SnL9pYsmCkj_"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "cL9JLe4yKJUG",
        "outputId": "de47d887-57d0-422d-8a77-f6534e4134c8"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-8075d2f0-2c23-447a-a9fc-afa27210b23d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>689</th>\n",
              "      <th>690</th>\n",
              "      <th>691</th>\n",
              "      <th>692</th>\n",
              "      <th>693</th>\n",
              "      <th>694</th>\n",
              "      <th>695</th>\n",
              "      <th>696</th>\n",
              "      <th>697</th>\n",
              "      <th>Class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>246</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>189</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>131</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>95</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 699 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-8075d2f0-2c23-447a-a9fc-afa27210b23d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-8075d2f0-2c23-447a-a9fc-afa27210b23d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-8075d2f0-2c23-447a-a9fc-afa27210b23d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   0  1  2  3  4  5  6  7  8  9  ...  689  690  691  692  693  694  695  696  \\\n",
              "0  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
              "1  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
              "2  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
              "3  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
              "4  0  0  0  0  0  0  0  0  0  0  ...    0    0    0    0    0    0    0    0   \n",
              "\n",
              "   697  Class  \n",
              "0    0    246  \n",
              "1    0    131  \n",
              "2    0    189  \n",
              "3    0    131  \n",
              "4    0     95  \n",
              "\n",
              "[5 rows x 699 columns]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.head()"
      ],
      "id": "cL9JLe4yKJUG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tqRQcBuYF0fq"
      },
      "outputs": [],
      "source": [
        "Data = pd.concat([data, Node_Fec], axis=1)"
      ],
      "id": "tqRQcBuYF0fq"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "id": "m2xMvumCF6GB",
        "outputId": "5998c70a-0372-4477-e9e7-d383b1c7a44c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-b374c937-8d52-4521-9a85-5453ff7dd3db\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>w_118</th>\n",
              "      <th>w_119</th>\n",
              "      <th>w_120</th>\n",
              "      <th>w_121</th>\n",
              "      <th>w_122</th>\n",
              "      <th>w_123</th>\n",
              "      <th>w_124</th>\n",
              "      <th>w_125</th>\n",
              "      <th>w_126</th>\n",
              "      <th>w_127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.146386</td>\n",
              "      <td>-0.378653</td>\n",
              "      <td>-0.328871</td>\n",
              "      <td>-0.136878</td>\n",
              "      <td>-0.007779</td>\n",
              "      <td>-0.157634</td>\n",
              "      <td>-0.069693</td>\n",
              "      <td>0.061569</td>\n",
              "      <td>-0.027663</td>\n",
              "      <td>-0.133832</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047749</td>\n",
              "      <td>-0.065133</td>\n",
              "      <td>-0.208891</td>\n",
              "      <td>-0.366014</td>\n",
              "      <td>-0.045869</td>\n",
              "      <td>-0.297474</td>\n",
              "      <td>0.056755</td>\n",
              "      <td>0.345754</td>\n",
              "      <td>-0.027737</td>\n",
              "      <td>-0.218527</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.047763</td>\n",
              "      <td>-0.394512</td>\n",
              "      <td>-0.238776</td>\n",
              "      <td>0.080928</td>\n",
              "      <td>0.057844</td>\n",
              "      <td>-0.205098</td>\n",
              "      <td>-0.067513</td>\n",
              "      <td>0.173058</td>\n",
              "      <td>-0.156445</td>\n",
              "      <td>-0.277954</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.116500</td>\n",
              "      <td>-0.255289</td>\n",
              "      <td>-0.278228</td>\n",
              "      <td>-0.081235</td>\n",
              "      <td>0.251969</td>\n",
              "      <td>-0.066206</td>\n",
              "      <td>0.095656</td>\n",
              "      <td>0.282228</td>\n",
              "      <td>0.016337</td>\n",
              "      <td>-0.212731</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.117917</td>\n",
              "      <td>-0.139355</td>\n",
              "      <td>-0.228969</td>\n",
              "      <td>-0.062332</td>\n",
              "      <td>0.132850</td>\n",
              "      <td>-0.269122</td>\n",
              "      <td>0.024864</td>\n",
              "      <td>0.288779</td>\n",
              "      <td>-0.186985</td>\n",
              "      <td>-0.205087</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 827 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b374c937-8d52-4521-9a85-5453ff7dd3db')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-b374c937-8d52-4521-9a85-5453ff7dd3db button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-b374c937-8d52-4521-9a85-5453ff7dd3db');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "   0  1  2  3  4  5  6  7  8  9  ...     w_118     w_119     w_120     w_121  \\\n",
              "0  0  0  0  0  0  0  0  0  0  0  ...  0.146386 -0.378653 -0.328871 -0.136878   \n",
              "1  0  0  0  0  0  0  0  0  0  0  ...  0.047749 -0.065133 -0.208891 -0.366014   \n",
              "2  0  0  0  0  0  0  0  0  0  0  ...  0.047763 -0.394512 -0.238776  0.080928   \n",
              "3  0  0  0  0  0  0  0  0  0  0  ...  0.116500 -0.255289 -0.278228 -0.081235   \n",
              "4  0  0  0  0  0  0  0  0  0  0  ... -0.117917 -0.139355 -0.228969 -0.062332   \n",
              "\n",
              "      w_122     w_123     w_124     w_125     w_126     w_127  \n",
              "0 -0.007779 -0.157634 -0.069693  0.061569 -0.027663 -0.133832  \n",
              "1 -0.045869 -0.297474  0.056755  0.345754 -0.027737 -0.218527  \n",
              "2  0.057844 -0.205098 -0.067513  0.173058 -0.156445 -0.277954  \n",
              "3  0.251969 -0.066206  0.095656  0.282228  0.016337 -0.212731  \n",
              "4  0.132850 -0.269122  0.024864  0.288779 -0.186985 -0.205087  \n",
              "\n",
              "[5 rows x 827 columns]"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "Data.head()"
      ],
      "id": "m2xMvumCF6GB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "x2UTao5-PKZ0",
        "outputId": "de30874f-feb7-4995-8a0a-5bb93d121998"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[     0      0      0 ... 736388 736388 736388]\n",
            "[    88  27449 121051 ... 421711 427339 439864]\n",
            "[2010 2011 2012 2013 2014 2015 2016 2017 2018 2019]\n",
            "41939\n"
          ]
        }
      ],
      "source": [
        "Node_Year=graph[0]['node_year']['paper']\n",
        "Node_Year_list=list(Node_Year[:,0])\n",
        "\n",
        "E=graph[0]['edge_index_dict'][('paper', 'cites', 'paper')]\n",
        "EdgeL=E[0]\n",
        "EdgeR=E[1]\n",
        "print(EdgeL)\n",
        "print(EdgeR)\n",
        "print(np.unique(Node_Year_list))\n",
        "\n",
        "Node_Year_list.index(2019)\n",
        "list_size = len(Node_Year_list)\n",
        "NIndex_19=[]\n",
        "# declare for loop\n",
        "for itr in range(list_size):\n",
        " \n",
        "      # check the condition\n",
        "    if(Node_Year_list[itr] == 2019):\n",
        "        NIndex_19.append(itr)\n",
        " \n",
        "          # print the indices\n",
        "print(len(NIndex_19))"
      ],
      "id": "x2UTao5-PKZ0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eAYhC_zJPN2n",
        "outputId": "e8a461d2-a01c-4262-d994-b9d70c65008d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[246 131 189 ... 266 289   1]\n"
          ]
        }
      ],
      "source": [
        "N_class_train=Node_Class\n",
        "for d in NIndex_19:\n",
        "    N_class_train[d]=500\n",
        "print(N_class_train)"
      ],
      "id": "eAYhC_zJPN2n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "pi_LNaZ-SPGY"
      },
      "outputs": [],
      "source": [
        "N_class_train=Node_Class\n",
        "y_pred = [np.argmax(i) for i in y_pred]\n",
        "for i, d in enumerate(NIndex_19):\n",
        "  N_class_train[d] = y_pred[i]"
      ],
      "id": "pi_LNaZ-SPGY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PXmFauFSY7sX",
        "outputId": "508fec77-16f1-4c6a-c2d9-3ff1d17a01a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "736389"
            ]
          },
          "execution_count": 46,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(N_class_train)"
      ],
      "id": "PXmFauFSY7sX"
    },
    {
      "cell_type": "code",
      "source": [
        "F_vec = n*classes\n",
        "# for i in range(len(EdgeL)):\n",
        "  left, right = EdgeL[i], EdgeR[i]\n",
        "  get_class(right)\n",
        "  F_vec[left][get_class(right)] += 1\n",
        "\n",
        "  #O(E)\n",
        "  "
      ],
      "metadata": {
        "id": "ceDgJTqRAqmz"
      },
      "id": "ceDgJTqRAqmz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "N9c1wfEtPFmc",
        "outputId": "80f20731-c269-4542-f0bd-0408c5ee03ab"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processing file 736388 (99%)"
          ]
        }
      ],
      "source": [
        "def CountX(lst,x):\n",
        "    return(lst.count(x))\n",
        "\n",
        "Node_class=list(range(349))\n",
        "F_vec=[]\n",
        "n=736389\n",
        "for i in range(n):\n",
        "    print(\"\\rProcessing file {} ({}%)\".format(i, 100*i//n), end='', flush=True)\n",
        "    node_F=[]\n",
        "    list_out=[]\n",
        "    list_In=[]\n",
        "    #S_nbd_out=[]\n",
        "    #S_nbd_in=[]\n",
        "    indx=np.where(EdgeL==i)\n",
        "    indxIn=np.where(EdgeR==i)\n",
        "    #print(indx)\n",
        "    for j in indx[0]:\n",
        "        list_out.append(N_class_train[EdgeR[j]])\n",
        "   # indx[0].clear()\n",
        "    for k in indxIn[0]:\n",
        "        list_In.append(N_class_train[EdgeL[k]])\n",
        "   # indxIn[0].clear()\n",
        "\n",
        "                \n",
        "                \n",
        "    for d in Node_class:\n",
        "        node_F.append(CountX(list_out,d))\n",
        "        node_F.append(CountX(list_In,d))\n",
        "    F_vec.append(node_F)\n"
      ],
      "id": "N9c1wfEtPFmc"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "5PDty8tMS5pC"
      },
      "outputs": [],
      "source": [
        "data = pd.DataFrame(F_vec)\n",
        "data.insert(loc=698, column='Class', value=Node_Class)\n"
      ],
      "id": "5PDty8tMS5pC"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "bOGwtR6tUQbN"
      },
      "outputs": [],
      "source": [
        "Data = pd.concat([data, Node_Fec], axis=1)"
      ],
      "id": "bOGwtR6tUQbN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsMfhdRAWv9U",
        "outputId": "41165445-dace-4f5a-e127-ac0bf7fccebd"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "698"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data.columns.get_loc('Class')"
      ],
      "id": "gsMfhdRAWv9U"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fFObsX1XO4m",
        "outputId": "f776eb72-e47d-45ba-ba9f-13a446da1832"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "736389"
            ]
          },
          "execution_count": 31,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "len(Node_Class)"
      ],
      "id": "6fFObsX1XO4m"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rFOvn0MCWQXO",
        "outputId": "e4c622c1-d9bc-4a02-dddd-120c560dfd1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96971\n"
          ]
        }
      ],
      "source": [
        "with open('/content/Feature_nbd_mag.csv') as f:\n",
        "  lines = f.readlines()\n",
        "  line = lines[96970].split(',')\n",
        "  print(len(lines))"
      ],
      "id": "rFOvn0MCWQXO"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kp3AvQbjGnni"
      },
      "outputs": [],
      "source": [
        "X = Data.drop('Class', axis=1)\n",
        "y = Data['Class']\n",
        "X_train=X.iloc[list(train_idx['paper'])+list(valid_idx['paper'])]\n",
        "X_test=X.iloc[test_idx['paper']]\n",
        "y_train=y.iloc[list(train_idx['paper'])+list(valid_idx['paper'])]\n",
        "y_test=y.iloc[test_idx['paper']]"
      ],
      "id": "Kp3AvQbjGnni"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "CxBRldqyLE4F"
      },
      "outputs": [],
      "source": [
        "earlystop1 = EarlyStopping(patience=7)\n",
        "earlystop2 = ReduceLROnPlateau(monitor='val_loss',\n",
        "                               min_lr = 3e-7, \n",
        "                               patience = 4,\n",
        "                               factor=0.3,\n",
        "                               verbose = 1)"
      ],
      "id": "CxBRldqyLE4F"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 467
        },
        "id": "ujSOTIuaJhAG",
        "outputId": "bfa1acc5-1964-40e4-f847-1b5fb6f75258"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-ff867d25-b50b-4270-9afe-622a299d34bf\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>...</th>\n",
              "      <th>w_118</th>\n",
              "      <th>w_119</th>\n",
              "      <th>w_120</th>\n",
              "      <th>w_121</th>\n",
              "      <th>w_122</th>\n",
              "      <th>w_123</th>\n",
              "      <th>w_124</th>\n",
              "      <th>w_125</th>\n",
              "      <th>w_126</th>\n",
              "      <th>w_127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>359</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.098125</td>\n",
              "      <td>-0.268199</td>\n",
              "      <td>-0.205214</td>\n",
              "      <td>-0.097784</td>\n",
              "      <td>0.193881</td>\n",
              "      <td>-0.225376</td>\n",
              "      <td>-0.054646</td>\n",
              "      <td>0.123194</td>\n",
              "      <td>0.025784</td>\n",
              "      <td>-0.188598</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>411</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>6</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.070180</td>\n",
              "      <td>-0.256800</td>\n",
              "      <td>-0.201145</td>\n",
              "      <td>-0.040072</td>\n",
              "      <td>0.137642</td>\n",
              "      <td>-0.249818</td>\n",
              "      <td>0.019517</td>\n",
              "      <td>0.283035</td>\n",
              "      <td>-0.211814</td>\n",
              "      <td>-0.444755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>608</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>30</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.227743</td>\n",
              "      <td>-0.848724</td>\n",
              "      <td>-0.380617</td>\n",
              "      <td>0.224889</td>\n",
              "      <td>-0.349129</td>\n",
              "      <td>-0.437328</td>\n",
              "      <td>0.015382</td>\n",
              "      <td>0.271330</td>\n",
              "      <td>-0.378993</td>\n",
              "      <td>-0.388307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>873</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.015031</td>\n",
              "      <td>-0.349369</td>\n",
              "      <td>-0.389953</td>\n",
              "      <td>0.037686</td>\n",
              "      <td>0.309974</td>\n",
              "      <td>-0.082996</td>\n",
              "      <td>0.041261</td>\n",
              "      <td>0.167244</td>\n",
              "      <td>-0.091509</td>\n",
              "      <td>-0.015484</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11951</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.026104</td>\n",
              "      <td>-0.084677</td>\n",
              "      <td>-0.135089</td>\n",
              "      <td>-0.213256</td>\n",
              "      <td>0.223176</td>\n",
              "      <td>-0.161862</td>\n",
              "      <td>0.032179</td>\n",
              "      <td>0.279161</td>\n",
              "      <td>-0.085118</td>\n",
              "      <td>-0.231291</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736349</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.090542</td>\n",
              "      <td>-0.163129</td>\n",
              "      <td>-0.285086</td>\n",
              "      <td>-0.358813</td>\n",
              "      <td>0.259896</td>\n",
              "      <td>0.033160</td>\n",
              "      <td>0.008394</td>\n",
              "      <td>0.146009</td>\n",
              "      <td>0.122744</td>\n",
              "      <td>-0.156794</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736354</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.115263</td>\n",
              "      <td>-0.148100</td>\n",
              "      <td>-0.342683</td>\n",
              "      <td>0.172944</td>\n",
              "      <td>0.077405</td>\n",
              "      <td>0.099258</td>\n",
              "      <td>0.026068</td>\n",
              "      <td>0.458861</td>\n",
              "      <td>-0.165574</td>\n",
              "      <td>-0.159797</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736358</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.023282</td>\n",
              "      <td>-0.202687</td>\n",
              "      <td>-0.163427</td>\n",
              "      <td>0.055721</td>\n",
              "      <td>0.159572</td>\n",
              "      <td>0.089135</td>\n",
              "      <td>0.016526</td>\n",
              "      <td>0.169853</td>\n",
              "      <td>-0.309299</td>\n",
              "      <td>-0.111012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736384</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0.043255</td>\n",
              "      <td>-0.065534</td>\n",
              "      <td>-0.255169</td>\n",
              "      <td>-0.117872</td>\n",
              "      <td>0.276171</td>\n",
              "      <td>-0.185582</td>\n",
              "      <td>-0.005036</td>\n",
              "      <td>0.197039</td>\n",
              "      <td>-0.060304</td>\n",
              "      <td>-0.160619</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>736385</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.209520</td>\n",
              "      <td>-0.452308</td>\n",
              "      <td>0.058459</td>\n",
              "      <td>-0.077297</td>\n",
              "      <td>0.114042</td>\n",
              "      <td>-0.241362</td>\n",
              "      <td>0.234753</td>\n",
              "      <td>0.546750</td>\n",
              "      <td>-0.273364</td>\n",
              "      <td>-0.521035</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>41939 rows × 826 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff867d25-b50b-4270-9afe-622a299d34bf')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ff867d25-b50b-4270-9afe-622a299d34bf button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ff867d25-b50b-4270-9afe-622a299d34bf');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "        0  1   2  3  4  5  6  7  8  9  ...     w_118     w_119     w_120  \\\n",
              "359     0  0   0  0  0  0  0  0  0  0  ...  0.098125 -0.268199 -0.205214   \n",
              "411     0  0   6  0  0  0  0  0  0  0  ... -0.070180 -0.256800 -0.201145   \n",
              "608     0  0  30  0  0  0  0  0  0  0  ...  0.227743 -0.848724 -0.380617   \n",
              "873     0  0   0  0  0  0  0  0  0  0  ...  0.015031 -0.349369 -0.389953   \n",
              "11951   0  0   2  0  0  0  0  0  0  0  ... -0.026104 -0.084677 -0.135089   \n",
              "...    .. ..  .. .. .. .. .. .. .. ..  ...       ...       ...       ...   \n",
              "736349  0  0   0  0  0  0  0  0  0  0  ... -0.090542 -0.163129 -0.285086   \n",
              "736354  0  0   0  0  0  0  0  0  0  0  ... -0.115263 -0.148100 -0.342683   \n",
              "736358  0  0   0  0  0  0  0  0  0  0  ... -0.023282 -0.202687 -0.163427   \n",
              "736384  0  0   0  0  0  0  0  0  0  0  ...  0.043255 -0.065534 -0.255169   \n",
              "736385  0  0   1  0  0  0  0  0  0  0  ... -0.209520 -0.452308  0.058459   \n",
              "\n",
              "           w_121     w_122     w_123     w_124     w_125     w_126     w_127  \n",
              "359    -0.097784  0.193881 -0.225376 -0.054646  0.123194  0.025784 -0.188598  \n",
              "411    -0.040072  0.137642 -0.249818  0.019517  0.283035 -0.211814 -0.444755  \n",
              "608     0.224889 -0.349129 -0.437328  0.015382  0.271330 -0.378993 -0.388307  \n",
              "873     0.037686  0.309974 -0.082996  0.041261  0.167244 -0.091509 -0.015484  \n",
              "11951  -0.213256  0.223176 -0.161862  0.032179  0.279161 -0.085118 -0.231291  \n",
              "...          ...       ...       ...       ...       ...       ...       ...  \n",
              "736349 -0.358813  0.259896  0.033160  0.008394  0.146009  0.122744 -0.156794  \n",
              "736354  0.172944  0.077405  0.099258  0.026068  0.458861 -0.165574 -0.159797  \n",
              "736358  0.055721  0.159572  0.089135  0.016526  0.169853 -0.309299 -0.111012  \n",
              "736384 -0.117872  0.276171 -0.185582 -0.005036  0.197039 -0.060304 -0.160619  \n",
              "736385 -0.077297  0.114042 -0.241362  0.234753  0.546750 -0.273364 -0.521035  \n",
              "\n",
              "[41939 rows x 826 columns]"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "X_test"
      ],
      "id": "ujSOTIuaJhAG"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ebMMFl8NQao",
        "outputId": "6b03cfe5-a18e-40a9-e0cc-6238eefb8736"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "min(Node_Class)"
      ],
      "id": "8ebMMFl8NQao"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lCgRwWB2Jjr8",
        "outputId": "00a3535f-7c0f-4431-8d85-f267f03f5769"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "359       500\n",
              "411       500\n",
              "608       500\n",
              "873       500\n",
              "11951     500\n",
              "         ... \n",
              "736349    500\n",
              "736354    500\n",
              "736358    500\n",
              "736384    500\n",
              "736385    500\n",
              "Name: Class, Length: 41939, dtype: int64"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "y_test"
      ],
      "id": "lCgRwWB2Jjr8"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "8cZMB880LE4F",
        "outputId": "833e3d46-c09a-48de-d440-4f82bb3541c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "------------------------------------------------------------------------\n",
            "Training for fold 1 ...\n",
            "Epoch 1/100\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 3.5877 - sparse_categorical_accuracy: 0.2834 - val_loss: 2.7656 - val_sparse_categorical_accuracy: 0.3953 - lr: 6.0000e-05\n",
            "Epoch 2/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.8138 - sparse_categorical_accuracy: 0.3721 - val_loss: 2.6073 - val_sparse_categorical_accuracy: 0.4094 - lr: 6.0000e-05\n",
            "Epoch 3/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.6519 - sparse_categorical_accuracy: 0.3939 - val_loss: 2.5581 - val_sparse_categorical_accuracy: 0.4105 - lr: 6.0000e-05\n",
            "Epoch 4/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.5752 - sparse_categorical_accuracy: 0.4035 - val_loss: 2.5259 - val_sparse_categorical_accuracy: 0.4126 - lr: 6.0000e-05\n",
            "Epoch 5/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.5305 - sparse_categorical_accuracy: 0.4088 - val_loss: 2.5041 - val_sparse_categorical_accuracy: 0.4130 - lr: 6.0000e-05\n",
            "Epoch 6/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4982 - sparse_categorical_accuracy: 0.4134 - val_loss: 2.4999 - val_sparse_categorical_accuracy: 0.4132 - lr: 6.0000e-05\n",
            "Epoch 7/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4756 - sparse_categorical_accuracy: 0.4162 - val_loss: 2.4726 - val_sparse_categorical_accuracy: 0.4154 - lr: 6.0000e-05\n",
            "Epoch 8/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4597 - sparse_categorical_accuracy: 0.4176 - val_loss: 2.4691 - val_sparse_categorical_accuracy: 0.4124 - lr: 6.0000e-05\n",
            "Epoch 9/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4461 - sparse_categorical_accuracy: 0.4194 - val_loss: 2.4666 - val_sparse_categorical_accuracy: 0.4129 - lr: 6.0000e-05\n",
            "Epoch 10/100\n",
            "10851/10851 [==============================] - 32s 3ms/step - loss: 2.4327 - sparse_categorical_accuracy: 0.4203 - val_loss: 2.4555 - val_sparse_categorical_accuracy: 0.4169 - lr: 6.0000e-05\n",
            "Epoch 11/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4245 - sparse_categorical_accuracy: 0.4212 - val_loss: 2.4544 - val_sparse_categorical_accuracy: 0.4152 - lr: 6.0000e-05\n",
            "Epoch 12/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4149 - sparse_categorical_accuracy: 0.4230 - val_loss: 2.4530 - val_sparse_categorical_accuracy: 0.4148 - lr: 6.0000e-05\n",
            "Epoch 13/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4080 - sparse_categorical_accuracy: 0.4236 - val_loss: 2.4500 - val_sparse_categorical_accuracy: 0.4139 - lr: 6.0000e-05\n",
            "Epoch 14/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.4022 - sparse_categorical_accuracy: 0.4232 - val_loss: 2.4496 - val_sparse_categorical_accuracy: 0.4160 - lr: 6.0000e-05\n",
            "Epoch 15/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3942 - sparse_categorical_accuracy: 0.4247 - val_loss: 2.4405 - val_sparse_categorical_accuracy: 0.4151 - lr: 6.0000e-05\n",
            "Epoch 16/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3911 - sparse_categorical_accuracy: 0.4249 - val_loss: 2.4365 - val_sparse_categorical_accuracy: 0.4154 - lr: 6.0000e-05\n",
            "Epoch 17/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3881 - sparse_categorical_accuracy: 0.4247 - val_loss: 2.4420 - val_sparse_categorical_accuracy: 0.4143 - lr: 6.0000e-05\n",
            "Epoch 18/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3819 - sparse_categorical_accuracy: 0.4254 - val_loss: 2.4380 - val_sparse_categorical_accuracy: 0.4163 - lr: 6.0000e-05\n",
            "Epoch 19/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3777 - sparse_categorical_accuracy: 0.4259 - val_loss: 2.4355 - val_sparse_categorical_accuracy: 0.4149 - lr: 6.0000e-05\n",
            "Epoch 20/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3768 - sparse_categorical_accuracy: 0.4259 - val_loss: 2.4379 - val_sparse_categorical_accuracy: 0.4150 - lr: 6.0000e-05\n",
            "Epoch 21/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3736 - sparse_categorical_accuracy: 0.4265 - val_loss: 2.4339 - val_sparse_categorical_accuracy: 0.4163 - lr: 6.0000e-05\n",
            "Epoch 22/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3694 - sparse_categorical_accuracy: 0.4266 - val_loss: 2.4296 - val_sparse_categorical_accuracy: 0.4157 - lr: 6.0000e-05\n",
            "Epoch 23/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3651 - sparse_categorical_accuracy: 0.4270 - val_loss: 2.4312 - val_sparse_categorical_accuracy: 0.4130 - lr: 6.0000e-05\n",
            "Epoch 24/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3644 - sparse_categorical_accuracy: 0.4271 - val_loss: 2.4405 - val_sparse_categorical_accuracy: 0.4138 - lr: 6.0000e-05\n",
            "Epoch 25/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3627 - sparse_categorical_accuracy: 0.4273 - val_loss: 2.4300 - val_sparse_categorical_accuracy: 0.4168 - lr: 6.0000e-05\n",
            "Epoch 26/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3608 - sparse_categorical_accuracy: 0.4267 - val_loss: 2.4294 - val_sparse_categorical_accuracy: 0.4151 - lr: 6.0000e-05\n",
            "Epoch 27/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3591 - sparse_categorical_accuracy: 0.4278 - val_loss: 2.4333 - val_sparse_categorical_accuracy: 0.4147 - lr: 6.0000e-05\n",
            "Epoch 28/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3574 - sparse_categorical_accuracy: 0.4278 - val_loss: 2.4252 - val_sparse_categorical_accuracy: 0.4169 - lr: 6.0000e-05\n",
            "Epoch 29/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3545 - sparse_categorical_accuracy: 0.4273 - val_loss: 2.4199 - val_sparse_categorical_accuracy: 0.4166 - lr: 6.0000e-05\n",
            "Epoch 30/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3531 - sparse_categorical_accuracy: 0.4282 - val_loss: 2.4244 - val_sparse_categorical_accuracy: 0.4157 - lr: 6.0000e-05\n",
            "Epoch 31/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3517 - sparse_categorical_accuracy: 0.4284 - val_loss: 2.4223 - val_sparse_categorical_accuracy: 0.4159 - lr: 6.0000e-05\n",
            "Epoch 32/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3507 - sparse_categorical_accuracy: 0.4280 - val_loss: 2.4271 - val_sparse_categorical_accuracy: 0.4138 - lr: 6.0000e-05\n",
            "Epoch 33/100\n",
            "10843/10851 [============================>.] - ETA: 0s - loss: 2.3500 - sparse_categorical_accuracy: 0.4287\n",
            "Epoch 33: ReduceLROnPlateau reducing learning rate to 1.7999999545281754e-05.\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3499 - sparse_categorical_accuracy: 0.4287 - val_loss: 2.4268 - val_sparse_categorical_accuracy: 0.4165 - lr: 6.0000e-05\n",
            "Epoch 34/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3406 - sparse_categorical_accuracy: 0.4297 - val_loss: 2.4162 - val_sparse_categorical_accuracy: 0.4167 - lr: 1.8000e-05\n",
            "Epoch 35/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3409 - sparse_categorical_accuracy: 0.4299 - val_loss: 2.4191 - val_sparse_categorical_accuracy: 0.4168 - lr: 1.8000e-05\n",
            "Epoch 36/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3399 - sparse_categorical_accuracy: 0.4300 - val_loss: 2.4214 - val_sparse_categorical_accuracy: 0.4162 - lr: 1.8000e-05\n",
            "Epoch 37/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3393 - sparse_categorical_accuracy: 0.4298 - val_loss: 2.4207 - val_sparse_categorical_accuracy: 0.4161 - lr: 1.8000e-05\n",
            "Epoch 38/100\n",
            "10840/10851 [============================>.] - ETA: 0s - loss: 2.3384 - sparse_categorical_accuracy: 0.4298\n",
            "Epoch 38: ReduceLROnPlateau reducing learning rate to 5.399999645305797e-06.\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3384 - sparse_categorical_accuracy: 0.4298 - val_loss: 2.4168 - val_sparse_categorical_accuracy: 0.4167 - lr: 1.8000e-05\n",
            "Epoch 39/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3368 - sparse_categorical_accuracy: 0.4299 - val_loss: 2.4173 - val_sparse_categorical_accuracy: 0.4173 - lr: 5.4000e-06\n",
            "Epoch 40/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3363 - sparse_categorical_accuracy: 0.4303 - val_loss: 2.4141 - val_sparse_categorical_accuracy: 0.4169 - lr: 5.4000e-06\n",
            "Epoch 41/100\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 2.3356 - sparse_categorical_accuracy: 0.4304 - val_loss: 2.4144 - val_sparse_categorical_accuracy: 0.4175 - lr: 5.4000e-06\n",
            "Epoch 42/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3349 - sparse_categorical_accuracy: 0.4301 - val_loss: 2.4182 - val_sparse_categorical_accuracy: 0.4167 - lr: 5.4000e-06\n",
            "Epoch 43/100\n",
            "10851/10851 [==============================] - 34s 3ms/step - loss: 2.3334 - sparse_categorical_accuracy: 0.4301 - val_loss: 2.4171 - val_sparse_categorical_accuracy: 0.4171 - lr: 5.4000e-06\n",
            "Epoch 44/100\n",
            "10837/10851 [============================>.] - ETA: 0s - loss: 2.3352 - sparse_categorical_accuracy: 0.4305\n",
            "Epoch 44: ReduceLROnPlateau reducing learning rate to 1.6199999208765803e-06.\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3352 - sparse_categorical_accuracy: 0.4305 - val_loss: 2.4184 - val_sparse_categorical_accuracy: 0.4164 - lr: 5.4000e-06\n",
            "Epoch 45/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3335 - sparse_categorical_accuracy: 0.4305 - val_loss: 2.4160 - val_sparse_categorical_accuracy: 0.4165 - lr: 1.6200e-06\n",
            "Epoch 46/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3342 - sparse_categorical_accuracy: 0.4306 - val_loss: 2.4173 - val_sparse_categorical_accuracy: 0.4163 - lr: 1.6200e-06\n",
            "Epoch 47/100\n",
            "10851/10851 [==============================] - 33s 3ms/step - loss: 2.3343 - sparse_categorical_accuracy: 0.4300 - val_loss: 2.4166 - val_sparse_categorical_accuracy: 0.4163 - lr: 1.6200e-06\n",
            "Score for fold 1: loss of 2.4143717288970947; sparse_categorical_accuracy of 41.75349771976471%\n"
          ]
        }
      ],
      "source": [
        "acc_per_fold = []\n",
        "loss_per_fold = []\n",
        "fold_no = 1\n",
        "checkpoint_filepath='/tmp/checkpoint'\n",
        "# for train, test in kfold.split(X_train, y_train):\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Input(shape=(len(X_train.iloc[0]),)),\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dropout(.2),\n",
        "    tf.keras.layers.Dense(100),\n",
        "    tf.keras.layers.Dropout(.2),\n",
        "    tf.keras.layers.Dense(max(y_train)+1, activation='softmax')\n",
        "])\n",
        "\n",
        "loss_fn = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(6e-5), loss=loss_fn, \n",
        "              metrics=[tf.keras.metrics.SparseCategoricalAccuracy()])\n",
        "print('------------------------------------------------------------------------')\n",
        "print(f'Training for fold {fold_no} ...')\n",
        "history = model.fit(np.array(X_train), np.array(y_train),\n",
        "                    batch_size = 64,\n",
        "                    epochs=100,\n",
        "                    use_multiprocessing=True,\n",
        "                    workers=4,\n",
        "                    validation_data=(np.array(X_test), np.array(y_test)),\n",
        "                    callbacks=[earlystop1, earlystop2,\n",
        "                                tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_filepath,\n",
        "                                                                save_best_only=True,\n",
        "                                                                save_weights_only=True,\n",
        "                                                                monitor='val_sparse_categorical_accuracy',\n",
        "                                                                mode='max')])\n",
        "model.load_weights(checkpoint_filepath)\n",
        "scores = model.evaluate(np.array(X_test), np.array(y_test), verbose=0)\n",
        "print(f'Score for fold {fold_no}: {model.metrics_names[0]} of {scores[0]}; {model.metrics_names[1]} of {scores[1]*100}%')\n",
        "acc_per_fold.append(scores[1] * 100)\n",
        "loss_per_fold.append(scores[0])\n",
        "fold_no += 1"
      ],
      "id": "8cZMB880LE4F"
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "V100"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}